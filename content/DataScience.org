#+TITLE: Data Science
#+OPTIONS: num:nil
#+SETUPFILE: /home/silin/.emacs.d/org-html-themes/org/theme-readtheorg.setup
#+STARTUP: overview
* Big data
** Association Rule Minning
- Transaction: T, one behavior, which accomplished a lot of things(items)
  -transaction instances t,
- Item : I, which stands for the smallest unit, that can be done.
- Our task is to find out, the relationship between items

*Support*
the probabilty of a itemset occurs
$$Support (IS) = \frac{|t \in T: IS \subseteq t |}{|T|}$$
All IS bigger than a setted value is called frequent itemset,
but how to set the value is self-define



*Confidence*
$$C(a->b) = P(b|a) = \frac{P(ab)}{P(a)}$$
#+begin_src 
the Probabilities  If a is done, b will also be done.
#+end_src

*Support*
$$S(a->b)=P(a\cap b) = P(ab)$$
#+begin_src 
a and b be done together from all Transaction, identify the special Transaction case
#+end_src

$$X -> Y(C=c, S=s)$$

*practicability* (Lift)
$$L(a->b)=\frac{Confidence(a->b)}{P(b)} =\frac{P(ab)}{P(a)P(b)}$$
#+begin_src 
The impact of a on b to be done
#+end_src


*Aprioir* *algorithm*

1.(with support level(S) )
- find the frequently itemset(L)
- the subsets of frequently itemset is also frequently itemset
- collect the total Transaction set(T), and set the the support level
- find all $L_{1}$, which satisfied S,
- find all $L_{2}$, which come from all 2-items combinations, which satisfied S
- .....to only one left, $L_{k}$.

2.(with Confidence (C) )find all subsets of $L_{k}$, which satisfied C.

*Note* all the operations in this 2 step is done in the whole Transaction sets

** Data Exploration
Singal feather: histogram density, rug, Box-Whisker
Box-Whisker: low quartile to high quartile is interquartile range (IQR)

low boundary: low quartile - IQR
high boundary: high quartile + IQR

pair-wise scatterplot

hexbin plot

correlation heatmap

** Time Series Analysis
*** Descripation
Discrete values  {$x_1$,....,$x_T$} = $(x_t)_{t=1}^T$
A core  assumpation is the time difference between $x_{t}$ and $x_{t+1}$ is equal for $t \in (1...T)$.
$x_{t}$ can be decomposed into 3 components:

| 1. | trend component | T | change over all time                    |
| 2. | seasonality     | S | the results of seasons effect           |
| 3. | autocorrelation | R | how the values depends on  prior values |
so   $$X = T + S + R$$
*** Box-Jenkins for stationary
**** stationary
Time series is stationary
Mean  and Variance of  the trend and seasonality  are constant and can be removed
so the autocorrelation is stochastic process, 
**** Trend and Series Effects
1. model the trend $T_{t}$ on the  time series $x_{t}$
2. detrended time series $\hat{x}=x_{t}-T_{t}$
3. model the seasonality  $S_{t}$ on the time series $x_{t}$
4. get the seasonality adjusted time series $\hat{\hat{x_{t}}}=\hat{x}-S_{t}$
**** Regession and Seasonal Means
In this context we can only use linear regession to fit the all time series, get $\hat{x}$.

and then substract the seasonal Means: $\hat{\hat{x}}= \hat{x}-(Mean(\hat x^{´}_{t})-Mean(\hat x_{t}))$
$Mean(\hat x^{´}_{t})$ is special for mod(t, s), in the recursive seasonal effect,
only the same time slot element will be calculated, this happens if
the last season is not complete.

*Cons* only works for linear trends and with seasonal effects that have no trend. 

*** Differencing for not stationary
for $y=f(x)$ for two points($x_1$, $y_1$) and ($x_2, y_2$),
the first-order difference to detrended time series:  $\hat x_{t} = \Delta x^{1}_{t}=x_{t}-x_{t-1}$.
or if you want, you can get the second-order-difference
$\Delta^{2}x_{}{t} = \Delta^{1}x_{t}-\Delta^{1}x_{t-1} = x_{t}-2x_{t-1}+x_{t-2}$

using difference to adjust the seasonal effect:
using  the difference  between two consecutive points in time during the season.
$\hat{\hat{x}} = \Delta_{s} \hat x_{t} = \hat x_{t} - \hat x_{t-s}$

*pro* it can deal with both changes in the mean, as well as changes in the movement of the mean

*** Correlation
Autocorrelation is the direct relationship of the values
of the time series at different points in time, for two adjacent points

Partial autocorrelation is the autocorrelation without the carryover,
i.e., only the direct correlation, not  for two adjacent points

for Authentication and Partial authentication we can see the
residual seasonal effect for regession and seasonal means
*** ARIMA
three ways to model correlation
**** AR: atuoregressive
model the direct influence of the past p points on time series
$c + \epsilon_{t} + \sum_{i=1}^{p}a_{i}x_{t-i}$
c :constant over all time
$\epsilon_{t}$: white noise, mean of 0, normal distribution

**** MA: Moving average
model the random effect on time series
$c + \epsilon_{t} + \sum_{j=1}^{q}b_{j}\epsilon_{t-j}$
the difference is the random influence of the past noise to next value 

**** ARMA: autoregressive and Moving average

$c + \epsilon_{t} + \sum_{i=1}^{p}a_{i}x_{t-i} + \sum_{j=1}^{q}b_{j}\epsilon_{t-j}$

**** select p and q
partial authentication estimate the p for AR, but if p can cover all the season,
but if p is too big, it can lead to overfitting.

autocorrelation can estimate the q for MA, using q as the steps for autocorrelation
to be 0, so we look at when the autocorrelation goes towards zero and use this for q.
at the same time the effect of AR should also be counted for determinate q.

** Text minning
*** Preprocessing
**** Creation of a Corpus
contains all text to analysis
**** remove the irrelevant content,
links, timestamps
**** Punctuation and Cases
remove all Punctuation, and all use small cases
a problem is about acronyms
**** Stop words
commons words should be removed, auch as I, to ,a
**** Stemming and Lemmatization
first Lemmatization, and then Stemming
*** Visualiztation
**** bag-of-words with wordclouds
**** Term frequency(TF)
is the count of a words within document
**** Inverse Document Frequency(IDF)
is to weight words by their uniqueness within the corpus
$$IDF_{t} = \log \frac{N}{D_{t}}$$
t: word(term)
N: the number of document in corpus
$D_{t}$: the number of document in corpus, which contains word t

**** TFIDF
$$TFIDF = TF \cdot IDF $$
**** beyond the bag-of-words
ignore the structure of document
ignore simiarity of words

*** challages
**** dimensionality
**** Ambiguities
* Sensor Fusion
** Sensor Dataverarbeitung
Tensor Fehler, Präzision: stochastisch
Richtigkeit: systematisch
** concepts
|----------------+-----------------------------------------------------|
| competitive    | many sensor for the same place für higher accuracy  |
| complementary  | many sensor for many places für higher completeness |
| dead reckoning | errors accumulation over previous knowlegde         |
|----------------+-----------------------------------------------------|

$$ y = H x + e $$
|----------------------+-----------------------------------------------|
| measurement equation | projects the state onto the measurement space |
| y                    | measurement                                   |
| x                    | state                                         |
| H                    | measurement matrix                            |
| e                    | measurement error                             |
|----------------------+-----------------------------------------------|

*Jacobian Matrix* one order

*Hessian Matrix*  two order

*Partial Matrix*
- $$ \frac{\partial}{\partial x}c^{T}x = \frac{\partial}{\partial x}x^{T}c  = c $$
- $$ \frac{\partial}{\partial x}x^{T}Ax = 2 Ax $$
- $$ \frac{\partial}{\partial x}Ax = \frac{\partial}{\partial x}x^{T}A = A $$

* data analysis
** statistical mothode
#+BEGIN_SRC python :results output
  from scipy import stats
  from scipy.stats import norm
  import numpy as np
  import scipy as sp
  print(sp.stats.t.ppf(0.95,6))
  print(norm.cdf([-1,0,1]))
  print(norm.cdf(np.array([-1,0,1])))
  print(norm.mean(), norm.std(), norm.var() )
  print(norm.pdf(0))
  print(norm.cdf(1.96))
  print(norm.ppf(0.975))
  print(norm.cdf(1))
  print(norm.ppf(0.841344746090))
  print(norm.sf(1-norm.cdf(1)))
  print(norm.ppf(0.9))
  print(stats.t.ppf(0.975,3))
  print(stats.t.ppf(0.975,3))

#+END_SRC

#+RESULTS:
#+begin_example
1.9431802803927816
[0.15865525 0.5        0.84134475]
[0.15865525 0.5        0.84134475]
0.0 1.0 1.0
0.3989422804014327
0.9750021048517795
1.959963984540054
0.8413447460685429
1.0000000000886762
0.4369702468636344
1.2815515655446004
3.182446305284263
3.182446305284263
#+end_example

** confidence level interval determinate
#+BEGIN_SRC python
  import numpy as np
  import scipy as sp
  import scipy.stats

  b = [8*x**0 for x in range(200)] + np.random.normal(0, 0.05, (200))


  def t_stastik(data, confidence):
      m, se = np.mean(data), sp.stats.sem(data)
      h = se*sp.stats.t.isf((1-confidence)/2. , df = (len(data)-1) )
      return m, m-h, m+h
  print(" For given data sete we have their mean  with 95% confidence level of region :",t_stastik(b,0.95))

  def mean_confidence_interval(data, confidence):
      m, se = np.mean(data), sp.stats.sem(data)
      h = se*sp.stats.t.ppf((1+confidence)/2.,len(data)-1)
      return m, m-h, m+h
  print('For data the mean  can also  be calcaleted as at 95% confidence level is :', mean_confidence_interval(b, 0.95))


#+END_SRC
** a complete ploted distribution of confidence level on t mode
#+BEGIN_SRC python
  import numpy as np
#  import seaborn as sns
  from scipy import stats
  import matplotlib.pyplot as plt

  np.random.seed(3)
  MU = 64
  sigma = 5
  size = 10
  heights = np.random.normal(MU, sigma,size)
  print("accoding to the mean and deviation we have a example of 10 rondom number : ", heights)

  mean_heights = np.mean(heights)
  deviation_heights = np.std(heights)
  SE = np.std(heights)/np.sqrt(size)

  print('99% confidence interval is :', stats.t.interval(0.99, df = size-1 , loc = mean_heights, scale=SE))
  print('90% confidence interval is :', stats.t.interval(0.90, df = size-1 , loc = mean_heights, scale=SE))
  print('80% confidence interval is :', stats.t.interval(0.80, df = size-1 , loc = mean_heights, scale=SE)) 

#+END_SRC
** a complete ploted distribution
#+BEGIN_SRC python
  import numpy as np

  sample_size = 1000
  heights = np.random.normal(MU, sigma, sample_size)
  SE = np.std(heights)/np.sqrt(sample_size)
  (l,u) = stats.norm.interval(0.95, loc = np.mean(heights), scale = SE)
  print(l,u)
  plt.hist(heights, bins = 20)
  y_height = 5
  plt.plot([l,u], [y_height, y_height], '_', color='r')
  plt.plot(np.mean(heights), y_height, 'o', color= 'b')
  plt.show()

#+END_SRC

#+RESULTS:

** a complete ploted distribution on between region

#+BEGIN_SRC python
  x = np.linspace(-5,5,100)
  y = stats.norm.pdf(x,0,1)
  plt.plot(x,y)
  plt.vlines(-1.96,0,1,colors='r',linestyles='dashed')
  plt.vlines(1.96,0,1,colors='r',linestyles='dashed')
  fill_x = np.linspace(-1.96,1.96,500)
  fill_y = stats.norm.pdf(fill_x, 0,1)
  plt.fill_between(fill_x,fill_y)
  plt.show()

#+END_SRC
** a example from internet
#+BEGIN_SRC python
  import pandas as pd
  from scipy import stats as ss
  data_url = "https://raw.githubusercontent.com/alstat/Analysis-with-Programming/master/2014/Python/Numerical-Descriptions-of-the-Data/data.csv"
  df = pd.read_csv(data_url)
  print(df.describe())
  import matplotlib.pyplot as plt
  pd.options.display.mpl_style = 'default' 
  plt.show(df.plot(kind = 'box'))

#+END_SRC
** 1 2 3 order and gauss fitting
#+BEGIN_SRC python
  import numpy as np
  import matplotlib.pyplot as plt
  from scipy import optimize
  from scipy.optimize import curve_fit


  def f_1_degree(x,A,B):
      return A*x + B

  def f_2_degree(x,A,B,C):
      return A*x**2 + B*x + C

  def f_3_degree(x,A,B,C,D):
      return A*x**3 + B*x**2 + C*x + D


  def f_gauss(x,A,B,sigma):
      return A*np.exp(-(x-B)**2/(2*sigma**2))

  def plot_figure():
      plt.figure()

      x0 = [1,2,3,4,5]
      y0 = [1,3,8,18,36]

      #plot original data
      plt.scatter(x0,y0,25,"red")

      # plot f1
      params_1, pcovariance_1 = optimize.curve_fit(f_1_degree,x0,y0)

      params_f_1, pcovariance_f_1 = curve_fit(f_1_degree,x0,y0)
      x1 = np.arange(0,6,0.01)
      y1 = params_1[0]*x1+params_1[1]
      plt.plot(x1,y1,"blue")
      print("The liear fitting for date is : y = ",params_1[1],"*x + ",params_1[0])
      print("The params uncertainies are:")
      print("a =", params_1[0], "+/-", round(pcovariance_1[0,0]**0.5,3))
      print("b =", params_1[1], "+/-", round(pcovariance_1[1,1]**0.5,3))


      #plot f2
      params_2, pcovariance_2 = curve_fit(f_2_degree,x0,y0)
      x2 = np.arange(0,6,0.01)
      y2 = params_2[0]*x1**2+params_2[1]*x1 + params_2[2]
      plt.plot(x2,y2,"green")
      print("The second order curve fitting for date is : y = " ,params_2[2],"*x² + " ,params_2[1],"*x + ",params_2[0])
      print("The params uncertainies are:")
      print("a =", params_2[0], "+/-", round(pcovariance_2[0,0]**0.5,3))
      print("a =", params_2[0], "+/-", round(pcovariance_2[0,0]**0.5,3))
      print("b =", params_2[1], "+/-", round(pcovariance_2[1,1]**0.5,3))
      print("c =", params_2[2], "+/-", round(pcovariance_2[2,2]**0.5,3))

      #plot f3
      params_3, pcovariance_3 = curve_fit(f_3_degree,x0,y0)
      x3 = np.arange(0,6,0.01)
      y3 = params_3[0]*x1**3+params_3[1]*x1**2 + params_3[2]*x1 + params_3[3]
      plt.plot(x3,y3,"purple")
      print("The second order curve fitting for date is:y =",params_3[3],"*x³+",params_2[2],"*x² + " ,params_2[1],"*x + ",params_2[0])
      print("The params uncertainies are:")
      print("a =", params_3[0], "+/-", round(pcovariance_3[0,0]**0.5,3))
      print("b =", params_3[1], "+/-", round(pcovariance_3[1,1]**0.5,3))
      print("c =", params_3[2], "+/-", round(pcovariance_3[2,2]**0.5,3))
      print("d =", params_3[3], "+/-", round(pcovariance_3[3,3]**0.5,3))

      #plot gauss
      params_gauss, pcovariance_gauss = curve_fit(f_gauss,x0,y0)
      xgauss = np.arange(0,6,0.01)
      ygauss = params_gauss[0]*np.exp(-(xgauss-params_gauss[1])**2/(2*params_gauss[2]**2))
      plt.plot(xgauss,ygauss,"black")
      print("The gauss function curve fitting for date is : y = ",params_gauss[2],"*exp{-(x-",params_gauss[1],")²/(2*sigma²) +",params_gauss[0])
      print("The params uncertainies are:")
      print("a =", params_gauss[0], "+/-", round(pcovariance_gauss[0,0]**0.5,3))
      print("mean =", params_gauss[1], "+/-", round(pcovariance_gauss[1,1]**0.5,3))
      print("std =", params_gauss[2], "+/-", round(pcovariance_gauss[2,2]**0.5,3))
 
    

      plt.title("plot for different fittign")
      plt.xlabel("x")
      plt.ylabel("y")
      plt.show()
      return

  plot_figure()


#+END_SRC
** linear fitting
#+BEGIN_SRC python
  # matplotlib inline
  import matplotlib.pyplot as plt;
  import numpy as np;
  from scipy import integrate
  from scipy.optimize import curve_fit
  import math

  #1. x axis coordinnat for 10 points data
  xmin=0.01; xmax=2; pts = 10;
  xx = np.linspace(xmin, xmax, pts);

  #2. y axis coordinnat for 10 points data
  rho = np.sqrt(1/xx) + 0.5*np.exp(xx)*xx**2;

  #plot the original data
  plt.plot(xx, rho, 'bo', label='Original data')

  #3. x axis coordinnat for 200 points fitting
  x_fine = np.linspace(xmin, xmax, 200);

  #fiting
  params, cov = np.polyfit(xx, rho, 1, cov=True)

  #to reconstruct the linear function
  bestfit_rho = params[0]*x_fine + params[1]
  plt.plot(x_fine, bestfit_rho, 'r-', lw=2, label='One order of linear fit');

  print(params)



#+END_SRC
** linear fitting with ployfit

#+BEGIN_SRC python
  # matplotlib inline
  import matplotlib.pyplot as plt;
  import numpy as np;
  from scipy import integrate
  from scipy.optimize import curve_fit
  import math

  #1. x axis coordinnat for 10 points data
  xmin=0.01; xmax=2; pts = 10;
  xx = np.linspace(xmin, xmax, pts);

  #2. y axis coordinnat for 10 points data
  rho = np.sqrt(1/xx) + 0.5*np.exp(xx)*xx**2;

  #plot the original data
  plt.plot(xx, rho, 'bo', label='Original data')

  #3. x axis coordinnat for 200 points fitting
  x_fine = np.linspace(xmin, xmax, 200);

  #fiting  it can be any order 
  params, cov = np.polyfit(xx, rho, 4, cov=True);
  p = np.poly1d(params)
  plt.plot(x_fine, p(x_fine), 'g-', lw=2, label='The Best poly1d fit');

  print(params)
  plt.xlabel('$x$');
  plt.ylabel(r'$\rho$');
  plt.legend(fontsize=13);
  plt.show()


#+END_SRC

* Hadoop
** one node installation
*** .bashrc
#+begin_src
export HADOOP_HOME=/home/cloud/hadoop-3.3.1
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
export HADOOP_OPTS"-Djava.library.path=$HADOOP_HOME/lib/nativ"
#+end_src

*** $HADOOP_HOME/etc/hadoop/hadoop-env.sh
append to end
#+begin_src
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
#+end_src

*** $HADOOP_HOME/etc/hadoop/core-site.xml
in configuration
#+begin_src
   <property>
        <name>hadoop.tmp.dir</name>
        <value>/home/cloud/tmpdata</value>
        <description>A base for other temporary directories.</description>
    </property>
    <property>
        <name>fs.default.name</name>
        <value>hdfs://localhost:9000</value>
        <description>The name of the default file system></description>
    </property>
#+end_src

*** $HADOOP_HOME/etc/hadoop/hdfs-site.xml
in configuration
#+begin_src
<property>
  <name>dfs.data.dir</name>
  <value>/home/cloud/dfsdata/namenode</value>
</property>
<property>
  <name>dfs.data.dir</name>
  <value>/home/cloud/dfsdata/datanode</value>
</property>
<property>
  <name>dfs.replication</name>
  <value>1</value>
</property>
#+end_src

*** $HADOOP_HOME/etc/hadoop/mapred-site.xml
in configuration
#+begin_src
<property>
  <name>mapreduce.framework.name</name>
  <value>yarn</value>
</property>
#+end_src

*** $HADOOP_HOME/etc/hadoop/yarn-site.xml
#+begin_src
<property>
  <name>yarn.nodemanager.aux-services</name>
  <value>mapreduce_shuffle</value>
</property>
<property>
  <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
  <value>org.apache.hadoop.mapred.ShuffleHandler</value>
</property>
<property>
  <name>yarn.resourcemanager.hostname</name>
  <value>127.0.0.1</value>
</property>
<property>
  <name>yarn.acl.enable</name>
  <value>0</value>
</property>
<property>
  <name>yarn.nodemanager.env-whitelist</name>
  <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PERPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
</property>
#+end_src

*** init nodename
#+begin_src
cd hadoop-3---
hdfs namenode -format
cd sbin
./start-dfs.sh
./start-yarn.sh
jps
#+end_src

** 3 node installation
*** GWDG deployment
|----------------+-------------+-----------------+-------------------|
| software       | .1          | .10             | .19               |
|----------------+-------------+-----------------+-------------------|
| hadoop         | hadoop1     | hadoop2         | hadoop3           |
|----------------+-------------+-----------------+-------------------|
| hostname       | project     | q3lb            | q3l               |
|----------------+-------------+-----------------+-------------------|
| HDFS Namenode  | NameNode    |                 | SecondaryNameNode |
| HDFS DataNode  | DataNode    | DataNode        | DataNode          |
|----------------+-------------+-----------------+-------------------|
| YARN ResourceM |             | ResourceManager |                   |
| YARN NodeM     | NodeManager | NodeManager     | NodeManager       |
|----------------+-------------+-----------------+-------------------|
*** .bashrc
#+begin_src
export HADOOP_HOME=/home/cloud/hadoop-3.3.1
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
export HADOOP_OPTS"-Djava.library.path=$HADOOP_HOME/lib/nativ"
#+end_src

*** $HADOOP_HOME/etc/hadoop/hadoop-env.sh
append to end
#+begin_src
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
#+end_src
*** $HADOOP_HOME/etc/hadoop/core-site.xml
in configuration

#+begin_src
   <property>
        <name>hadoop.tmp.dir</name>
        <value>/home/cloud/hadoop-3.3.1/data</value>
        <description>A base for other temporary directories.</description>
    </property>
    <property>
        <name>fs.default.name</name>
        <value>hdfs://hostname:9000</value> watch out for inter floatip for localhost 
        <description>The name of the default file system></description>
    </property>
#+end_src
*** $HADOOP_HOME/etc/hadoop/hdfs-site.xml
in configuration
#+begin_src
<property>
  <name>dfs.data.dir</name>
  <value>/home/cloud/hahoop-3.3.1/dfsdata/namenode</value>
</property>
<property>
  <name>dfs.data.dir</name>
  <value>/home/cloud/hahoop-3.3.1/dfsdata/datanode</value>
</property>
<property>
  <name>dfs.replication</name>
  <value>3</value>
</property>
<property>
  <name>dfs.namenode.http-address</name>
  <value>*inter floatip:9870*</value>
</property>
<property>
  <name>dfs.namenode.secondary.http-address</name>
  <value>inter floatip:9868</value>
</property>



#+end_src
*** $HADOOP_HOME/etc/hadoop/yarn-site.xml
#+begin_src
<property>
  <name>yarn.nodemanager.aux-services</name>
  <value>mapreduce_shuffle</value>
</property>
<property>
  <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
  <value>org.apache.hadoop.mapred.ShuffleHandler</value>
</property>
<property>
  <name>yarn.resourcemanager.hostname</name>
  <value>*inter floatip*</value>
</property>
<property>
  <name>yarn.acl.enable</name>
  <value>0</value>
</property>
<property>
  <name>yarn.nodemanager.env-whitelist</name>
  <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PERPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
</property>
#+end_src
*** $HADOOP_HOME/etc/hadoop/mapred-site.xml
in configuration
#+begin_src
<property>
  <name>mapreduce.framework.name</name>
  <value>yarn</value>
</property>
#+end_src
*** $HADOOP_HOME/etc/hadoop/wores
gwdg01
gwdg10
gwdg19
*** init nodename
#+begin_src
cd hadoop-3---
xsycn etc/hadoop
hdfs namenode -format
cd sbin
./start-dfs.sh
./start-yarn.sh
jps
#+end_src

** command
general comands
#+begin_src 
hdfs dfs -ls /
hdfs dfs -chmod 777 /testFolder
hdfs dfs -cat /tesFolder/text.txt
hdfs dfs -get hdfspath localpath
hdfs dfs -put localpath hdfspath
hdfs dfsadmin -report
hdfs fsck /
#+end_src

word example
#+begin_src 
hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar wordcount /input /output/
hadoop fs -cat /output/part-r-00000
cd output
hadoop fs -getmerge /hpda04-2.3-output/ out
cat out
#+end_src

** map()
map(fun <key1, val1>) -> list(<key2, val2>)
to a list of key-value pairs
all elemenet in list must have the same type
** Schuffle
schuffle(list(<key2, val2>)) -> list(<key2, list(val2)>)
** reduce
reduce (fun, list(<key2, list(val2)>)) -> list(val3)

** Limitation
1, multiple map() and reduce() must be manually specified
2, intermediary results has to be written to  the HDFS, not on memory
iterative algorithms are not very efficient with Hadoop.

* HDFS
** descripation
Hadoop distributed file system
Namenode vs Datanodes

1, high throughout with low latency
2, support large file
3, locally computation in Node, less transfer zwischen Nodes
4, resilient design for hardware failurs

* YARN
Yet Another Resource Negotiator
Resource Manager vs NodeManager
Resource Manager avoid overutilization and underutilization
The NodeManager execute tasks on the local resources
1, Client send a requirement to Resource Manager
2, Resource manager allocate container in Node Manager
3, Container in Node Manager start the application Master
4, Application Master require Resource from Resoure Manager
5, as the required Resoure is allocated, application master start the Application
* Spark
** 3 node installation
*** GWDG deployment
|----------------+-------------+-----------------+-------------------|
| floatip        | .1          | .10             | .19               |
|----------------+-------------+-----------------+-------------------|
| hostname       | gwdg01      | gwdg10          | gwdg19            |
|----------------+-------------+-----------------+-------------------|
| ip             | .8          | .5              | .10               |
|----------------+-------------+-----------------+-------------------|
| HDFS Namenode  | NameNode    |                 | SecondaryNameNode |
| HDFS DataNode  | DataNode    | DataNode        | DataNode          |
|----------------+-------------+-----------------+-------------------|
| YARN ResourceM |             | ResourceManager |                   |
| YARN NodeM     | NodeManager | NodeManager     | NodeManager       |
|----------------+-------------+-----------------+-------------------|
*** .bashrc
#+begin_src

#+end_src

** descripation
results do not  need to save in HDFS, it support in memory  executation.
Resilient Distributed Datasets RDDS
DataFrame from SparkSQL

** scala
can from binary file
can from source file
can from IDEA blugin
can from spark installation

** install
*** from source
this is a full eco system, can build a cluster by my own,
with embended scala
*** from pip
my Prof can also build a eco system in pip download file, with config in  master:
spark-submit --deploy-mode --master yarn test.py
But I can't, I can even not find conf file in pip file for pyspark,
if you still want to consturcte a cluster, use spark installation from source file,
like following

** single  master node configuration with
#+begin_src sh :results output
cat ~/Documents/spark/myown/test.py
#+end_src
#+RESULTS:
 from pyspark.sql import SparkSession
 spark = SparkSession.builder.appName("examples").getOrCreate()
 
print("hello world")

#+begin_src 
cd .../spark
./sbin/start-all
curl localhost:8080(spark-url for master)
./bin/spark-submit --master spark-url ./myown/test.py
#+end_src
test.py will be executed 

#+begin_src 
./bin/pyspark --master spark-url 
#+end_src
will open  a terminal with master configuration

** pyspark
#+begin_src
cd spark
bin/spark-submit examples/src/main/python/wordcount.py testtext.txt &> output.txt
#+end_src

* High performance Data Analysis
** concepts
High performance Data Analysis:
with parallel processing to quickly find the insights from extremely large data sets
** Chap01 overview
*** Distributed System
1. Definiation:
- Components separate located
- communicatation through passing massage between components

Characteristics:
- own memory
- concurrency
- locks

Applcation:
- cloud compuation
- internet of Things

Algorithm:
Consensus, Repication

Challages:
- Programm
- resource sharing

*** Levels of parallelism
Bit-level, Instruction level, Data level, Task level

*** Name typical applications for high-performance data analytics
1. weather forecast
2. Simulating   kernel fusion,  tokamak reactor

*** Distinguish HPDA from D/P/S computing and how these topics blend
Stricter than distributed system( strongly scalling: weak scalling)

*** Describe use-cases and challenges in the domain of D/P/S computing
Big Data 5 Vs

*** Describe how the scientific method relies on D/P/S computing
Simulation models real systems to gain new insight
Big Data Analytics extracts insight from data
*** Name big data challenges and the typical workflow
how to deal with big data(5Vs)
Raw-> Descriptive -> Diagnostics-> Predictive-> Prescriptive

*** Recite system characteristics for distributed/parallel/computational science
*** Sketch generic D/P system architectures

** Chap02  DataModels & Data Processing Strategies
*** Define important terminology for data handling and data processing
Raw data, semantic normalization, Data management plan, Data life cycle,
data governance,  data provenance...

*** Sketch the ETL process used in data warehouses
extract from a source database,
transform with controlling, error and missing treatment
loading, integrate them into data warehouses for user
*** Sketch a typical HPDA data analysis workflow
classical: discovery, integration, exploitation
in high level,  with  SQL, java, scala, Python, with parallelism for data Exploration

*** Sketch the lambda architecture
Lambda architecture is a concept for enabling real-time processing and batch methods together.
batch layer(large scala) + serving layer
speed layer(read time)

*** Construct suitable data models for a given use-case and discuss their pro/cons
*** Define relevant semantics for data
*** data models
Concurrency, Durability, Consensus,
- relational model
- Clumnar Model (combinded relational model)
- key-value model (json)
- Documents model (xml)
- Graph
** Chap03 Databases and DataWarehouses
*** relatation model 
**** Cardinality
- one to one
- one to many
- many to many
**** Normalization Form
reduces dependencies, prevents inconsistency, save space
- 1NF:  no collections in row tuples
- 2NF: no redundancy
- 3NF: no dependence between columns
- 4NF: no multiplie relationships in one table(not good for big data)
**** group by
it's done with Aggregatation(in sql or in python, both)
**** join
cross join: Cartesian product of two tables
inner join: only all condition satisfied
left join: condition strict on left
right join: condition strict on right
full join
**** Transactions


*** Define Database, DBMS, and Data Warehouse
- an organized collection of data
- software application for user to use the collected data
- a system used for reporting and data analysis,  with multidimensional data cube
  


*** Create a relational model for a given problem
*** Draw an ER(Entity Relational) diagram for a given relational model (and vice versa)
*** Normalize a small relational model into a redundant-free model
*** List the result of an inner join of two tables to resolve relationships
*** Formulate SQL queries for a relational model
*** Create a Star-Schema from a relational model (and formulate queries)
*** Sketch the operations for an OLAP cube
- Slice
- Dice
- Roll up
- Pivot
*** Appraise the pro/cons of OLAP vs. traditional relational model
*** Describe DBMS optimizations: index, bulk loading, garbage cleaning
  
** Chap04 Distributed Storage and Processing with Hadoop
*** hadoop
map: filter and convert all input into key-value tuples
reduce: receives all tuples with the same keys, accumulated


*** Describe the architecture and features of Apache Hadoop
*** Formulate simple algorithms using the MapReduce programming model
*** Justify architectural decisions made in Apache Hadoop
*** Sketch the execution phases of MapReduce and describe their behavior
1. distributed code
2. determine fiels
3. map
4. combine
5. shuffle
6. partition
7. reduce
8. output
*** Describe limitations of Hadoop1 and the benefits of Hadoop2 with TEZ
- Directed acyclic graph (DAG) of tasks
- Task and resource aware scheduling
- Pre-launch and re-use containers and caching intermediate results
- Everyone has to wait for the prozess between mapping and reducing
*** Sketch the parallel file access performed by MapReduce jobs

** Chap05 Big Data SQL using Hive



** 03-01
#+begin_src sql :engine postgresql :dbhost localhost :dbuser postgres :dbpassword du :database postgres :dbport 5432
    drop table if exists WikipediaArticles ;
    create table WikipediaArticles (
    id int,
    title varchar(50), 
    text varchar(50),
    category varchar(50),
    link int
    ) ;
    \d wikipediaarticles;
#+end_src

#+RESULTS:
| DROP TABLE                       |                       |           |          |         |
|----------------------------------+-----------------------+-----------+----------+---------|
| CREATE TABLE                     |                       |           |          |         |
| Table "public.wikipediaarticles" |                       |           |          |         |
| Column                           | Type                  | Collation | Nullable | Default |
| id                               | integer               |           |          |         |
| title                            | character varying(50) |           |          |         |
| text                             | character varying(50) |           |          |         |
| category                         | character varying(50) |           |          |         |
| link                             | integer               |           |          |         |


#+begin_src sql :engine postgresql :dbhost localhost :dbuser postgres :dbpassword du :database postgres :dbport 5432
  drop table if exists linkarticles ;
      create table linkarticles (
      id int,
      linked int
   ) ;
#+end_src

#+RESULTS:
| DROP TABLE   |
|--------------|
| CREATE TABLE |



#+begin_src sql :engine postgresql :dbhost localhost :dbuser postgres :dbpassword du :database postgres :dbport 5432
  delete from wikipediaarticles where id = 1;
  insert into WikipediaArticles (id, title, text, category, link) values (1, 'math', 'mathematics and nature and nature', 'nature', 1) ;
  delete from wikipediaarticles where id = 2;
  insert into WikipediaArticles (id, title, text, category, link) values (2, 'phy', 'physics', 'nature', 2) ;
  delete from wikipediaarticles where id = 3;
  insert into WikipediaArticles (id, title, text, category, link) values (3, 'chemie', 'chemistry', 'science', 3) ;
  delete from wikipediaarticles where id = 4;
  insert into WikipediaArticles (id, title, text, category, link) values (4, 'bio', 'biology', 'science', 4) ;
  select * from wikipediaarticles ;
#+end_src

#+RESULTS:
| DELETE 0   |        |                                   |          |      |
|------------+--------+-----------------------------------+----------+------|
| INSERT 0 1 |        |                                   |          |      |
| DELETE 0   |        |                                   |          |      |
| INSERT 0 1 |        |                                   |          |      |
| DELETE 0   |        |                                   |          |      |
| INSERT 0 1 |        |                                   |          |      |
| DELETE 0   |        |                                   |          |      |
| INSERT 0 1 |        |                                   |          |      |
| id         | title  | text                              | category | link |
| 1          | math   | mathematics and nature and nature | nature   |    1 |
| 2          | phy    | physics                           | nature   |    2 |
| 3          | chemie | chemistry                         | science  |    3 |
| 4          | bio    | biology                           | science  |    4 |


#+begin_src sql :engine postgresql :dbhost localhost :dbuser postgres :dbpassword du :database postgres :dbport 5432
  delete from linkarticles where id = 1;
  insert into Linkarticles (id, linked) values (1, 2) ;
  insert into Linkarticles (id, linked) values (1, 3) ;
  delete from linkarticles where id = 2;
  insert into Linkarticles (id, linked) values (2, 3) ;
  delete from linkarticles where id = 3;
  insert into Linkarticles (id, linked) values (3, 4) ;
  delete from linkarticles where id = 4;
  insert into Linkarticles (id, linked) values (4, 1) ;
  select * from linkarticles ;
#+end_src

#+RESULTS:
| DELETE 0   |        |
|------------+--------|
| INSERT 0 1 |        |
| INSERT 0 1 |        |
| DELETE 0   |        |
| INSERT 0 1 |        |
| DELETE 0   |        |
| INSERT 0 1 |        |
| DELETE 0   |        |
| INSERT 0 1 |        |
| id         | linked |
| 1          |      2 |
| 1          |      3 |
| 2          |      3 |
| 3          |      4 |
| 4          |      1 |


#+begin_src sql :engine postgresql :dbhost localhost :dbuser postgres :dbpassword du :database postgres :dbport 5432
select * from wikipediaarticles where title = 'phy';
#+end_src

#+RESULTS:
| id | title | text    | category | link |
|----+-------+---------+----------+------|
|  2 | phy   | physics | nature   |    2 |


#+begin_src sql :engine postgresql :dbhost localhost :dbuser postgres :dbpassword du :database postgres :dbport 5432
  select * from wikipediaarticles where id in
   (select linked from linkarticles where id in
    (select id from wikipediaarticles where title = 'math')
  );

#+end_src

#+RESULTS:
| id | title  | text      | category | link |
|----+--------+-----------+----------+------|
|  2 | phy    | physics   | nature   |    2 |
|  3 | chemie | chemistry | science  |    3 |


#+begin_src sql :engine postgresql :dbhost localhost :dbuser postgres :dbpassword du :database postgres :dbport 5432
  select count(*) , linked from linkarticles group by linked;
#+end_src

#+RESULTS:
| count | linked |
|-------+--------|
|     2 |      3 |
|     1 |      4 |
|     1 |      2 |
|     1 |      1 |


#+begin_src sql :engine postgresql :dbhost localhost :dbuser postgres :dbpassword du :database postgres :dbport 5432
  select unnest(string_to_array('this is is is a test', ' '))
#+end_src

#+RESULTS:
| unnest |
|--------|
| this   |
| is     |
| is     |
| is     |
| a      |
| test   |


#+begin_src sql :engine postgresql :dbhost localhost :dbuser postgres :dbpassword du :database postgres :dbport 5432
select id,  unnest(string_to_array(text , ' ')) as word, count(*) from WikipediaArticles group by id, word
#+end_src

#+RESULTS:
| id | word        | count |
|----+-------------+-------|
|  4 | biology     |     1 |
|  3 | chemistry   |     1 |
|  2 | physics     |     1 |
|  1 | nature      |     2 |
|  1 | and         |     2 |
|  1 | mathematics |     1 |



#+begin_src sql :engine postgresql :dbhost localhost :dbuser postgres :dbpassword du :database postgres :dbport 5432
  select * from wikipediaarticles where category = 'science';
#+end_src

#+RESULTS:
| id | title  | text      | category | link |
|----+--------+-----------+----------+------|
|  3 | chemie | chemistry | science  |    3 |
|  4 | bio    | biology   | science  |    4 |

** 03-02
#+BEGIN_SRC  dot :file ./foto/hpdas03-02.png
  digraph diagramm {
    WikipediaArticles  -> id
    WikipediaArticles  -> Title
    WikipediaArticles  -> Text
    WikipediaArticles  -> Category
    WikipediaArticles  -> Links
    Links  -> linkarticles
    linkarticles -> lid
    linkarticles -> linked
  }

#+END_SRC

#+RESULTS:
[[file:./foto/hpdas03-02.png]]

** 04-01
*** mapper and reducer in own
#+begin_src python
  def mapper(key, value):
    words = key.split()
    for word in words:
      Wmr.emit(word, 1)

  def mapper(key, value):
    words = key.split()
    for word in words:
      Wmr.emit("s", stem(word), 1)
    for word in words:
      Wmr.emit("l", lemmatize(word), 1)

    
  def reducer(key, values):
    count = 0
    for value in values:
      count += int(value)
      Wmr.emit(key, count)

#+end_src

*** sql
#+begin_src sh
  cat ~/Documents/hpda0404.csv 
#+end_src

#+RESULTS:

#+begin_src sql :engine postgresql :dbhost localhost :dbuser postgres :dbpassword du :database postgres :dbport 5432
  drop table if exists hpda0401 ;

  create table hpda0401 (
  num int,
  germany varchar(10),
  english varchar(10),
  chinese varchar(10),
  listed int
  ) ;

  insert into hpda0401 (num, germany, english, chinese, listed) values (1, 'eins', 'one','一', 1);
  insert into hpda0401 (num, germany, english, chinese, listed) values (2, 'zwei', 'two','二', 1);
  insert into hpda0401 (num, germany, english, chinese, listed) values (3, 'drei', 'three','三', 2);
  insert into hpda0401 (num, germany, english, chinese, listed) values (6, 'sechs', 'six','六', 2);

  select germany from hpda0401 where  germany = 'zwei';

  select listed, sum(num) as mysum from hpda0401 group by listed;
#+end_src

#+RESULTS:
| DROP TABLE   |       |
|--------------+-------|
| CREATE TABLE |       |
| INSERT 0 1   |       |
| INSERT 0 1   |       |
| INSERT 0 1   |       |
| INSERT 0 1   |       |
| germany      |       |
| zwei         |       |
| listed       | mysum |
| 2            |     9 |
| 1            |     3 |

*** select
#+begin_src python  :results output
  import csv
  from functools import reduce
  path = "/home/si/Documents/hpda0404.csv"
  data = []
  with open(path) as f:
      records = csv.DictReader(f)
      for row in records:
          data.append(row)
      print(data)


  mapiter = map(lambda x: x["germany"], data)
  maplist = [ele for ele in mapiter]
  print(maplist)    

  filteriter = filter(lambda x: x=="zwei", maplist)
  filterlist = [ele for ele in filteriter]
  print("select germany WHERE germany == zwei :", filterlist)

#+end_src

#+RESULTS:
: [{'num': '1', 'germany': 'eins', 'english': 'one', 'chinese': '一', 'listed': '1'}, {'num': '2', 'germany': 'zwei', 'english': 'two', 'chinese': '二', 'listed': '1'}, {'num': '3', 'germany': 'drei', 'english': 'three', 'chinese': '三', 'listed': '2'}, {'num': '6', 'germany': 'sechs', 'english': 'six', 'chinese': '六', 'listed': '2'}]
: ['eins', 'zwei', 'drei', 'sechs']
: select germany WHERE germany == zwei : ['zwei']

*** summation
#+begin_src python  :results output
  import csv
  from functools import reduce
  path = "/home/si/Documents/hpda0404.csv"
  data = []
  with open(path) as f:
      records = csv.DictReader(f)
      for row in records:
          data.append(row)
      print(data)


  iters = map(lambda x: x["listed"], data)
  iterslist = [ele for ele in iters]
  iterset = set(iterslist)
  print("grouped by ", iterset)

  dic = {}
  for i in iterset:
      temp = []
      for d in data:
          for (j, n) in [b for b in map(lambda x: (x["listed"],x["num"]), [d])]:
              if i == j:
                  temp.append(int(n))
      reduer = reduce(lambda x, y:x+y, temp)
      dic[i]= reduer

  print("sum (num) GROUP) BY listed : ", dic)
#+end_src

#+RESULTS:
: [{'num': '1', 'germany': 'eins', 'english': 'one', 'chinese': '一', 'listed': '1'}, {'num': '2', 'germany': 'zwei', 'english': 'two', 'chinese': '二', 'listed': '1'}, {'num': '3', 'germany': 'drei', 'english': 'three', 'chinese': '三', 'listed': '2'}, {'num': '6', 'germany': 'sechs', 'english': 'six', 'chinese': '六', 'listed': '2'}]
: grouped by  {'1', '2'}
: sum (num) GROUP) BY listed :  {'1': 3, '2': 9}

*** join
#+begin_src sh
  cat ~/Documents/hpda0404a.csv 
  cat ~/Documents/hpda0404b.csv
#+end_src

#+RESULTS:
| id | germany | english | chinese | listed |
|  1 | eins    | one     | 一      |      1 |
|  2 | zwei    | two     | 二      |      1 |
|  3 | drei    | three   | 三      |      2 |
|  6 | sechs   | six     | 六      |      2 |
| id | fan     |         |         |        |
|  1 | une     |         |         |        |
|  3 | trois   |         |         |        |
|  4 | quatre  |         |         |        |
|  8 | huit    |         |         |        |

#+begin_src python  :results output
  import csv
  from functools import reduce
  path1 = "/home/si/Documents/hpda0404a.csv"
  path2 = "/home/si/Documents/hpda0404b.csv"
  data1 = []
  with open(path1) as f:
      records = csv.DictReader(f)
      for row in records:
          data1.append(row)
      print(data1)

  data2 = []
  with open(path2) as f:
      records = csv.DictReader(f)
      for row in records:
          data2.append(row)
      print(data2)    


  for a in data1:
      aid = [y for y in map(lambda x: x["id"], [a])]
      for b in data2:
          bid = [y for y in map(lambda x: x["id"], [b])]
          if aid == bid:
              (af1, bf2) = ([y for y in map(lambda x: x["germany"], [a])], [y for y in map(lambda x: x["fan"], [b])])
              print(af1, bf2)

#+end_src

#+RESULTS:
: [{'id': '1', 'germany': 'eins', 'english': 'one', 'chinese': '一', 'listed': '1'}, {'id': '2', 'germany': 'zwei', 'english': 'two', 'chinese': '二', 'listed': '1'}, {'id': '3', 'germany': 'drei', 'english': 'three', 'chinese': '三', 'listed': '2'}, {'id': '6', 'germany': 'sechs', 'english': 'six', 'chinese': '六', 'listed': '2'}]
: [{'id': '1', 'fan': 'une'}, {'id': '3', 'fan': 'trois'}, {'id': '4', 'fan': 'quatre'}, {'id': '8', 'fan': 'huit'}]
: ['eins'] ['une']
: ['drei'] ['trois']

** 04-02
*** 2.1
#+begin_src python :results output
  from nltk.stem.snowball import SnowballStemmer
  from nltk.stem import WordNetLemmatizer

  stemmer = SnowballStemmer("english")
  lemmatizer = WordNetLemmatizer()

  file = "/home/si/Documents/hpda0402wordscount.txt"
  sdict = {}
  ldict = {}
  with open(file, "r") as data:
      datas = data.read()
      words = datas.split(' ')
      for word in words:
          sword = stemmer.stem(word)
          lword = lemmatizer.lemmatize(word)
          if sword in sdict:
              sdict[sword] += 1
          else:
              sdict[sword] = 1

          if lword in ldict:
              ldict[lword] += 1
          else:
              ldict[lword] = 1

      print("---------sdict----------------------")
      for (item, key) in sdict.items():
            print(item, key)

      print("---------ldict----------------------")
      for (item, key) in sdict.items():
          print(item, key)


#+end_src

#+RESULTS:
#+begin_example
---------sdict----------------------
word 43
count 20
from 2
wikipedia 1
the 39
free 1
encyclopedia
th 1
is 16
number 4
of 23
in 11
a 26
document 3
or 10
passag 1
text 6
may 8
be 9
need 1
when 3
text
i 1
requir 2
to 17
stay 1
within 1
certain 2
this 3
particular 1
case 1
academia 1
legal
proceed 1
journal 1
and 20
advertis 1
common 1
use 4
by 5
translat 2
determin 2
price 1
for
th 1
job 1
also 5
calcul 1
measur 3
readabl 1
typing
and 1
read 1
speed 1
usual 4
per 3
minut 1
convert 1
charact 3
five 1
or
six 1
general 2
content 1
detail 2
variat 3
definit 5
softwar 4
fiction
in 1
non 2
fiction 4
see 1
refer 3
sourc 3
extern 1
link 1
definition
thi 1
section 3
doe 1
not 2
cite 1
ani 2
pleas 1
help 1
improv 1
ad 1
citat 1
to
reli 1
unsourc 1
materi 1
challeng 1
removed
vari 1
oper 2
how 2
can 5
occur 1
name 2
what 1
as 10
and
which 1
don't 2
toward 2
total 2
howev 3
especi 1
sinc 1
advent 1
widespread 1
process 4
there
i 1
broad 2
consensus 2
on 7
these 3
henc 1
bottom 1
line 3
integ 1
result
th 1
accept 2
segment 3
rule 6
found 1
most 4
includ 3
how
word 1
boundari 2
are 4
which 3
depend 4
divid 2
defin 2
first 1
trait 1
that 5
space 3
various 1
whitespace
charact 1
such 7
regular 1
an 4
em 1
tab 1
hyphen 2
slash 1
too
differ 1
program 3
give 2
vari 4
result 2
rule
detail 2
whether 1
outsid 1
main 1
footnot 2
endnot 2
hidden 2
text) 1
but 3
behavior
of 1
major 1
applic 2
similar 1
dure 1
era 2
school 1
assign 1
were 3
done 1
in
handwrit 1
with 2
typewrit 1
for 9
often 3
differ 2
today 1
consensus
most 1
import 2
mani 2
student 2
drill 1
articl 1
but
sometim 1
other 1
conjunct 1
exampl 2
some 1
preposit 1
permanent
compound 1
follow 2
up 1
noun 1
long 1
term 1
adject 1
one 1
save 1
time 1
effort 1
counting
word 1
thumb 1
averag 1
was 1
10 1
rules
hav 1
fallen 1
waysid 1
featur 1
text
segment 1
mention 1
earlier 2
now 1
standard 1
arbit 1
becaus 2
it 5
larg 2
consist 1
across 1
and
appl 1
fast 1
effortless 1
costless 1
alreadi 1
of
a 1
abstract 1
list 2
bibliographi 1
tabl 1
figure
capt 1
person 1
charg 1
teacher 1
client 1
their 1
choic 1
user 1
worker 1
simply
select 1
exclud 1
element 1
accord 1
watch 1
automat 1
updat 1
modern 1
web 1
browsers
support 1
via 2
extens 1
javascript 1
bookmarklet 1
script 1
host 1
websit 1
word
processor 1
unix 1
like 1
system 1
wc 1
specif 1
counting
a 1
explain 1
exact 1
strict 1
thus 1
acceptable
in 1
novelist 1
jane 1
smiley 2
suggest 1
length 7
qualiti 1
novel 8
vary
tremend 1
typic 1
between 2
while 3
nation 1
write 1
month
requir 1
at 3
least 1
there 1
no 1
firm 1
novella 2
novel
i 1
arbitrari 1
literari 1
work 1
difficult 1
categoris 1
extent 1
up
to 1
writer 2
subgenr 1
chapter 1
book 1
children 1
start 1
about 1
a
typ 1
mysteri 1
might 1
rang 1
thriller 1
could 1
over 2
words
th 1
scienc 1
fantasi 1
america 1
specifi 1
each 1
categori 1
nebula 1
award 1
categories
classification	word 1
novelett 1
short 1
stori 1
under 1
words
in 1
academ 1
dissert 2
great 1
predomin 1
subject
numer 1
american 1
univers 1
limit 1
ph.d. 1
bar 1
special 1
permiss 1
exceed 1
limit
 1
---------ldict----------------------
word 43
count 20
from 2
wikipedia 1
the 39
free 1
encyclopedia
th 1
is 16
number 4
of 23
in 11
a 26
document 3
or 10
passag 1
text 6
may 8
be 9
need 1
when 3
text
i 1
requir 2
to 17
stay 1
within 1
certain 2
this 3
particular 1
case 1
academia 1
legal
proceed 1
journal 1
and 20
advertis 1
common 1
use 4
by 5
translat 2
determin 2
price 1
for
th 1
job 1
also 5
calcul 1
measur 3
readabl 1
typing
and 1
read 1
speed 1
usual 4
per 3
minut 1
convert 1
charact 3
five 1
or
six 1
general 2
content 1
detail 2
variat 3
definit 5
softwar 4
fiction
in 1
non 2
fiction 4
see 1
refer 3
sourc 3
extern 1
link 1
definition
thi 1
section 3
doe 1
not 2
cite 1
ani 2
pleas 1
help 1
improv 1
ad 1
citat 1
to
reli 1
unsourc 1
materi 1
challeng 1
removed
vari 1
oper 2
how 2
can 5
occur 1
name 2
what 1
as 10
and
which 1
don't 2
toward 2
total 2
howev 3
especi 1
sinc 1
advent 1
widespread 1
process 4
there
i 1
broad 2
consensus 2
on 7
these 3
henc 1
bottom 1
line 3
integ 1
result
th 1
accept 2
segment 3
rule 6
found 1
most 4
includ 3
how
word 1
boundari 2
are 4
which 3
depend 4
divid 2
defin 2
first 1
trait 1
that 5
space 3
various 1
whitespace
charact 1
such 7
regular 1
an 4
em 1
tab 1
hyphen 2
slash 1
too
differ 1
program 3
give 2
vari 4
result 2
rule
detail 2
whether 1
outsid 1
main 1
footnot 2
endnot 2
hidden 2
text) 1
but 3
behavior
of 1
major 1
applic 2
similar 1
dure 1
era 2
school 1
assign 1
were 3
done 1
in
handwrit 1
with 2
typewrit 1
for 9
often 3
differ 2
today 1
consensus
most 1
import 2
mani 2
student 2
drill 1
articl 1
but
sometim 1
other 1
conjunct 1
exampl 2
some 1
preposit 1
permanent
compound 1
follow 2
up 1
noun 1
long 1
term 1
adject 1
one 1
save 1
time 1
effort 1
counting
word 1
thumb 1
averag 1
was 1
10 1
rules
hav 1
fallen 1
waysid 1
featur 1
text
segment 1
mention 1
earlier 2
now 1
standard 1
arbit 1
becaus 2
it 5
larg 2
consist 1
across 1
and
appl 1
fast 1
effortless 1
costless 1
alreadi 1
of
a 1
abstract 1
list 2
bibliographi 1
tabl 1
figure
capt 1
person 1
charg 1
teacher 1
client 1
their 1
choic 1
user 1
worker 1
simply
select 1
exclud 1
element 1
accord 1
watch 1
automat 1
updat 1
modern 1
web 1
browsers
support 1
via 2
extens 1
javascript 1
bookmarklet 1
script 1
host 1
websit 1
word
processor 1
unix 1
like 1
system 1
wc 1
specif 1
counting
a 1
explain 1
exact 1
strict 1
thus 1
acceptable
in 1
novelist 1
jane 1
smiley 2
suggest 1
length 7
qualiti 1
novel 8
vary
tremend 1
typic 1
between 2
while 3
nation 1
write 1
month
requir 1
at 3
least 1
there 1
no 1
firm 1
novella 2
novel
i 1
arbitrari 1
literari 1
work 1
difficult 1
categoris 1
extent 1
up
to 1
writer 2
subgenr 1
chapter 1
book 1
children 1
start 1
about 1
a
typ 1
mysteri 1
might 1
rang 1
thriller 1
could 1
over 2
words
th 1
scienc 1
fantasi 1
america 1
specifi 1
each 1
categori 1
nebula 1
award 1
categories
classification	word 1
novelett 1
short 1
stori 1
under 1
words
in 1
academ 1
dissert 2
great 1
predomin 1
subject
numer 1
american 1
univers 1
limit 1
ph.d. 1
bar 1
special 1
permiss 1
exceed 1
limit
 1
#+end_example
*** 2.2
*** 2.3
see in Document folder
*** 2.4
mapper
#+begin_src python
  import sys
  for line in sys.stdin:
    words = line.strip().split(" ")
      for word in words:
      print(word + "\t" + "1")

#+end_src

reducer
#+begin_src python
  import sys

  oldword = ""
  count = 0
  for line in sys.stdin:
      (word, c) = line.strip().split("\t")
      if word != oldword:
          if count != 0:
              print(oldword +"\t"+ str(count))
          count = 0
          oldword = word
      count = count + int(c)
  if oldword != "":
      print(oldword +"\t%d" %(count))
#+end_src



#+begin_src shell
cd /home/hadoop/hadoop-3-3.1/sbin
./start-dfs.sh
./start-yarn.sh
jps
#+end_src

word count example
#+begin_src sh
  hdfs daf -put /home/si/Documents/hpda/hpda04-2.3.txt /
  hadoop fs -rm -r /hpda04-2.3-output/
  hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar wordcount /hpda04-2.3.txt /hpda04-2.3-output/
  hadoop fs -cat /hpda04-2.3-output/part-r-00000
  cd output
  hadoop fs -getmerge /hpda04-2.3-output/ out
#+end_src

With errors
#+begin_src shell
  yarn jar share/hadoop/tools/lib/hadoop-streaming-3.3.1.jar -Dmapred.reduce.tasks=1 -Dmapred.map.tasks=11 --mapper /home/si/Documents/hpda/04/mapper.py -reducer /home/si/Documents/hpda/04/reducer.py -input /hpda04-2.3.txt --output /hpda04-2.3-output/
#+end_src

** 05
#+begin_src python :results output
import csv

class dataflow:
    def __init__(self):
        self.data = []

    def read(filename):
        d = dataflow()
        with open(filename, newline='') as csvfile:
            spamreader = csv.reader(csvfile)
            for row in spamreader:
                d.data.append(row)
        return d

    def map(self, func):
        d = dataflow()
        for x in self.data:
            d.data.append(func(x))
        return d

    def filter(self, func):
        d = dataflow()
        for x in self.data:
            if func(x):
                d.data.append(x)
        return d

    def write(self, filename):
        d = dataflow()
        with open(filename, 'w', newline='') as csvfile:
            spamwriter = csv.writer(csvfile, quoting=csv.QUOTE_MINIMAL)
            for d in self.data:
                spamwriter.writerow(d)
        return d
    def __str__(self):
        return str(self.data)


d = dataflow.read("/home/si/Documents/hpda/05/file.csv")
print(d)
flat = d.map(lambda t: (t[0], eval(t[3])))
bd = flat.filter(lambda t: "HPDA" in t[1])
bd.write("/home/si/Documents/hpda/05/out.csv")

#+end_src

#+RESULTS:
: [['4711', 'Max Musterman', 'max.musterman@uni-goettingen.de', "['HPDA', 'MODULE2'] "], ['4710', 'Max musterman', 'max.musterman@uni-goettingen.doe', "['HPDA'] "], ['4712', 'Max Musterman', 'max.musterman@uni-goettingen.de', "['MODULE2'] "], ['4713', 'Max musterman', 'max.musterman@uni-goettingen.doe', "['HPDA'] "], ['4714', 'Max musterman', 'max.musterman@uni-goettingen.doe', "['HPDA'] "], ['4715', 'Max Musterman', 'max.musterman@uni-goettingen.de', "['MODULE2'] "], ['4716', 'Max musterman', 'max.musterman@uni-goettingen.doe', "['HPDA'] "]]

** 06
MongoDB
#+begin_src mongo :db testdatabase
  show dbs
#+end_src

#+RESULTS:
: admin         0.000GB
: config        0.000GB
: local         0.000GB
: testdatabase  0.000GB

#+begin_src mongo :db testdatabase
  use testdatabase
  db.getCollectionNames()
#+end_src

#+RESULTS:
: switched to db testdatabase
: [ "testColl" ]


#+begin_src  mongo :db testdatabase
  use testdatabase;
  db.wiki.drop();
  db.createCollection("wiki");
  show collections;
#+end_src

#+RESULTS:
: switched to db testdatabase
: true
: { "ok" : 1 }
: testColl
: wiki


#+begin_src  mongo :db testdatabase
  use testdatabase;
  db.wiki.insert({_id:1, "person":"Gauss","Beruf":"Mathematiker" })
  db.wiki.find()
#+end_src

#+RESULTS:
: switched to db testdatabase
: WriteResult({ "nInserted" : 1 })
: { "_id" : 1, "person" : "Gauss", "Beruf" : "Mathematiker" }


#+begin_src  mongo :db testdatabase
  use testdatabase;
  db.wiki.update({"person":"Gauss"},{"Beruf": "Mathematiker Physiker" })
  db.wiki.find()
#+end_src

#+RESULTS:
: switched to db testdatabase
: WriteResult({ "nMatched" : 1, "nUpserted" : 0, "nModified" : 1 })
: { "_id" : 1, "Beruf" : "Mathematiker Physiker" }



#+begin_src  mongo :db testdatabase
  use testdatabase;
  db.wiki.update({"person":"Gauss"}, {"Beruf": "Mathematiker Physiker", "Wohnsite": "Göttingen Hannover"})
  db.wiki.find()
#+end_src

#+RESULTS:
: switched to db testdatabase
: WriteResult({ "nMatched" : 0, "nUpserted" : 0, "nModified" : 0 })
: { "_id" : 1, "Beruf" : "Mathematiker Physiker" }




#+begin_src  mongo :db testdatabase
  use testdatabase;
  db.wiki.drop()
#+end_src

#+RESULTS:
: switched to db testdatabase
: true


* Paralle compuation
** performance
***  Andel's law: 
$$ S_{total} = \frac{1}{1-p+\frac{p}{s}} $$
$$S = \frac{s}{1-P_{B}-P_{D} + \frac{P_{B}}{N_{B}} + \frac{P_{D}}{N_{D}}} $$
 

*** if the task is changed, Gostafan's law,
$$ s_{g} = \frac{T_{s} + p T_{p}}{T_{s} + T_{p}} $$


$$ S = \frac{s_g}{(s_g - P_p) + \frac{P_p}{N_p}}$$

all $P_{p}$ is changed task, such as 70% task doubled, will be 1.4


*** Effectivy:
$$ E = \frac{S}{P}$$

$$ S = \frac{T_{s}}{T_{p}} = \frac{n}{\frac{n}{p}+ \log_{2} p}$$


