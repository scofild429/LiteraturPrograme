#+title: AIEngineering
#+OPTIONS: num:t
#+STARTUP: overview
#+EXPORT_FILE_NAME: /home/si/Dropbox/Base/html/AIEnginerring.html
#+PROPERTY: header-args :eval no-export
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="https://gongzhitaao.org/orgcss/org.css"/>

* Background
** AI development
- First time: proposed by Alen Turing
- Second time: IBM deep Blue for chase
- Third time: GPT, Generative pre-trained transformer
** LLM
- pretraing: big unlabeled data
- fine-turing: small labeled data
- tool: an extremely advanced autocomplete
- limit: Inability to Distinguish Fact from Fiction

- Temperature
- Top-P
- Max Token
- Frequency Penalty: Avoiding Repetition
- Presence Penalty: Encouraging Novelty
- Stop Sequences: Structuring Responses

** Prompt engineering
 Principles:
 - write clear and specific instructions
   1. use delimiters
   2. ask for structured ouput
   3. check the satified conditions
   4. with few-shot prompting
      
 - give the model time to think

 - capabilities:
   - iteration
   - summary with specified aspects
   - infering for sentiment
   - transformering
     Proofread and correct the following text  and rewrite the corrected version. If you don't find   and errors, just say "No errors found"

  
- Include details in your query to get more relevant answers
- Ask the model to adopt a persona
- Use delimiters to clearly indicate distinct parts of the input
- Specify the steps required to complete a task
- Ask for a specific output format or length
- Instruct to check whether conditions are satisfied
- Provide examples (Few-shot prompting)

  
  
* Vector Database
** Comparing

- Milvus Lite, FAISS, ChromaDB   →  vector engine, embedded vector databases

- Milvus Standalone, ChromaDB  → single-node production database

- Milvus Cluster                            → scalable production infrastructure

#+BEGIN_TABLE
| System              | Embedded | Server Required | Persistence | Metadata | Distributed | GPU | Production Use |
|---------------------+----------+-----------------+-------------+----------+-------------+-----+----------------|
| FAISS               | Yes      | No              | Manual      | No       | No          | Yes | No             |
| ChromaDB (embedded) | Yes      | No              | Yes         | Yes      | No          | No  | No             |
| ChromaDB (server)   | No       | Yes (optional)  | Yes         | Yes      | No          | No  | Limited        |
| Milvus Lite         | Yes      | No              | Yes         | Limited  | No          | No  | No             |
| Milvus Standalone   | No       | Yes             | Yes         | Yes      | No          | No  | Limited        |
| Milvus Cluster      | No       | Yes             | Yes         | Yes      | Yes         | Yes | Yes            |
#+END_TABLE

** FAISS

** ChromaDB

** Milvus
*** Pymilvus
Embended
*** Standalone
#+begin_src yaml
  version: '3.5'

services:
  etcd:
    container_name: milvus-etcd
    image: quay.io/coreos/etcd:v3.5.14
    environment:
      - ETCD_AUTO_COMPACTION_MODE=revision
      - ETCD_AUTO_COMPACTION_RETENTION=1000
      - ETCD_QUOTA_BACKEND_BYTES=4294967296
      - ETCD_SNAPSHOT_COUNT=50000
    volumes:
      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/etcd:/etcd
    command: etcd -advertise-client-urls=http://127.0.0.1:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd
    healthcheck:
      test: ["CMD", "etcdctl", "endpoint", "health"]
      interval: 30s
      timeout: 20s
      retries: 3

  minio:
    container_name: milvus-minio
    image: minio/minio:RELEASE.2023-03-20T20-16-18Z
    environment:
      MINIO_ACCESS_KEY: minioadmin
      MINIO_SECRET_KEY: minioadmin
    ports:
      - "9001:9001"
      - "9000:9000"
    volumes:
      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/minio:/minio_data
    command: minio server /minio_data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  standalone:
    container_name: milvus-standalone
    image: milvusdb/milvus:v2.5.0
    command: ["milvus", "run", "standalone"]
    security_opt:
    - seccomp:unconfined
    environment:
      ETCD_ENDPOINTS: etcd:2379
      MINIO_ADDRESS: minio:9000
    volumes:
      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/milvus:/var/lib/milvus
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9091/healthz"]
      interval: 30s
      start_period: 90s
      timeout: 20s
      retries: 3
    ports:
      - "19530:19530"
      - "9091:9091"
    depends_on:
      - "etcd"
      - "minio"

networks:
  default:
    name: milvus
#+end_src

#+begin_src sh
  docker-compose up -d
#+end_src
*** Cluster

#+begin_src yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
  - role: control-plane
  - role: worker
  - role: worker
  - role: worker
  - role: worker
#+end_src

#+begin_src sh
  kind create clueter --config cluster.yaml
#+end_src

#+begin_src
helm repo add milvus https://zilliztech.github.io/milvus-helm/
helm repo update
  
helm install my-milvus milvus/milvus \
  --set cluster.enabled=true \
  --set etcd.replicaCount=3 \
  --set pulsar.enabled=false \
  --set pulsarv3.enabled=false \
  --set kafka.enabled=true \
  --set kafka.replicaCount=3 \
  --set minio.mode=distributed
  
#+end_src

#+begin_src
  # Scale down all Deployments (Stateless stuff like Proxies)
kubectl scale deployment -l app.kubernetes.io/instance=my-milvus --replicas=0

# Scale down all StatefulSets (Stateful stuff like Kafka, Etcd, DataNodes)
kubectl scale statefulset -l app.kubernetes.io/instance=my-milvus --replicas=0
  
#+end_src


#+begin_src
helm uninstall my-milvus  
kubectl delete pvc -l release=my-milvus
kubectl delete pvc -l app.kubernetes.io/instance=my-milvus
#+end_src

*** Debug
#+begin_src python
import time
import logging
from pymilvus import connections, utility, MilvusException

# 1. Configure detailed logging to see where it hangs
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("milvus_debug")

def diagnose_milvus():
    print("\n[1/4] Attempting TCP Connection to localhost:19530...")
    try:
        # Set a short timeout (5 seconds) so we don't wait forever
        connections.connect("default", host="localhost", port="19530", timeout=5)
        print("✅ TCP Connection Successful.")
    except Exception as e:
        print(f"❌ Connection Failed. Is 'kubectl port-forward' running?\nError: {e}")
        return

    print("\n[2/4] Checking Server Version (Health Check)...")
    try:
        # This tests if the Proxy can talk to the Root Coordinator
        ver = utility.get_server_version()
        print(f"✅ Server is responsive. Version: {ver}")
    except Exception as e:
        print(f"❌ Server Unresponsive. The Proxy is up, but the RootCoord is down.\nError: {e}")
        return

    print("\n[3/4] Checking dependencies (Etcd)...")
    try:
        # Listing collections requires reading from Etcd
        cols = utility.list_collections()
        print(f"✅ Etcd is readable. Existing collections: {cols}")
    except Exception as e:
        print(f"❌ Read Error. Etcd might be unreadable.\nError: {e}")
        return

    print("\n[4/4] Attempting to Create a Tiny Collection (Write Check)...")
    test_col = "debug_probe_collection"
    try:
        if utility.has_collection(test_col):
            utility.drop_collection(test_col)
        
        # We create a collection with NO index first to test raw creation (Kafka/Etcd)
        from pymilvus import FieldSchema, CollectionSchema, DataType, Collection
        
        dim = 4
        fields = [
            FieldSchema(name="pk", dtype=DataType.INT64, is_primary=True, auto_id=False),
            FieldSchema(name="v", dtype=DataType.FLOAT_VECTOR, dim=dim)
        ]
        schema = CollectionSchema(fields, "Probe collection")
        
        # If this hangs, KAFKA is usually the culprit
        print("   >>> Sending create request (this usually hangs if Kafka is broken)...")
        collection = Collection(test_col, schema)
        print(f"✅ Write Success. Collection '{test_col}' created.")
        
        utility.drop_collection(test_col)
        print("✅ Cleanup Success.")

    except Exception as e:
        print(f"❌ Write Error. This usually means Kafka (Message Queue) is down.\nError: {e}")

if __name__ == "__main__":
    diagnose_milvus()
#+end_src
*** test
#+begin_src python
from pymilvus import MilvusClient, DataType
import random

# 1. Connect to Milvus (Localhost via Port-Forward)
client = MilvusClient(
    uri="http://localhost:19530"
)

# 2. Create a Collection
collection_name = "kind_test_collection"
if client.has_collection(collection_name):
    client.drop_collection(collection_name)

client.create_collection(
    collection_name=collection_name,
    dimension=5,  # Small vector dimension for testing
    metric_type="COSINE"
)

print(f"Collection '{collection_name}' created successfully.")

# 3. Insert Mock Data (Vectors)
data = [
    {"id": i, "vector": [random.random() for _ in range(5)], "color": "red" if i % 2 == 0 else "blue"}
    for i in range(100)
]

res = client.insert(
    collection_name=collection_name,
    data=data
)

print(f"Inserted {res['insert_count']} entities.")

# 4. Perform a Vector Search
# Generate a random query vector
query_vector = [random.random() for _ in range(5)]

search_res = client.search(
    collection_name=collection_name,
    data=[query_vector],
    limit=3,
    search_params={"metric_type": "COSINE", "params": {}}, 
    output_fields=["color"]
)

print("\nSearch Results (Top 3 Nearest Neighbors):")
for hits in search_res:
    for hit in hits:
        print(f"ID: {hit['id']}, Score: {hit['distance']:.4f}, Color: {hit['entity'].get('color')}")
#+end_src
** Qdrant(distributed)
*** Cluster
#+begin_src yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
  - role: control-plane
  - role: worker
  - role: worker
  - role: worker
  - role: worker
#+end_src

#+begin_src sh
  kind create clueter --config cluster.yaml
#+end_src

*** Docker-compose
initial
#+begin_src yaml
version: '3.8'

services:
  # Node 1: The Leader (Starts first)
  qdrant-1:
    image: qdrant/qdrant:v1.16.3
    container_name: qdrant-1
    ports:
      - "6333:6333"
    environment:
      - QDRANT__CLUSTER__ENABLED=true
    # Explicitly tell Node 1 its own address
    command: ["./qdrant", "--uri", "http://qdrant-1:6335"]
    volumes:
      - ./storage-1:/qdrant/storage

  # Node 2: The Joiner
  qdrant-2:
    image: qdrant/qdrant:v1.16.3
    container_name: qdrant-2
    ports:
      - "6334:6333"  # Different host port for testing
    environment:
      - QDRANT__CLUSTER__ENABLED=true
    # FIXED: We use flags instead of Env Vars for reliability
    # 1. --bootstrap: "Go ask Node 1 for data"
    # 2. --uri: "My name is qdrant-2"
    command: ["./qdrant", "--bootstrap", "http://qdrant-1:6335", "--uri", "http://qdrant-2:6335"]
    volumes:
      - ./storage-2:/qdrant/storage
    depends_on:
      - qdrant-1

  # Node 3: The Joiner
  qdrant-3:
    image: qdrant/qdrant:v1.16.3
    container_name: qdrant-3
    ports:
      - "6335:6333"
    environment:
      - QDRANT__CLUSTER__ENABLED=true
    command: ["./qdrant", "--bootstrap", "http://qdrant-1:6335", "--uri", "http://qdrant-3:6335"]
    volumes:
      - ./storage-3:/qdrant/storage
    depends_on:
      - qdrant-1

#+end_src

add new node
#+begin_src sh
   docker run -d
   --name qdrant-4
   --network qdrant_default
   -p 6338:6333
   -v ./storage-4:/qdrant/storage
   -e QDRANT__CLUSTER__ENABLED=true
   qdrant/qdrant:v1.16.3   ./qdrant --bootstrap http://qdrant-1:6335 --uri http://qdrant-4:6335
#+end_src

to remove node, do not forget to remove the mapped volume 

*** Kubernetes
#+begin_src yaml
apiVersion: v1
kind: Service
metadata:
  name: qdrant-headless
  labels:
    app: qdrant
spec:
  clusterIP: None  # <--- This makes it "Headless"
  ports:
  - port: 6333
    name: http
  - port: 6334
    name: grpc
  - port: 6335
    name: p2p      # The internal communication port
  selector:
    app: qdrant
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: qdrant
spec:
  serviceName: "qdrant-headless"
  replicas: 5
  selector:
    matchLabels:
      app: qdrant
  template:
    metadata:
      labels:
        app: qdrant
    spec:
      containers:
      - name: qdrant
        image: qdrant/qdrant:v1.16.3
        ports:
        - containerPort: 6333
        - containerPort: 6335
        env:
        # 1. Enable Distributed Mode
        - name: QDRANT__CLUSTER__ENABLED
          value: "true"
        # 2. Get the Pod Name (e.g., qdrant-0) dynamically
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        # 3. Construct the Bootstrap URL (Everyone points to node 0)
        - name: BOOTSTRAP_PEER
          value: "http://qdrant-0.qdrant-headless:6335"
        
        # The Startup Command
        command:
        - /bin/bash
        - -c
        - |
          # Construct my own full DNS name (e.g., http://qdrant-1.qdrant-headless:6335)
          MY_URI="http://${POD_NAME}.qdrant-headless:6335"
          
          echo "My URI: $MY_URI"
          
          if [[ "$POD_NAME" == "qdrant-0" ]]; then
            echo "I am the Leader (Node 0). Starting without bootstrap..."
            ./qdrant --uri "$MY_URI"
          else
            echo "I am a Follower. Bootstrapping from $BOOTSTRAP_PEER..."
            ./qdrant --bootstrap "$BOOTSTRAP_PEER" --uri "$MY_URI"
          fi
          
        volumeMounts:
        - name: qdrant-storage
          mountPath: /qdrant/storage
  
  # Request 10GB of disk for each pod
  volumeClaimTemplates:
  - metadata:
      name: qdrant-storage
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 10Gi
    

#+end_src
