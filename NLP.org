#+TITLE:  NLP
#+OPTIONS: num:t
#+STARTUP: overview
* Data process
** Data load
#+begin_src python  :results output :session mydata
  import nltk
  nltk.download('punkt')

  with open('/home/si/Desktop/textPreprocessing/frankenstein.txt') as f:
       frankensteintext = f.read()
  sentences = nltk.sent_tokenize(frankensteintext)
  print(f'the length of frankenstein txt is {len(sentences)}!')  
#+end_src

#+RESULTS:
: [nltk_data] Error loading punkt: <urlopen error [Errno -3] Temporary
: [nltk_data]     failure in name resolution>
: the length of frankenstein txt is 3430!


** Alpha filter
#+begin_src python :results output :session mydata
    AlphaFilter_words = nltk.word_tokenize(sentences[0])
    AlphaFilter_words = [token for token in AlphaFilter_words if token[0].isalpha()]
    print(f'the length of the first sentence is {len(AlphaFilter_words)}')
    print(f'the word is {AlphaFilter_words}')
#+end_src

#+RESULTS:
: the length of the first sentence is 26
: the word is ['Project', 'Gutenberg', 'Frankenstein', 'by', 'Mary', 'Wollstonecraft', 'Godwin', 'Shelley', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever']

** Stop words
#+begin_src python :results output :session mydata
    import nltk
    nltk.download('stopwords')
    from nltk.corpus import stopwords
    from nltk.tokenize import word_tokenize
    stopwords = stopwords.words("english")
    print(f"the length of stop words is {len(stopwords)}")

    words = word_tokenize(sentences[0])
    words = [token for token in words if not token in stopwords]
    print(f'the length of the first sentence is {len(words)}')
    print(f'the word is {words}')

#+end_src

#+RESULTS:
: [nltk_data] Downloading package stopwords to /home/si/nltk_data...
: [nltk_data]   Package stopwords is already up-to-date!
: the length of stop words is 179
: the length of the first sentence is 21
: the word is ['Project', 'Gutenberg', "'s", 'Frankenstein', ',', 'Mary', 'Wollstonecraft', '(', 'Godwin', ')', 'Shelley', 'This', 'eBook', 'use', 'anyone', 'anywhere', 'cost', 'almost', 'restrictions', 'whatsoever', '.']
** Lemmatization
#+begin_src python :results output :session mydata
    nltk.download("wordnet")
    from nltk.stem import PorterStemmer
    from nltk.stem import WordNetLemmatizer
    from nltk.tokenize import word_tokenize
    import spacy

    ps = PorterStemmer()
    stemmed_words = []
    words = word_tokenize(sentences[0])
    for token in words:
        stemmed_words.append(ps.stem(token))
    print(f'the length of the first sentence is {len(stemmed_words)}')
    print(f'the word is {stemmed_words}')

#+end_src

#+RESULTS:
: [nltk_data] Downloading package wordnet to /home/si/nltk_data...
: [nltk_data]   Package wordnet is already up-to-date!
: the length of the first sentence is 31
: the word is ['project', 'gutenberg', "'s", 'frankenstein', ',', 'by', 'mari', 'wollstonecraft', '(', 'godwin', ')', 'shelley', 'thi', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyon', 'anywher', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrict', 'whatsoev', '.']
** Stemming
#+begin_src python :results output :session mydata
  nlp = spacy.load('en_core_web_sm')
  doc = nlp(sentences[0])
  lemma_words = [token.lemma_ for token in doc]
  print(f'the length of the first sentence is {len(lemma_words)}')
  print(f'the word is {lemma_words}')
#+end_src

#+RESULTS:
: the length of the first sentence is 33
: the word is ['Project', 'Gutenberg', "'s", 'Frankenstein', ',', 'by', 'Mary', 'Wollstonecraft', '(', 'Godwin', ')', 'Shelley', '\n\n', 'this', 'eBook', 'be', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', '\n', 'almost', 'no', 'restriction', 'whatsoever', '.']
** Part of speech Tagging
#+begin_src python :results output :session mydata
  import spacy
  from spacy.lang.en.examples import sentences

  nlp = spacy.load('en_core_web_sm')
  doc = nlp(sentences[0])

  print(doc.text)

  for token in doc:
      print(token.text, token.pos_, token.dep_)
#+end_src

#+RESULTS:
#+begin_example
Apple is looking at buying U.K. startup for $1 billion
Apple PROPN nsubj
is AUX aux
looking VERB ROOT
at ADP prep
buying VERB pcomp
U.K. PROPN dobj
startup NOUN dep
for ADP prep
$ SYM quantmod
1 NUM compound
billion NUM pobj
#+end_example
** Named Entity Recognition (NER)
#+begin_src python :results output :session mydata
  print(doc.text)

  for token in doc.ents:
      print(token.text, token.label_)
#+end_src

#+RESULTS:
: Apple is looking at buying U.K. startup for $1 billion
: Apple ORG
: U.K. GPE
: $1 billion MONEY
** Frequency Analysis
#+begin_src python :results output :session mydata
  import nltk
  from nltk import FreqDist

  # Assuming 'tokens' are already generated from the text
  print(sentences[0])
  freq_dist = FreqDist(sentences[0])
  print(freq_dist.most_common(10))

  words = word_tokenize(sentences[0])
  print(words)
  freq_dist = FreqDist(words)
  print(freq_dist.most_common(10))

#+end_src

#+RESULTS:
: Project Gutenberg's Frankenstein, by Mary Wollstonecraft (Godwin) Shelley
: 
: This eBook is for the use of anyone anywhere at no cost and with
: almost no restrictions whatsoever.
: [(' ', 23), ('e', 17), ('o', 15), ('t', 13), ('n', 13), ('s', 11), ('r', 10), ('a', 9), ('i', 7), ('h', 6)]
: ['Project', 'Gutenberg', "'s", 'Frankenstein', ',', 'by', 'Mary', 'Wollstonecraft', '(', 'Godwin', ')', 'Shelley', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', '.']
: [('no', 2), ('Project', 1), ('Gutenberg', 1), ("'s", 1), ('Frankenstein', 1), (',', 1), ('by', 1), ('Mary', 1), ('Wollstonecraft', 1), ('(', 1)]

** one-hot coding
all input should numerical,
categorized character shoud be one-hot coded, starting with 1
** Tokenizationn
- *Breaking* text to words (There are many steps to consider )
- *CountWordFrequencies*  (counted key-value dictionary)
  + list all sorted dictionary
  + if the list is too big, removing infrequent words(because of incorrection, or neame...) good for one-hot coding
- *encode* to sequences
  + with counted dictionary index
  + index length is one-hot coding vector length
- *one-hot* coding all sequences
  +  if one-hot code vector is not so lang, word embedding is not needed
#+begin_src
  tests[5] = "this is a cat and a"
  tests_dict = {"this": {1:1}, "is": {2:1}, "a":{3:2}, "cat":{4:1}, "and":{5:1}}
  tests_sequences = [1,2,3,4,3]
#+end_src

*** Tokenize with nltk
#+begin_src python  :results output :session mydata
    tokenize_words = nltk.word_tokenize(sentences[0])
    print(f'the length of the first sentence is {len(tokenize_words)}')
    print(f'the word is {tokenize_words}')
#+end_src

** Word Embedding
compose high dimension one-hot vector to low dimension
$$ X_{i}    = P^{T} \cdot e^{i}$$
$e^{i}$ is high dimensional vector after one-hot coding(v,1) of collected data
$P^{T}$ is the parameter matrix trained by data(d,v),
$X_{i}$ is low dimensional vector(d,1), for further training
$d$ :  The dimension parameter d  is important, can be vertified with corss validation.
Each row of $P$  is called (words vector词向量), can be interpreted with classification

*Embedding* layer need the number of vocabulary(v), embedding_dim(d), and word_num(cuted words number)
v*d : parameters for this layer

** Words Count



** TF-IDF for weight title and body
the idea is to give weights for title and text. Afterwards the title has a huge impact for document = body + title
$TF-IDF(document) = TF-IDF(title) * alpha + TF-IDF(body) * (1-alpha)$
** Vectorizing tf-idf




* language model
A model that computes $P(W)$ or $P(W_{n}|W_{1},W_{2},W_{3},W_{4}....W_{n-1})$ is called  a language model.



** skip-gram
** CBOW
continuous bag of words

* text generatation
Encoder:
A is RNN layer or LMST layer,
all input(x1 to xm) share the same A,
hm is the last result,
only give hm to decoder, we can generate text,
but many content of input will be forget
* seq2seq
After one resulte in Decode is generated,
With Corss Enteopy to update the Network,
using all the resulte we get, to predict the next resulte until all is finished
consuming the previously generated symbols as additional input when generating the next.
* Transformer
** simple RNN + attention 
Encoder Input  E$X = x_{1}, x_{2},,,,x_{m}$
Decoder Input  $X^{'} = x_{1}^{'}, x_{2}^{'},,,,x_m^{'}$
after RNN or LSTM we get $H = h_{0}, h_{1},,,,,h_{m}$
Now unlike before only pass the last element $h_{m}$  to Decoder,
we use attention skill to mix all input information
1. Notion:
   - Encoder, lower index $i$ stands for the index of input order in Encoder
   - Decoder, high index $j$ stands for the index of generated items in Decoder
   $a^{j}_{i}$ stands for the parameter for generate the j-th item ($s_j$)in Decoder with respect of the i-th input($x_{i}$) in X.
2. Variables
   - Encoder input,   $X = x_{1}, x_{2},,,x_{m}$  , 
   - Encoder  shared parameter,   A: RNN or LSMT shared parameter
   - Encoder output ,   $H: = h_{1}, h_{2},,,h_{m}$   output at each step of RNN or LSMT
   -  Decoder initial input   $h_{m}$ ,  denote also as $s^{0}$
   - key, $q_{i}^{j} = W_{q}^{j} s^{i}$
   - query $k_{i}^{j} = W_{k}^{j} h_{i}$
   - Query Martix,  $K^{j} = [k_{i}^{j}, k_{2}^{j},,,k_{m}^{j}]$
   - Encoder Weight  $a^{j}_{i}$,   $a^{j}_{i} = Softmax(K^{jT} q_{i})$
   - Eecoder Context Vector, $c^{j} = a_{1}^{j}h_{1} + a_{2}^{j}h_{2}+,,,,+a_{m}^{j}h_{m}$
   -  Decoder initial input   $h_{m}$ ,  denote also as $s^{0}$
   - Decoder output, $s^{j} = \tanh(A^{'}\cdot [x^{'j}, s_{j-1}, c_{j-1}]^{T})$
3. update Network
       with softmax(c)  get the prediciton, and corss enteopy update network back($W^{j} -> W^{j+1}$)

** simple RNN + self attention
only Encoder, e$X = x_{1}, x_{2},,,,x_{m}$
Without Decoder and Decoder input, 
after RNN or LSTM we get $H = h_{0}, h_{1},,,,,h_{m}$
Now unlike before only pass the last element $h_{m}$  to Decoder,
we use attention skill to mix all input information
1. Notion:
   - Encoder, lower index $i$ stands for the index of input order in Encoder
   - Generation, high index $j$ stands for the index of generated items 
   $a^{j}_{i}$ stands for the parameter for generate the j-th item ($s_j$)in Encoder with respect of the i-th input($x_{i}$) in X.
2. Variables
   - Encoder input,   $X = x_{1}, x_{2},,,x_{m}$  , 
   - Encoder  shared parameter,   A: RNN or LSMT shared parameter
   - Encoder output ,   $H: = h_{1}, h_{2},,,h_{m}$   output at each step of RNN or LSMT
   - key, $q_{i}^{j} = W_{q}^{j} h_{i}$
   - query $k_{i}^{j} = W_{k}^{j} h_{i}$
   - Query Martix,  $K^{j} = [k_{i}^{j}, k_{2}^{j},,,k_{m}^{j}]$
   - Encoder Weight  $a^{j}_{i}$,   $a^{j}_{i} = Softmax(K^{jT} q_{i})$
   - Eecoder Context Vector, $c^{j} = a_{1}^{j}h_{1} + a_{2}^{j}h_{2}+,,,,+a_{m}^{j}h_{m}$

3. update Network
     - with softmax(c)  get the prediciton, and corss enteopy update network back($W^{j} -> W^{j+1}$)
4. Note       
    - attention:  key, $q_{i}^{j} = W_{q}^{j} s^{i}$ with  $s^{j} = \tanh(A^{'}\cdot [x^{'j}, s_{j-1}, c_{j-1}]^{T})$
    - self attention: key, $q_{i}^{j} = W_{q}^{j} h_{i}$

** attention layer
An attention function can be described as mapping a query and a set of key-value pairs to an output
Encoder Input  E$X = x_{1}, x_{2},,,,x_{m}$
Decoder Input  $X^{'} = x_{1}^{'}, x_{2}^{'},,,,x_m^{'}$
Removing RNN or LSMT, only constructing attention layer
1. Notion:
   - Encoder, lower index $i$ stands for the index of input order in Encoder
   - Decoder, high index $j$ stands for the index of generated items in Decoder
   $a^{j}_{i}$ stands for the parameter for generate the j-th item ($s_j$)in Decoder with respect of the i-th input($x_{i}$) in X.
2. Variables
   - value, $v_{i}^{j} = W_{v}^{j} x_{i}$
   - query $k_{i}^{j} = W_{k}^{j} x_{i}$ 
   - key, $q_{i}^{j} = W_{q}^{j} x_{'i}$
   - Query Martix,  $K^{j} = [k_{i}^{j}, k_{2}^{j},,,k_{m}^{j}]$
   - Encoder Weight  $a^{j}_{i}$,   $a^{j}_{i} = Softmax(K^{jT} q_{i})$
   - Eecoder Context Vector, $c^{j} = a_{1}^{j}v_{1}^{j} + a_{2}^{j}v_{2}^{j}+,,,,+a_{m}^{j}v_{m}^{j}$
3. update Network
   -  with softmax(c)  get the prediciton, and corss enteopy update network back($W^{j} -> W^{j+1}$)
4. Note
    - X replace H, but still seq2seq model(with X')

** self attention layer
only Encoder, e$X = x_{1}, x_{2},,,,x_{m}$
Without Decoder and Decoder input, 
1. Notion:
   - Encoder, lower index $i$ stands for the index of input order in Encoder
   - Generation, high index $j$ stands for the index of generated items 
   $a^{j}_{i}$ stands for the parameter for generate the j-th item ($s_j$)in Encoder with respect of the i-th input($x_{i}$) in X.
2. Variables
   - Encoder input,   $X = x_{1}, x_{2},,,x_{m}$  , 
   - value, $v_{i}^{j} = W_{v}^{j} x_{i}$
   - key, $q_{i}^{j} = W_{q}^{j} x_{i}$
   - query $k_{i}^{j} = W_{k}^{j} x_{i}$
   - Query Martix,  $K^{j} = [k_{i}^{j}, k_{2}^{j},,,k_{m}^{j}]$
   - Encoder Weight  $a^{j}_{i}$,   $a^{j}_{i} = Softmax(K^{jT} q_{i})$
   - Eecoder Context Vector, $c^{j} = a_{1}^{j}v_{1}^{j} + a_{2}^{j}v_{2}^{j}+,,,,+a_{m}^{j}v_{m}^{j}$

3. update Network
     with softmax(c)  get the prediciton, and corss enteopy update network back($W^{j} -> W^{j+1}$)
4. Note
   - in query $k_{i}^{j} = W_{k}^{j} x_{i}$, it's X , not X'
** transformer model

#+begin_src ditaa :file foto/RNN_attention.png
    /--+--\    /-----\    /-----\    /-----\   
    |a_1  |    |a_2  |    |a_3  |    |a_m  |  
    |     |    |     |    |     |    |     |
    \-----/    \-----/    \-----/    \-----/

  +-------------------------------------------+
  |cBLU                                       |
  |Encoder                                    |
  |    h1         h2         h3        hm     |
  | +------+   +-----+    +-----+   +-----+   |
  | |      |   |     |    |     |   |     |   |>------\
  | | A    |   | A   |    | A   |   | A   |   |       |
  | +--+---+   +--+--+    +--+--+   +---+-+   |       |
  |    ^          ^          ^          ^     |       |
  +----+----------+----------+----------+-----+       |
       |          |          |          |             |
    /--+--\    /-----\    /-----\    /-----\          |
    |X_1  |    |X_2  |    |X_3  |    |X_m  |          |
    |     |    |     |    |     |    |     |          |
    \-----/    \-----/    \-----/    \-----/          |
                                                      |        /--+--\    /-----\    /-----\    /-----\       
                                                      |        |c_1  |    |c_2  |    |c_3  |    |c_m  |  
                                                      |        |s_1  |    |s_2  |    |s_3  |    |s_m  |
                                                      |        \-----/    \-----/    \-----/    \-----/
                                                      |
                                                      |        +-------------------------------------------+
                                                      |        | c1AB                                      |
                                                      \------->| Decoder                                   |
                                                               | +------+   +-----+    +-----+   +-----+   |
                                                               | |      |   |     |    |     |   |     |   |
                                                               | | A'   |   | A'  |    | A'  |   | A'  |   |
                                                               | +--+---+   +--+--+    +--+--+   +---+-+   |
                                                               |    ^          ^          ^          ^     |
                                                               +----+----------+----------+----------+-----+
                                                                    |          |          |          |	
                                                                 /--+--\    /-----\    /-----\    /-----\ 
                                                                 |X'1  |    |X'2  |    |X'3  |    |X'm  |  
                                                                 |     |    |     |    |     |    |     |
                                                                 \-----/    \-----/    \-----/    \-----/

#+end_src

#+RESULTS:
[[./foto/RNN_attention.png]]
after 6 stacked multi head self attention layers,
another 6 stacked multi head attention layers, each time take the input of 6 self attention layer

* Bert
* ViT
