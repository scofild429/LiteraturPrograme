#+TITLE: k8s
#+OPTIONS: num:t
#+STARTUP: overview
#+EXPORT_FILE_NAME: /home/si/Dropbox/LiteraturPrograme/html/k8s.html
#+PROPERTY: header-args :eval no-export
#+SETUPFILE: https://fniessen.github.io/org-html-themes/org/theme-readtheorg.setup
* Docker
** pull
Docker daemon prefixes docker.io/library/ for default images on Docker Hub when no registry is specified
** build
#+begin_src 
  docker build -t name:tag .
  docker build -t entry-image -f /path/to/custom-dockerfile /path/to/build-context
#+end_src
  -t:  Name and optionally a tag (format: "name:tag")
  -  ADD and COPY can only access files within the build context specified in the docker build command (e.g., /path/to/build-context). Files outside the build context cannot be directly referenced in the Dockerfile.

** tag
#+begin_src
  docker tag my-current-image:tag my-username-in-dockerhub/my-wanted-image-name:mytag
  docker push my-current-image:tag my-username-in-dockerhub/my-wanted-image-name:mytag
#+end_src
tag change the name(with dockerhub name) and the tag for pushing
** Microservices
- Scalability	Each microservice can scale independently based on demand.
- Fault Isolation	Failure in one service does not crash the entire application.
- Faster Deployment	Teams can deploy and update services independently.
- Technology Flexibility	Different services can use different programming languages.
- Easier Maintenance	Smaller codebases are easier to manage and debug.

* Dokcerfile
** EXPOSE
Missing EXPOSE 5000 won’t break your setup if you explicitly map ports with -p 80:5000. It’s primarily a documentation and automation feature. For clarity and best practices, include EXPOSE 5000 in your Dockerfile to indicate the port your application uses.

** CMD echo "Hello World" && CMD ["echo", "Hello World"]
- CMD echo "Hello World":  start a shell and run the command, with shell features
- CMD ["echo", "Hello World"]:  run the command directly, withour to start a shell
- CMD: can be easyly overridden by docker run ...
** ENTRYPORT 
- Can only be explicitly overridden by docker run ... --entryport ...
- Implicity run without --entryport  after docker run... will append arguments at end

** Docker's multi-stage
builds offer a powerful way to manage production environments by separating build and runtime stages, ensuring smaller images, faster deployments, and enhanced security.
* Kubernetes
** Pod
Basic Pod-Related Commands
- Create a Pod: kubectl apply -f pod.yaml
- List all Pods: kubectl get pods
- List Pods with details: kubectl get pods -o wide
- Describe a Pod: kubectl describe pod <pod-name>
- View Pod logs: kubectl logs <pod-name>
- View logs for a specific container: kubectl logs <pod-name> -c <container-name>
- Stream live Pod logs: kubectl logs -f <pod-name>
- Execute a command in a Pod: kubectl exec -it <pod-name> -- /bin/bash
- Execute a command in a specific container: kubectl exec -it <pod-name> -c <container-name> -- /bin/bash
- Delete a Pod: kubectl delete pod <pod-name>
** Labels
- Labels: Key-value pairs attached to Kubernetes objects like pods, enabling logical grouping and selection.

- Kubernetes Features That Use Labels and Selectors:
  - ReplicationController,
  - ReplicaSet,
  - Deployments,
  - StatefulSets,
  - DaemonSets,
  - Services,
  - Jobs,
  - CronJobs,
  - and NetworkPolicies.
** Selectors
- Selectors: Expressions or equality-based filters used to match objects based on their labels, enabling precise control over resource management.
- It only apply to the Pod level(Node or Service), not to the container level

*** Equality-Based
for
- rc,
- rs,
- deployment
  #+begin_src
    selector:
      matchLabels:
        app: nginx
  #+end_src

*** Set-Based
- for rs, and deloyment
- usign: In, NotIn, Exists, DoesNotExist
  #+begin_src
  selector:
    matchExpressions:
      - key: app
        operator: In
        values:
          - nginx
          - apache
      - key: environment
        operator: NotIn
        values:
          - development
      - key: tier
        operator: Exists
      - key: debug
        operator: DoesNotExist
  #+end_src

** ReplicationController
- Ensures a specified number of pod replicas are running at any given time

** ReplicaSet
- ReplicaSet is an improved version of ReplicationController.
- It supports set-based selectors, providing more flexibility in managing pods.
- ReplicaSets are often managed by Deployments, which add advanced capabilities.
** Deployment
-  build on top of ReplicaSets, with rolling update and rollbacks, automated update, scaling and simplified management
*** Annotation
*A*: 
#+begin_src
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    kubernetes.io/change-cause: "Initial release with nginx 1.19"
  name: nginx-deployment
spec:
  ....
#+end_src

*B*: 
after applying the file and check the annoation
#+begin_src
  kubectl rollout history deployment nginx-deployment
#+end_src
I would able to see the default annotation.

*C*:
Then I can update my container for next version(only for deployment, not rc and rs)
#+begin_src
  kubectl set image deployment nginx-deployment nginx-container=nginx:1.20
#+end_src
 and set new annotation with
 #+begin_src
   kubectl annotate deployments nginx-deployment kubernetes.io/change-cause="Upgraded my container"
 #+end_src
OR modify my Deployment manifest and apply.
Now verify the history with
#+begin_src
  kubectl rollout history deployment nginx-deployment
#+end_src

 
*D*:
rollback to special version with
#+begin_src
  kubectl rollout undo deployment nginx-deployment --to-revision=1
#+end_src

** Service
- A fixed IP and DNS name so that pods can always be found, even if their individual IPs change.
- Load balancing across multiple pod replicas to distribute traffic evenly.
- Internal and external communication
*** ClusterIP

*** NordPort
*** Loadbalance
*** ExternalName

** Manual scheduling
- Explicitly assigning a pod to a node using the nodeName field in the pod’s YAML manifest
#+begin_src
apiVersion: v1
kind: Pod
metadata:
  name: nginx-manual
spec:
  nodeName: my-second-cluster-worker2  # Assign pod to a specific worker node
  containers:
    - name: nginx
      image: nginx
#+end_src
** Static pods
- Static pods are defined in /etc/kubernetes/manifests/ and  not managed by the Kubernetes API server
- To ensure visibility in kubectl get pods, Kubelet creates a read-only "mirror pod" on the API server

** Node Selector

*** label: Apply label to node
#+begin_src
  kubectl label nodes my-second-cluster-worker storage=ssd
  kubectl label nodes my-second-cluster-worker storage-
#+end_src
*** selector apply to pod/Deployment
#+begin_src
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ns-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: app1
  template:
    metadata:
      labels:
        app: app1
    spec:
      nodeSelector:
        storage: ssd
      containers:
        - name: nginx
          image: nginx
#+end_src

- A node can have multiple labels. A pod will only be scheduled if all labels in nodeSelector match.

*** Takeaway: Pure lebel matching mechanismus,l ack of flexibility, only with key-value matching selection
** Taints and Tolerations
- Taints and Tolerations help control which pods can be scheduled on which nodes
- Taints and tolerations are applied during scheduling

*** Taints Apply to node
  #+begin_src 
    kubectl taint nodes my-second-cluster-worker storage=ssd:NoSchedule
    kubectl taint nodes my-second-cluster-worker2 storage=hdd:NoSchedule
    kubectl taint nodes my-second-cluster-worker storage=ssd:NoSchedule-  
  #+end_src

| Effect           | Behavior                                                                   |
|------------------+----------------------------------------------------------------------------|
| NoSchedule       | New pods will not be scheduled unless they have a matching toleration.     |
| PreferNoSchedule | Kubernetes tries to avoid scheduling pods but doesn’t enforce it strictly. |
| NoExecute        | Existing pods without a matching toleration will be evicted from the node. |

*** Toleration Apply to pod
#+begin_src
  tolerations:
  - key: "storage"
    operator: "Equal"
    value: "ssd"
    effect: "NoSchedule"
#+end_src

- This tolerates nodes that have the taint storage=ssd:NoSchedule, allowing the pod to be scheduled on them.
- The Effect in the toleration must match the Effect in the taint for it to take effect.
*** Takeaway: Lebel matching with hardness configuation, NoExecute supports re-evaluation. If a node’s taints change and a pod doesn’t tolerate them, it will be evicted
** Node Affinity
- requiredDuringSchedulingIgnoredDuringExecution: Hard rule, must match
- preferredDuringSchedulingIgnoredDuringExecution: Soft rule, NOT must
- node labels are the same with node selector
- all requests from matchExpressions need to be filled
#+begin_src
apiVersion: apps/v1
kind: Deployment
metadata:
  name: preferred-na-deploy
spec:
  replicas: 5
  selector:
    matchLabels:
      app: app1
  template:
    metadata:
      labels:
        app: app1
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: storage
                operator: In
                values:
                  - ssd
                  - hdd
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 10
              preference:
                matchExpressions:
                  - key: storage
                    operator: In
                    values:
                      - ssd
            - weight: 5
              preference:
                matchExpressions:
                  - key: storage
                    operator: In
                    values:
                      - hdd
      containers:
        - name: nginx
          image: nginx
#+end_src


| Operator     | Behavior                                                            |
|--------------+---------------------------------------------------------------------|
| In           | Matches if the node’s label exists in the provided values.          |
| NotIn        | Matches if the node’s label does not exist in the provided values.  |
| Exists       | Matches if the node has the specified key, regardless of its value. |
| DoesNotExist | Matches if the node does not have the specified key.                |
| Gt           | Matches if the label’s value is greater than the specified number.  |
| Lt           | Matches if the label’s value is less than the specified number.     |
  


*** Takeaway: The most flexible way to optimize the resource utilization

** Request and limits
- Requests and limits must be defined per container inside a Pod.
- If a container exceeds its requested resources, it can still use more resources if available on the node.
- If a container exceeds its requested CPU resource: CPU is a compressible resource, so workloads slow down instead of failing
- If a container exceeds its requested Memary resource: the kernel may kill it,  But If there is no memory pressure, the container may not be killed immediately
*** Monitoring
Install metrics server on Kubernetes
#+begin_src
  kubectl top nodes
#+end_src

** Scaling
- Scaling refers to adjusting the resources available to an application based on its demand/load.
*** Horizontal Scaling
- Scale-Out: Increasing the number of pod replicas
- Scaling In: Removing instances/pods when load decreases
- Manually: Editing the Deployment YAML (replicas field) and apply
- Automatic: HPA (Horizontal Pod Autoscaler)

  
*** Vertical Scaling
- Scaling Up: Increasing CPU/Memory resources of the existing pod or VM.
- Scaling Down: Reducing CPU/Memory.
- Manually: Editing the Deployment YAML (replicas field) and apply
- Automatic: VPA (Vertical Pod Autoscaler)


*** Node side
clusters rely on horizontal scaling of nodes to maintain availability. vertical scaling requeir restarting, which is not working for production environment

** Horizontal Pod Autoscaler (HPA)
based on the default/custom/External Resource Metrics to automatically scale out/in


** Vertical Pod Autoscaler (VPA)
** Multi-container pods
a primary container complemented by one or more helper containers
*** Init Containers
successfully complete before the primary container runs as pre-requests
#+begin_src
apiVersion: v1
kind: Pod
metadata:
  name: init-demo
spec:
  initContainers:
  - name: check-api
    image: curlimages/curl:latest
    command:
      - sh
      - -c
      - |
        echo 'Checking API availability...'
        sleep 25
        until curl -s https://kubernetes.io > /dev/null; do
          echo 'Waiting for API...'
          sleep 5
        done
        echo 'API is accessible, proceeding to main-app container!'
  containers:
  - name: main-app
    image: nginx:latest
#+end_src
*** Sidecar Pattern
run alongside the primary container and operate independently to extend or complement the main container's functionality.
#+begin_src
apiVersion: v1
kind: Pod
metadata:
  name: sidecar-logging-demo
spec:
  containers:
  # Main application container
  - name: main-app
    image: nginx:latest
    ports:
    - containerPort: 80
  # Sidecar container
  - name: health-logger
    image: curlimages/curl:latest
    command:
    - sh
    - -c
    - |
      while true; do
        curl -s http://localhost:80 > /dev/null && echo 'Main app is healthy' || echo 'Main app is unhealthy'
        sleep 5
      done

#+end_src
*** Ambassador Pattern
like sidecar but only focus on handling external communication
*** Adapter Pattern
like sidecar but only focus on data transforming or normalizing

** Pod Policies
*** Delete policies
- graceful termination for 30 seconds, after that SIGKILL will terminate it immediately if it still lives
  #+begin_src
apiVersion: v1
kind: Pod
metadata:
  name: nginx- graceful
spec:
  terminationGracePeriodSeconds: 30
  containers:
  - name: nginx
    image: nginx:latest
  #+end_src
apply and delete
    #+begin_src
    kubectl delete pod xxx
  #+end_src

  for forceful deletion:
  #+begin_src
    kubectl delete pod mypod --force=true --grace-period=0
  #+end_src

*** Restart Policies
how Kubernetes responds when containers within a pod terminate, it's at the pod level and apply to all containers within the pod
- Always(whether it succeeds with exit code 0 or fails with a non-zero exit code): Pods, Deployments, ReplicaSets, StatefulSets, DaemonSets
- OnFailure (exit code 0): Jobs, Cronjobs
- Never: not restarted, regardless of the exit code (whether it succeeds or fails)

  #+begin_src
apiVersion: batch/v1
kind: Job
metadata:
  name: batch-job-example
spec:
  template:
    spec:
      restartPolicy: OnFailure
      containers:
      - name: processor
        image: my-batch-job:latest
  #+end_src

*** Image Pull Policies
how the container runtime pulls container images for initialization. At Container level for each containter customizable
#+begin_src
apiVersion: v1
kind: Pod
metadata:
  name: always-example
spec:
  containers:
  - name: my-app
    image: myrepo/my-app:latest
    imagePullPolicy: Always
#+end_src

- Always, default, prefer for development
- IfNotPresent, only NOT available on the node cache
- Never, assuming the image is already present on the node.
- if Image tag is latest, default police is Always,
- if image tag is with specific version tag, default police is IfNotPresent

*** Pod Lifecycle and debugging see [[https://github.com/CloudWithVarJosh/CKA-Certification-Course-2025/tree/main/Day%2022][Day 22]]





** health probes
The kubelet is responsible for performing these probes. 
*** Readiness Probes
only health pods receiving traffic, otherwise removed, no restart
*** Liveness Probes
with restarting if failed
*** Startup Probes
set threshold times of attempts before marked as failed for slow-starting

** Ephemeral Storage
*** emptyDir
It is ideal for scratch space (Temporary workspace for processing.), caching, or temporary computations where data persistence isn’t required.

#+begin_src
apiVersion: v1  # Specifies the API version used to create the Pod.
kind: Pod       # Declares the resource type as a Pod.
metadata:
  name: emptydir-example  # Name of the Pod.
spec:
  containers:
  - name: busybox-container  # Name of the container inside the Pod.
    image: busybox           # Using the lightweight BusyBox image.
    command: ["/bin/sh", "-c", "sleep 3600"]  # Overrides the default command. Keeps the container running for 1 hour (3600 seconds).
    volumeMounts:
    - mountPath: /data       # Mount point inside the container where the volume will be accessible.
      name: temp-storage     # Refers to the volume defined in the `volumes` section below.
  - name: busybox-container-2  # Name of the container inside the Pod.
    image: busybox           # Using the lightweight BusyBox image.
    command: ["/bin/sh", "-c", "sleep 3600"]
    volumeMounts:
    - mountPath: /data
      name: temp-storage
  volumes:
  - name: temp-storage       # Name of the volume, must match the name in `volumeMounts`.
    emptyDir: {}             # Creates a temporary directory that lives as long as the Pod exists.
                            # Useful for storing transient data that doesn't need to persist.

#+end_src
all containter by applying this yaml will access the same data as in volume.

*** Downward API
- It injectes into containers in a Pod either as environment variables or through mounted files via volumes
- Just like Sidecar pattern, This helps sidecar containter to get matrics from the main contrainter.
- please refer [[https://github.com/CloudWithVarJosh/CKA-Certification-Course-2025/tree/main/Day%2026][env and file mount at github]]
  


** Volume
[[https://github.com/CloudWithVarJosh/CKA-Certification-Course-2025/blob/main/Day%2027/README.md][link]]
- hostPath: Persistent change map to host machine, the data can be shared between different pods only if they are scheduled on the same node.
- Storage class, a replace of pv
- pv (persistentvolume) clain a  physical  storage(retain, delete) global at cluster level
- pvc(persistentvolumeClaim) use the pv under a namespace, all the charactore muss be passend
- pod, chaim the volume and mount it to somewhere in container
- to delete, pod at first, otherwise pv/pvc hold still until pod died

* Kubectl
** Imperative Commands for Backend Deployment and Service
Use --dry-run=client for local validation and --dry-run=server for API server validation
#+begin_src
  kubectl create deployment backend-deploy \
  --image=hashicorp/http-echo \
  --replicas=3 \
  --port=5678 \
  --dry-run=client -o yaml > backend-deploy.yaml
#+end_src

#+begin_src
  kubectl expose deployment backend-deploy \
  --type=ClusterIP \
  --port=9090 \
  --target-port=5678 \
  --name=backend-svc \
  --dry-run=client -o yaml >> backend-deploy.yaml
#+end_src


** namespace
- Context contains cluster, users, and namespace, they are not coupled.
#+begin_src
  kubectl config set-context --current --namespace=app1-ns
#+end_src
- This command is to change the namespace of a contex.

** config
* Matrics server
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
