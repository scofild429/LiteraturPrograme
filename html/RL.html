<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2026-01-25 So 20:30 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>RL</title>
<meta name="author" content="silin" />
<meta name="generator" content="Org Mode" />
<style type="text/css">
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="stylesheet" type="text/css" href="https://gongzhitaao.org/orgcss/org.css"/>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">RL</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orgcf38b7a">1. Markov Decision Process (MDP)</a></li>
<li><a href="#orga613506">2. Bellman Equation</a></li>
<li><a href="#orgbe5506a">3. Dynamic Programming Iterations in RL</a>
<ul>
<li><a href="#org907509c">3.1. Value Iteration</a></li>
<li><a href="#org1feae5e">3.2. Policy Iteration</a>
<ul>
<li><a href="#org6017ae3">3.2.1. Phase 1: Policy Evaluation (The Inner Loop)</a></li>
<li><a href="#org0d5e75c">3.2.2. Phase 2: Policy Improvement (The Outer Loop)</a></li>
</ul>
</li>
<li><a href="#orgc9e32a6">3.3. Truncated Policy Iteration</a></li>
</ul>
</li>
<li><a href="#orgb811f8f">4. Monte Carlo Methods in RL</a>
<ul>
<li><a href="#orgc3d8f2b">4.1. 1. MC Basic (The Theoretical Baseline)</a></li>
<li><a href="#org2758996">4.2. 2. MC Exploring Starts (The Efficient Simulator)</a></li>
<li><a href="#org9753f3d">4.3. 3. MC Epsilon-Greedy (The Practical Solution)</a>
<ul>
<li><a href="#org0240c57">4.3.1. Core Concept: Soft Policies</a></li>
<li><a href="#org784b29c">4.3.2. Exploration vs. Exploitation</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org8445c2f">5. Temporal Difference</a>
<ul>
<li><a href="#orge079f19">5.1. The Mathematical Driver</a></li>
<li><a href="#org286b27c">5.2. The Derivation</a></li>
<li><a href="#org3501a1f">5.3. Unified View of TD-series</a>
<ul>
<li><a href="#orgb862978">5.3.1. 1. The General Update Rule</a></li>
<li><a href="#orgda232e2">5.3.2. 2. Algorithm Specifics</a></li>
</ul>
</li>
<li><a href="#org2a19c52">5.4. Deep Q-learning</a></li>
</ul>
</li>
<li><a href="#org1e29f57">6. Policy Gradient</a>
<ul>
<li><a href="#org3fdb01d">6.1. Metrics</a></li>
<li><a href="#org0f91c0d">6.2. General objective function for metrics</a></li>
<li><a href="#org1393eee">6.3. Policy parameters update</a></li>
<li><a href="#org5de87d3">6.4. Theory</a></li>
<li><a href="#org7bdad34">6.5. REINFORCE Algorithm</a></li>
</ul>
</li>
<li><a href="#org80ba121">7. Actor-Critic</a>
<ul>
<li><a href="#orgb6dec31">7.1. Q-Actor-Critic (QAC)</a>
<ul>
<li><a href="#orge81dc0a">7.1.1. Background</a></li>
<li><a href="#org3707961">7.1.2. Initialization</a></li>
<li><a href="#org921f054">7.1.3. Loop (for each time step \(t\) in episode):</a></li>
</ul>
</li>
<li><a href="#orgc363376">7.2. Advantage Actor-Critic</a>
<ul>
<li><a href="#org159ac0b">7.2.1. Add baseline</a></li>
<li><a href="#org6bdc051">7.2.2. The Advantage Function (\(\delta_t\))</a></li>
<li><a href="#org5310ac4">7.2.3. Algorithm</a></li>
</ul>
</li>
<li><a href="#org3ec2b2c">7.3. Off-Policy Actor-Critic</a>
<ul>
<li><a href="#org14b7173">7.3.1. The Objective Gradient</a></li>
<li><a href="#orgf36b322">7.3.2. The Update Rule</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org6b58c83">8. RL for LLM</a>
<ul>
<li><a href="#orga2616a3">8.1. Objective function to maximase the Reward</a>
<ul>
<li><a href="#orgaf9f60b">8.1.1. Start from MDP</a></li>
<li><a href="#org3ed8aaa">8.1.2. The Policy Gradient Theorem</a></li>
<li><a href="#org362bc1d">8.1.3. Final Merged Expression for derivate of objective function</a></li>
<li><a href="#orgd534fe6">8.1.4. Advantage function \(A_{t}\)</a></li>
<li><a href="#org9ace333">8.1.5. Value model Loss function</a></li>
</ul>
</li>
<li><a href="#orgd7b58a9">8.2. Vanilla Policy Gradient (VPG / REINFORCE)</a></li>
<li><a href="#orgc69d066">8.3. Trust Region Policy Optimization (TRPO)</a></li>
<li><a href="#orgd74daac">8.4. Proximal Policy Optimization (PPO)</a></li>
<li><a href="#org1cc446d">8.5. Training</a>
<ul>
<li><a href="#org98221de">8.5.1. Using sepreate network</a></li>
<li><a href="#orgc9537bb">8.5.2. Processing</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org0ad63fb">9. GRPO</a>
<ul>
<li><a href="#orge5628c7">9.1. MoE</a></li>
</ul>
</li>
<li><a href="#org38cf18f">10. Multi-token training</a>
<ul>
<li><a href="#orgc35d326">10.1. Multi-token prodiction</a>
<ul>
<li><a href="#org2dbd2d3">10.1.1. The Core Concept: MTP as an Implicit Critic</a></li>
<li><a href="#org3e7c777">10.1.2. Architecture: The "Value-Aware" MTP Head</a></li>
<li><a href="#orgd62a136">10.1.3. The Algorithm: Recursive Value Propagation</a></li>
<li><a href="#org91d592f">10.1.4. Why this meets the criteria</a></li>
<li><a href="#org8ee24ed">10.1.5. Visualization of Data Flow</a></li>
</ul>
</li>
<li><a href="#orgc6d1934">10.2. New way with stable assumpation</a>
<ul>
<li><a href="#orgaa35fd1">10.2.1. 1. The Core Innovation</a></li>
<li><a href="#org76afdc5">10.2.2. 2. Strengths (Why do this?)</a></li>
<li><a href="#org9e91d2d">10.2.3. 3. The "Stability" Bottleneck (Critical Risk)</a></li>
<li><a href="#orgcf5080f">10.2.4. The Problem: Chasing your own Tail</a></li>
<li><a href="#orgb18a65e">10.2.5. The Solution: Periodic Target Updates (Polyak Averaging)</a></li>
<li><a href="#org5db302f">10.2.6. 4. Final Recommendation</a></li>
<li><a href="#org634dc78">10.2.7. The "Golden Ratio" Loss:</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-orgcf38b7a" class="outline-2">
<h2 id="orgcf38b7a"><span class="section-number-2">1.</span> Markov Decision Process (MDP)</h2>
<div class="outline-text-2" id="text-1">
<ul class="org-ul">
<li><b>State</b>: the set of states \( S \)
<ul class="org-ul">
<li>at a time point, for all parts, how the world looks like.</li>
<li>State space: at a time point, for all parts, how the world could look like</li>
</ul></li>
<li><b>Action</b>: the set of actions \( \mathcal{A}(s) \) is associated with state \( s \in S \)
<ul class="org-ul">
<li>at a time point, for each parts, what I do.</li>
<li>Action space : at a time point, for each parts, what I could do</li>
</ul></li>
<li><b>Reward</b>: the set of rewards \( \mathcal{R}(s, a) \)</li>

<li><b>Trajectory</b>
<ul class="org-ul">
<li>episode, return: sum of discount return \(\gamma\) in a episode(trajectory)</li>
</ul></li>

<li><b>State transition probability</b>:
<ul class="org-ul">
<li>state transition: at a time point, if I did one operation, what the world is changed to.</li>
<li>at a time point, if I did one operation, How the world can be changed.</li>
<li>At state \( s \), taking action \( a \), the probability to transit to state \( s' \) is    \( p(s' \mid s, a) \)</li>
</ul></li>
<li><b>Reward probability</b>:  
At state \( s \), taking action \( a \), the probability to get reward \( r \) is    \( p(r \mid s, a) \)</li>
<li><b>Policy</b>
<ul class="org-ul">
<li>deterministic &amp; stochastic</li>
<li>At state \( s \), the probability to choose action \( a \) is   \( \pi(a \mid s)  \)</li>
</ul></li>
</ul>


<ul class="org-ul">
<li><p>
<b>Markov property</b>
Memoryless property:
\[
  p(s_{t+1} \mid a_{t}, s_t, \ldots, a_0, s_0)
  =
  p(s_{t+1} \mid a_{t}, s_t)
  \]
</p>

<p>
\[
  p(r_{t+1} \mid a_{t}, s_t, \ldots, a_0, s_0)
  =
  p(r_{t+1} \mid a_{t}, s_t)
  \]
</p></li>
</ul>
</div>
</div>
<div id="outline-container-orga613506" class="outline-2">
<h2 id="orga613506"><span class="section-number-2">2.</span> Bellman Equation</h2>
<div class="outline-text-2" id="text-2">
<ul class="org-ul">
<li><b>Return</b> : summary of all discount return in one complete trajectory</li>

<li><b>State Value</b>: expectation of  Return for all possiable trajectories
 \[ V_{\pi}(s) = \sum_{a} \pi(a|s) \sum_{s', r} p(s', r | s, a) \left[ r + \gamma V_{\pi}(s') \right] \]
Relationship to action value:    \[ V_{\pi}(s) = \sum_{a \in \mathcal{A}} \pi(a|s) Q_{\pi}(s, a) \]</li>

<li><b>Action Value</b>: expectation of  Return for all possiable trajectories  after taking a specified action
 \[ Q_{\pi}(s, a) = \sum_{s', r} p(s', r | s, a) \left[ r + \gamma \sum_{a'} \pi(a'|s') Q_{\pi}(s', a') \right] \]
Relationship to state value   \[ Q_{\pi}(s, a) = \sum_{s', r} p(s', r | s, a) \left[ r + \gamma V_{\pi}(s') \right] \]</li>
</ul>


<ul class="org-ul">
<li><b>Bellman Optimality Equation</b> is bellman equation with the best policy</li>
</ul>
</div>
</div>
<div id="outline-container-orgbe5506a" class="outline-2">
<h2 id="orgbe5506a"><span class="section-number-2">3.</span> Dynamic Programming Iterations in RL</h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="outline-container-org907509c" class="outline-3">
<h3 id="org907509c"><span class="section-number-3">3.1.</span> Value Iteration</h3>
<div class="outline-text-3" id="text-3-1">
<p>
Value Iteration combines policy improvement and a single step of policy evaluation into one operation.
</p>
<ul class="org-ul">
<li><b><b>Initialization:</b></b> Start with an arbitrary initial value for all states (e.g., \(v_0 = 0\)).</li>
<li><b><b>Iteration:</b></b>
<ol class="org-ol">
<li><b>Implicit Policy Update:</b> For all states, apply all possible action (Q-table), look ahead to find the best action.
\[ \pi_{k+1} = \arg\max_{\pi} \left( r_{\pi} + \gamma P_{\pi} v_k \right) \]</li>
<li><b>Value Update:</b> Use that best action to update the state value immediately.
\[ v_{k+1} = r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}} v_k \]</li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-org1feae5e" class="outline-3">
<h3 id="org1feae5e"><span class="section-number-3">3.2.</span> Policy Iteration</h3>
<div class="outline-text-3" id="text-3-2">
<p>
Policy Iteration separates the process into two distinct phases. Crucially, the <b><b>Policy Evaluation</b></b> phase is itself an iterative process that looks very similar to Value Iteration, but with a <b>fixed</b> policy.
</p>
</div>
<div id="outline-container-org6017ae3" class="outline-4">
<h4 id="org6017ae3"><span class="section-number-4">3.2.1.</span> Phase 1: Policy Evaluation (The Inner Loop)</h4>
<div class="outline-text-4" id="text-3-2-1">
<p>
This phase starts with a policy \(\pi\) and calculates its value \(v_{\pi}\).
</p>
<ul class="org-ul">
<li><b><b>Input:</b></b> The current fixed policy \(\pi_k\).</li>
<li><b><b>Process:</b></b> We iterate to find the value by performing "Value Updates" repeatedly.
<ol class="org-ol">
<li>Initialize a value estimate \(v\) (e.g., \(v=0\)).</li>
<li>Apply the Bellman Expectation Operator repeatedly (infinite steps):
\[ v_{i+1} = r_{\pi_k} + \gamma P_{\pi_k} v_i \]</li>
<li><b><b>Stop</b></b> when \(v\) converges (i.e., \(i \to \infty\)).</li>
</ol></li>
<li><b><b>Output:</b></b> The converged state-value function \(v_{\pi_k}\).</li>
</ul>
</div>
</div>
<div id="outline-container-org0d5e75c" class="outline-4">
<h4 id="org0d5e75c"><span class="section-number-4">3.2.2.</span> Phase 2: Policy Improvement (The Outer Loop)</h4>
<div class="outline-text-4" id="text-3-2-2">
<p>
Once we know exactly how good the current policy \(\pi_k\) is (from Phase 1), we make it greedy.
</p>
<ul class="org-ul">
<li><b><b>Process:</b></b>
\[ \pi_{k+1} = \arg\max_{\pi} \left( r_{\pi} + \gamma P_{\pi} v_{\pi_k} \right) \]</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgc9e32a6" class="outline-3">
<h3 id="orgc9e32a6"><span class="section-number-3">3.3.</span> Truncated Policy Iteration</h3>
<div class="outline-text-3" id="text-3-3">
<p>
The mathematical connection is defined by the depth of the evaluation step (the number of iterations \(j\) in Phase 1):
</p>
<ul class="org-ul">
<li><b><b>Value Iteration</b></b> is a special case of Truncated Policy Iteration where the evaluation depth is \(j=1\).</li>
<li><b><b>Policy Iteration</b></b> is the limit of Truncated Policy Iteration where the evaluation depth \(j \to \infty\).</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgb811f8f" class="outline-2">
<h2 id="orgb811f8f"><span class="section-number-2">4.</span> Monte Carlo Methods in RL</h2>
<div class="outline-text-2" id="text-4">
<p>
Model-free, without to know the model, we use the expecation of example from MC to fine-tune Action value:  \(q_{\pi_{k}(s,a)} = E(G_{t}| S_{t}=s, A_{t}=a )\). Basic the process works the same as Policy iteration(Policy evaluation and policy improvement), only the evaluation is with the exampling from MC.
</p>
</div>
<div id="outline-container-orgc3d8f2b" class="outline-3">
<h3 id="orgc3d8f2b"><span class="section-number-3">4.1.</span> 1. MC Basic (The Theoretical Baseline)</h3>
<div class="outline-text-3" id="text-4-1">
<p>
This is the simplest form, adapted directly from Policy Iteration logic but using sample returns instead of models.
</p>
<ul class="org-ul">
<li><b>Theory:</b>
<ul class="org-ul">
<li>Operates in strict distinct steps: <b><b>Policy Evaluation</b></b> (wait for many episodes) \(\rightarrow\) <b><b>Policy Improvement</b></b> (update policy).</li>
<li>Uses the <b><b>Initial-Visit Strategy</b></b>: Only the start of an episode is used to update the value of the starting state-action pair \((s_0, a_0)\). Intermediate steps in the episode are ignored for updates.</li>
</ul></li>
<li><b>Limitation:</b>
<ul class="org-ul">
<li><b><b>Low Sample Efficiency</b></b>: Wastes data by ignoring subsequent state visits in an episode.</li>
<li><b><b>Impractical</b></b>: Requires collecting "sufficiently many episodes" for <b>every</b> state-action pair before making a single policy update.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org2758996" class="outline-3">
<h3 id="org2758996"><span class="section-number-3">4.2.</span> 2. MC Exploring Starts (The Efficient Simulator)</h3>
<div class="outline-text-3" id="text-4-2">
<p>
An extension designed to fix sample efficiency but introduces a strong dependency on environment control.
</p>
<ul class="org-ul">
<li><b>Theory:</b>
<ul class="org-ul">
<li><b><b>Data Efficiency</b></b>: Uses the <b><b>Every-Visit Strategy</b></b> (or sub-episode decomposition). One long episode is broken down into multiple "sub-episodes" to update values for every state visited, not just the start.</li>
<li><b><b>Episode-by-Episode Update</b></b>: Updates the policy immediately after a single episode (Generalized Policy Iteration), rather than waiting for a batch.</li>
</ul></li>
<li><b>Limitation:</b>
<ul class="org-ul">
<li><b><b>The "Exploring Starts" Assumption</b></b>: It requires the environment to be able to start an episode at <b>any</b> random state-action pair \((s, a)\).</li>
<li><b><b>Real-world Friction</b></b>: This is often impossible in physical reality (e.g., you cannot initialize a robot in a specific "falling over" state instantly).</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org9753f3d" class="outline-3">
<h3 id="org9753f3d"><span class="section-number-3">4.3.</span> 3. MC Epsilon-Greedy (The Practical Solution)</h3>
<div class="outline-text-3" id="text-4-3">
<p>
This method removes the unrealistic "Exploring Starts" assumption, making MC methods viable for real-world learning where you cannot control the starting state.
</p>
</div>
<div id="outline-container-org0240c57" class="outline-4">
<h4 id="org0240c57"><span class="section-number-4">4.3.1.</span> Core Concept: Soft Policies</h4>
<div class="outline-text-4" id="text-4-3-1">
<p>
Instead of forcing the environment to <b>start</b> randomly, we force the agent to <b>behave</b> randomly occasionally.
</p>
<ul class="org-ul">
<li><b><b>Goal</b></b>: Ensure all state-action pairs are visited "sufficiently many times" without external resets.</li>
<li><b><b>Mechanism</b></b>: Uses a <b><b>Soft Policy</b></b> ( \(\epsilon\) -greedy), meaning there is always a non-zero probability of taking any action in any state.</li>
</ul>
</div>
</div>
<div id="outline-container-org784b29c" class="outline-4">
<h4 id="org784b29c"><span class="section-number-4">4.3.2.</span> Exploration vs. Exploitation</h4>
<div class="outline-text-4" id="text-4-3-2">
<p>
The algorithm balances these two conflicting goals via the parameter \(\epsilon\) (epsilon).
</p>

<ul class="org-ul">
<li>Exploration ( The \(\epsilon\) component)
<ul class="org-ul">
<li><b><b>Purpose</b></b>: To discover new strategies and ensure the agent does not get stuck in a suboptimal loop. By taking random actions, the agent eventually wanders into every possible state, fulfilling the coverage requirement that "Exploring Starts" used to handle.</li>
<li><b><b>Mechanism</b></b>: With probability \(\epsilon\), the agent ignores its knowledge and chooses randomly.</li>
</ul></li>

<li>Exploitation (The \(1-\epsilon\) component)
<ul class="org-ul">
<li><b><b>Purpose</b></b>: To maximize rewards based on current knowledge. The agent selects the "Greedy" action (the one with the highest estimated value \(q(s,a)\)).</li>
<li><b><b>Mechanism</b></b>: With probability \(1 - \epsilon\), the agent chooses the best known action.</li>
</ul></li>
</ul>

<ul class="org-ul">
<li><b><b>Continuous Learning</b></b>: Even if the agent always starts at the same spot (Start Line), the stochastic nature of the policy (\(\epsilon\)) ensures that over many episodes, it will eventually drift into unvisited states.</li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-org8445c2f" class="outline-2">
<h2 id="org8445c2f"><span class="section-number-2">5.</span> Temporal Difference</h2>
<div class="outline-text-2" id="text-5">
<p>
<b>Core Concept</b>
TD Learning is the solution(policy) to the Bellman Expectation Equation, formulated as a root-finding problem and solved using the Robbins-Monro stochastic approximation algorithm.
</p>
</div>
<div id="outline-container-orge079f19" class="outline-3">
<h3 id="orge079f19"><span class="section-number-3">5.1.</span> The Mathematical Driver</h3>
<div class="outline-text-3" id="text-5-1">
<ul class="org-ul">
<li>The Goal: Bellman Expectation Equation
We seek \(V_{\pi}(s)\) such that:
\[V_{\pi}(s) = \mathbb{E}_{\pi} [ R_{t+1} + \gamma V_{\pi}(S_{t+1}) \mid S_t = s ]\]</li>

<li>The Problem: Root Finding
Define the objective function (Bellman Error) \(J(V)\), where we want the root \(J(V)=0\):
\[J(V)(s) = \mathbb{E} [ R_{t+1} + \gamma V(S_{t+1}) ] - V(s) = 0\]</li>

<li>The Solver: Robbins-Monro Algorithm
Iteratively find root \(\theta^*\) of \(f(\theta)=0\) using noisy observations \(\tilde{f}(\theta)\):   \[\theta_{t+1} = \theta_t + \alpha_t \cdot \tilde{f}(\theta_t)\]
<ul class="org-ul">
<li>\(\theta\) &gt; \(\theta^*\)   minus \(\tilde{f}(\theta_t)\)</li>
<li>\(\theta\) &lt; \(\theta^*\)   plus \(\tilde{f}(\theta_t)\)</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org286b27c" class="outline-3">
<h3 id="org286b27c"><span class="section-number-3">5.2.</span> The Derivation</h3>
<div class="outline-text-3" id="text-5-2">
<ul class="org-ul">
<li>The Noisy Observation (TD Error)
Since we cannot compute the expectation \(\mathbb{E}\), we take a single sample:
<ul class="org-ul">
<li><b>Observation:</b> \(\tilde{J}(V_t) = (r_{t+1} + \gamma V_t(s_{t+1})) - V_t(s_t)\)</li>
<li>This is the <b><b>TD Error</b></b> (\(\delta_t\)).</li>
</ul></li>

<li>The Sample Target
The term \(r_{t+1} + \gamma V_t(s_{t+1})\) acts as the "Label":
<ul class="org-ul">
<li><b>Reality:</b> \(r_{t+1}\) (Ground truth, low variance).</li>
<li><b>Guess:</b> \(\gamma V_t(s_{t+1})\) (Bootstrap estimate).</li>
<li><b>Function:</b> It acts as the target \(y\) for \(V(s_t)\), providing a better estimate than \(V(s_t)\) alone because it includes real data.</li>
</ul></li>

<li>The Solution (TD Update Rule)
Substituting \(\tilde{f}(\theta)\) into the Robbins-Monro update:
\[V(s_t) \leftarrow V(s_t) + \alpha \delta_t\]
\[V(s_t) \leftarrow V(s_t) + \alpha \underbrace{[ (r_{t+1} + \gamma V(s_{t+1})) - V(s_t) ]}_{\text{TD Error}}\]
\[V(s_t) \leftarrow V(s_t) - \alpha [ V(s_t) - \underbrace{(r_{t+1} + \gamma V(s_{t+1}))}_{\text{Target}} ]\]</li>
</ul>
</div>
</div>
<div id="outline-container-org3501a1f" class="outline-3">
<h3 id="org3501a1f"><span class="section-number-3">5.3.</span> Unified View of TD-series</h3>
<div class="outline-text-3" id="text-5-3">
</div>
<div id="outline-container-orgb862978" class="outline-4">
<h4 id="orgb862978"><span class="section-number-4">5.3.1.</span> 1. The General Update Rule</h4>
<div class="outline-text-4" id="text-5-3-1">
<p>
When we use Action value instead of policy for TD, all discussed algorithms can be expressed as a stochastic approximation update solving a Bellman equation.
The unified update rule (Gradient Descent form) is:
</p>

<p>
\[q_{t+1}(s_t, a_t) = q_t(s_t, a_t) - \alpha_t(s_t, a_t) [ q_t(s_t, a_t) - \bar{q}_t ]\]
</p>

<ul class="org-ul">
<li>\(q_t(s_t, a_t)\): Current Estimate</li>
<li>\(\alpha_t\): Learning rate (Step size)</li>
<li>\(\bar{q}_t\): The <b><b>Target</b></b> (The noisy sample derived from environment interaction)</li>
<li>The term \([q_t - \bar{q}_t]\) represents the error to minimize.</li>
</ul>
</div>
</div>
<div id="outline-container-orgda232e2" class="outline-4">
<h4 id="orgda232e2"><span class="section-number-4">5.3.2.</span> 2. Algorithm Specifics</h4>
<div class="outline-text-4" id="text-5-3-2">
<ul class="org-ul">
<li>Sarsa:
<ul class="org-ul">
<li><b><b>Target Expression (\(\bar{q}_t\)):</b></b>    \[\bar{q}_t = r_{t+1} + \gamma q_t(s_{t+1}, a_{t+1})\]</li>
<li><b><b>Equation Aimed to Solve:</b></b>   <b><b>Bellman Expectation Equation (BE) for \(q_{\pi}\)</b></b>:
\[q_{\pi}(s, a) = \mathbb{E} [R_{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1}) \mid S_t = s, A_t = a]\]</li>
<li><b><b>Note:</b></b> This is an on-policy method using the action \(a_{t+1}\) actually taken by the current policy.</li>
</ul></li>

<li>Q-learning
<ul class="org-ul">
<li><b><b>Target Expression (\(\bar{q}_t\)):</b></b>   \[\bar{q}_t = r_{t+1} + \gamma \max_{a} q_t(s_{t+1}, a)\]</li>
<li><b><b>Equation Aimed to Solve:</b></b>  <b>Bellman Optimality Equation  (BOE) for \(q_*\)</b>:
\[q_{*}(s, a) = \mathbb{E} [R_{t+1} + \max_{a} q_{*}(S_{t+1}, a) \mid S_t = s, A_t = a]\]</li>
<li><b><b>Note:</b></b> This is an off-policy method because it updates towards the best possible action (\(\max\)), regardless of the policy actually followed.</li>
</ul></li>
</ul>


<ul class="org-ul">
<li>Expected Sarsa
<ul class="org-ul">
<li><b><b>Target Expression (\(\bar{q}_t\)):</b></b>   \[\bar{q}_t = r_{t+1} + \gamma \sum_{a} \pi_t(a|s_{t+1})q_t(s_{t+1}, a)\]</li>
<li><b><b>Equation Aimed to Solve:</b></b>   <b><b>Bellman Expectation Equation (BE) for \(q_{\pi}\)</b></b>:
\[q_{\pi}(s, a) = \mathbb{E} [R_{t+1} + \gamma \mathbb{E}_{A_{t+1}}[q_{\pi}(S_{t+1}, A_{t+1})] \mid S_t = s, A_t = a]\]</li>
<li><b><b>Note:</b></b> It reduces variance by taking the expectation over all possible next actions rather than just sampling one.</li>
</ul></li>

<li>N-step Sarsa
<ul class="org-ul">
<li><b><b>Target Expression (\(\bar{q}_t\)):</b></b>   \[\bar{q}_t = r_{t+1} + \gamma r_{t+2} + \dots + \gamma^n q_t(s_{t+n}, a_{t+n})\]</li>
<li><b><b>Equation Aimed to Solve:</b></b>   <b><b>Bellman Expectation Equation (BE) for \(q_{\pi}\)</b></b>:
\[q_{\pi}(s, a) = \mathbb{E} [R_{t+1} + \gamma R_{t+2} + \dots + \gamma^n q_{\pi}(S_{t+n}, A_{t+n}) \mid S_t = s, A_t = a]\]</li>
<li><b><b>Note:</b></b> It balances bias and variance by looking \(n\) steps ahead before bootstrapping.</li>
</ul></li>

<li>Monte Carlo (MC)
<ul class="org-ul">
<li><b><b>Target Expression (\(\bar{q}_t\)):</b></b>  \[\bar{q}_t = r_{t+1} + \gamma r_{t+2} + \dots \text{ (Full Return } G_t)\]</li>
<li><b><b>Equation Aimed to Solve:</b></b>   <b><b>Bellman Expectation Equation (BE) for \(q_{\pi}\)</b></b>:
\[q_{\pi}(s, a) = \mathbb{E} [R_{t+1} + \gamma R_{t+2} + \dots \mid S_t = s, A_t = a]\]</li>
<li><b><b>Note:</b></b> Can be viewed as the unified expression where \(\alpha_t(s_t, a_t) = 1\), making \(q_{t+1} = \bar{q}_t\) (direct assignment of the return).</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org2a19c52" class="outline-3">
<h3 id="org2a19c52"><span class="section-number-3">5.4.</span> Deep Q-learning</h3>
<div class="outline-text-3" id="text-5-4">
<ul class="org-ul">
<li>Deep Q-learning replaces the tabular \(q(s,a)\) with a parameterized neural network \(\hat{q}(s, a, w)\).</li>
<li>We want the neural network to satisfy the <b><b>Bellman Optimality Equation</b></b>:
\[q_*(s, a) = \mathbb{E} [R_{t+1} + \gamma \max_{a'} q_*(S_{t+1}, a') \mid S_t=s, A_t=a]\]</li>
<li>It aims to minimize the loss function \(J(w)\):
\[J(w) = \mathbb{E} \left[ \left( \underbrace{R + \gamma \max_{a' \in \mathcal{A}(S')} \hat{q}(S', a', w)}_{\text{Target (Bellman Optimality)}} - \underbrace{\hat{q}(S, A, w)}_{\text{Prediction}} \right)^2 \right]\]</li>
</ul>


<p>
where The term inside the squared brackets is the <b><b>TD Error</b></b> (specifically for Q-Learning).
 \[\delta = (R + \gamma \max \hat{q}(S', a', w)) - \hat{q}(S, A, w)\]. In order to minimize the distance (error) between the <b>Prediction</b> and the <b>Target</b>, the network \(\hat{q}\) converges towards the optimal value function \(q_*\).
</p>

<ul class="org-ul">
<li>Algorithm
<ul class="org-ul">
<li><b><b>Sample:</b></b>
Uniformly draw a mini-batch of samples from \(\mathcal{B}\).</li>

<li><b><b>Calculate Targets:</b></b>
For each sample \((s, a, r, s')\) in the mini-batch, calculate the target value \(y_T\):
\[y_T = r + \gamma \max_{a \in \mathcal{A}(s')} \hat{q}(s', a, w_T)\]
<ul class="org-ul">
<li>Where \(w_T\) is the parameter of the <b><b>target network</b></b>.</li>
</ul></li>

<li><b><b>Update Main Network:</b></b>
Update the main network parameter \(w\) to minimize the loss:
\[Loss = (y_T - \hat{q}(s, a, w))^2\]
<ul class="org-ul">
<li>This update uses the mini-batch data \(\{(s, a, y_T)\}\).</li>
</ul></li>

<li><b><b>Update Target Network:</b></b>
Set \(w_T = w\) every \(C\) iterations.</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org1e29f57" class="outline-2">
<h2 id="org1e29f57"><span class="section-number-2">6.</span> Policy Gradient</h2>
<div class="outline-text-2" id="text-6">
</div>
<div id="outline-container-org3fdb01d" class="outline-3">
<h3 id="org3fdb01d"><span class="section-number-3">6.1.</span> Metrics</h3>
<div class="outline-text-3" id="text-6-1">
<ul class="org-ul">
<li>\(\bar{v}_\pi\)  (Discounted Average Value)
\[\sum_{s \in S} d(s)\, v_\pi(s)\]
\[\mathbb{E}_{S \sim d}[v_\pi(S)]\]
\[\mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^{t} R_{t+1}\right]\]</li>

<li>\(\bar{r}_\pi\) (Average Reward Objective)
\[\sum_{s \in S} d_\pi(s)\, r_\pi(s)\]
\[\mathbb{E}_{S \sim d_\pi}[r_\pi(S)]\]
\[\lim_{n \to \infty} \frac{1}{n} \mathbb{E}\left[\sum_{t=0}^{n-1} R_{t+1}\right]\]</li>
</ul>
</div>
</div>
<div id="outline-container-org0f91c0d" class="outline-3">
<h3 id="org0f91c0d"><span class="section-number-3">6.2.</span> General objective function for metrics</h3>
<div class="outline-text-3" id="text-6-2">
<p>
The gradient of the objective function \(J(\theta)\) is given by the Policy Gradient Theorem:
\[\nabla_\theta J(\theta)
= \sum_{s \in S} \eta(s)
  \sum_{a \in A}
  \nabla_\theta \pi(a \mid s, \theta)\, q_\pi(s, a)\]
</p>

<p>
where:
</p>
<ul class="org-ul">
<li>\(\eta(s)\) is the state distribution (discounted or stationary depending on metric)</li>
<li>\(\nabla_\theta \pi(a \mid s, \theta)\) denotes the gradient of the policy \(\pi\) with respect to the parameters \(\theta\),</li>
<li>\(q_\pi(s, a)\) is the action-value function.</li>
<li>Note: The theorem proves equality, but in practice, ignoring the gradient of the state distribution leads to an approximation often called the "proportional" gradient.</li>
</ul>

<p>
Moreover, as an expectation:\[  \nabla_\theta J(\theta)
  = \mathbb{E}_{S \sim \eta,\; A \sim \pi(S,\theta)}
  \left[
  \nabla_\theta \ln \pi(A \mid S, \theta)\, q_\pi(S, A)
  \right].\]
</p>
</div>
</div>
<div id="outline-container-org1393eee" class="outline-3">
<h3 id="org1393eee"><span class="section-number-3">6.3.</span> Policy parameters update</h3>
<div class="outline-text-3" id="text-6-3">
<p>
\[  \theta_{t+1} = \theta_t + \alpha \nabla_\theta J(\theta_t)\]
</p>

<p>
Using the expectation form of the policy gradient, this becomes:
</p>


<p>
\[\theta_{t+1} = \theta_t + \alpha \mathbb{E} [ \nabla_\theta \ln \pi(A \mid S, \theta_{t})\, q_{\pi}(S, A) ]\]
</p>

<p>
We do not have \(q_{\pi}(S, A)\), so use \(\hat{q}(S_t, A_t)\) from sampling.
\[\theta_{t+1} = \theta_t + \alpha \nabla_\theta \ln \pi(A_t \mid S_t, \theta_t)\, \hat{q}(S_t, A_t)\]
</p>
<ul class="org-ul">
<li>Sampling from MC: REINFORCE</li>
<li>Sampling from TD: Actor-Critic</li>
</ul>
</div>
</div>
<div id="outline-container-org5de87d3" class="outline-3">
<h3 id="org5de87d3"><span class="section-number-3">6.4.</span> Theory</h3>
<div class="outline-text-3" id="text-6-4">
<p>
\[\theta_{t+1} = \theta_t + \alpha \nabla_{\theta} \ln \pi(a_t \mid s_t, \theta_t) q_t(s_t, a_t)\]
Because of the log-derivative trick:
\[\nabla_{\theta} \ln \pi(a_t \mid s_t, \theta_t) = \frac{\nabla_{\theta} \pi(a_t \mid s_t, \theta_t)}{\pi(a_t \mid s_t, \theta_t)}\]
We have:
\[\theta_{t+1} = \theta_t + \alpha \frac{\nabla_{\theta} \pi(a_t \mid s_t, \theta_t)}{\pi(a_t \mid s_t, \theta_t)} q_t(s_t, a_t)\]
So, defining \(\beta_t = \frac{q_t(s_t, a_t)}{\pi(a_t \mid s_t, \theta_t)}\):
\[\theta_{t+1} = \theta_t + \alpha \beta_t \nabla_{\theta} \pi(a_t \mid s_t, \theta_t)\]
</p>
<ul class="org-ul">
<li>If \(\beta_t \ge 0 \implies\)  Move in direction of gradient:
<ul class="org-ul">
<li>-\(\implies \pi(a_t | s_t, \theta_{t+1}) \ge \pi(a_t | s_t, \theta_t)\)</li>
<li>Enhancement</li>
<li>Reinforce good actions</li>
</ul></li>

<li>If \(\beta_t < 0 \implies\) Move opposite to gradient :
<ul class="org-ul">
<li>\(\implies \pi(a_t | s_t, \theta_{t+1}) < \pi(a_t | s_t, \theta_t)\)</li>
<li>Decrease</li>
<li>Suppress bad actions</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org7bdad34" class="outline-3">
<h3 id="org7bdad34"><span class="section-number-3">6.5.</span> REINFORCE Algorithm</h3>
<div class="outline-text-3" id="text-6-5">
<p>
Monte Carlo Policy Gradient
</p>
<ul class="org-ul">
<li>Initialize: \(\theta\), \(\gamma \in (0,1)\), \(\alpha > 0\)</li>
<li><p>
Loop for each episode:
</p>
<ul class="org-ul">
<li>Generate episode \(\{s_0, a_0, r_1, \dots, s_{T-1}, a_{T-1}, r_T\}\) following policy \(\pi(\cdot|\cdot, \theta_k)\)</li>
<li>For \(t = 0\) to \(T-1\):
<ul class="org-ul">
<li>\(G_t \leftarrow \sum_{k=t+1}^{T} \gamma^{k-t-1} r_k\)  (Calculate return)</li>
<li>\(\theta_{t} \leftarrow \theta_{t} + \alpha \gamma^t G_t \nabla_{\theta} \ln \pi(a_t | s_t, \theta_{k})\)</li>
</ul></li>
</ul>
<ul class="org-ul">
<li>\(\theta_{k} \leftarrow \theta_{t}\)</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org80ba121" class="outline-2">
<h2 id="org80ba121"><span class="section-number-2">7.</span> Actor-Critic</h2>
<div class="outline-text-2" id="text-7">
</div>
<div id="outline-container-orgb6dec31" class="outline-3">
<h3 id="orgb6dec31"><span class="section-number-3">7.1.</span> Q-Actor-Critic (QAC)</h3>
<div class="outline-text-3" id="text-7-1">
</div>
<div id="outline-container-orge81dc0a" class="outline-4">
<h4 id="orge81dc0a"><span class="section-number-4">7.1.1.</span> Background</h4>
<div class="outline-text-4" id="text-7-1-1">
<p>
Derived from the Policy Gradient theorem, QAC replaces the Monte Carlo (MC) return with a <b><b>Temporal Difference (TD)</b></b> value function approximation.
The policy parameters are updated using the gradient of the log-probability scaled by the action-value function:
\[\theta_{t+1} = \theta_t + \alpha \nabla_{\theta} \ln \pi(a_t \mid s_t, \theta_t) q_t(s_t, a_t)\]
</p>
</div>
</div>
<div id="outline-container-org3707961" class="outline-4">
<h4 id="org3707961"><span class="section-number-4">7.1.2.</span> Initialization</h4>
<div class="outline-text-4" id="text-7-1-2">
<ul class="org-ul">
<li><b>Policy Function:</b> \(\pi(a|s, \theta_0)\) with initial parameters \(\theta_0\).</li>
<li><b>Value Function:</b> \(q(s, a, w_0)\) with initial parameters \(w_0\).</li>
<li><b>Learning Rates:</b> \(\alpha_w, \alpha_{\theta} > 0\).</li>
<li><b>Goal:</b> Maximize the expected return \(J(\theta)\).</li>
</ul>
</div>
</div>
<div id="outline-container-org921f054" class="outline-4">
<h4 id="org921f054"><span class="section-number-4">7.1.3.</span> Loop (for each time step \(t\) in episode):</h4>
<div class="outline-text-4" id="text-7-1-3">
<ol class="org-ol">
<li><b><b>Generate Action:</b></b> Sample \(a_t \sim \pi(a|s_t, \theta_t)\).</li>
<li><b><b>Observe Reward:</b></b> Get \(r_{t+1}\) and next state \(s_{t+1}\).</li>
<li><b><b>Select Next Action:</b></b> Sample \(a_{t+1} \sim \pi(a|s_{t+1}, \theta_t)\).</li>

<li><b><b>Actor (Policy Update):</b></b>
Update the policy parameters in the direction of higher rewards:
\[\theta_{t+1} = \theta_t + \alpha_{\theta} \nabla_{\theta} \ln \pi(a_t | s_t, \theta_t) q(s_t, a_t, w_t)\]</li>

<li><b><b>Critic (Value Update):</b></b>
Update the action-value parameters using the semi-gradient TD error:
\[w_{t+1} = w_t + \alpha_w \left[ r_{t+1} + \gamma q(s_{t+1}, a_{t+1}, w_t) - q(s_t, a_t, w_t) \right] \nabla_w q(s_t, a_t, w_t)\]</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-orgc363376" class="outline-3">
<h3 id="orgc363376"><span class="section-number-3">7.2.</span> Advantage Actor-Critic</h3>
<div class="outline-text-3" id="text-7-2">
</div>
<div id="outline-container-org159ac0b" class="outline-4">
<h4 id="org159ac0b"><span class="section-number-4">7.2.1.</span> Add baseline</h4>
<div class="outline-text-4" id="text-7-2-1">
<p>
To improve the stability of the Policy Gradient, we introduce a <b><b>baseline</b></b> to the update function. Subtracting a state-value function \(v(s)\) from the return reduces variance without introducing bias. The standard gradient update is modified by subtracting the baseline \(v_t(s_t)\):
\[ \theta_{t+1} = \theta_t + \alpha \nabla_{\theta} \ln \pi(a_t \mid s_t, \theta_t) [q_t(s_t, a_t) - v_t(s_t)] \]
</p>
</div>
</div>
<div id="outline-container-org6bdc051" class="outline-4">
<h4 id="org6bdc051"><span class="section-number-4">7.2.2.</span> The Advantage Function (\(\delta_t\))</h4>
<div class="outline-text-4" id="text-7-2-2">
<p>
We define the <b><b>Advantage Function</b></b> as the difference between the action-value and the state-value. However, since we often do not know \(q_t\) explicitly, we approximate it using the <b><b>TD Error</b></b> \(\delta_t\):
</p>

<p>
\[ \delta_t(s_t, a_t) = q_t(s_t, a_t) - v_t(s_t) \approx r_{t+1} + \gamma v_t(s_{t+1}) - v_t(s_t) \]
</p>

<p>
Substituting this back into our update rule gives us a more robust update:
\[ \theta_{t+1} = \theta_t + \alpha \nabla_{\theta} \ln \pi(a_t \mid s_t, \theta_t) \delta_t(s_t, a_t) \]
</p>

<p>
<b>Why use the Advantage Function?</b>
The advantage \(\delta_t\) effectively measures how much better an action \(a_t\) was compared to the "average" value of the state. This helps balance <b><b>exploration and exploitation</b></b> more effectively than raw returns, as the agent explicitly learns which actions outperform the expected baseline.
</p>
</div>
</div>
<div id="outline-container-org5310ac4" class="outline-4">
<h4 id="org5310ac4"><span class="section-number-4">7.2.3.</span> Algorithm</h4>
<div class="outline-text-4" id="text-7-2-3">
<ul class="org-ul">
<li><b>Policy Function (Actor):</b> \(\pi(a|s, \theta_0)\) with initial parameters \(\theta_0\).</li>
<li><b>Value Function (Critic):</b> \(v(s, w_0)\) with initial parameters \(w_0\).</li>
<li><b>Learning Rates:</b> \(\alpha_w, \alpha_{\theta} > 0\).</li>
<li><b>Goal:</b> Learn an optimal policy to maximize expected return \(J(\theta)\).</li>
</ul>

<p>
At time step \(t\) in each episode:
</p>

<ol class="org-ol">
<li><b><b>Generate Action &amp; Observe:</b></b>
Generate \(a_t\) following \(\pi(a|s_t, \theta_t)\), then observe reward \(r_{t+1}\) and next state \(s_{t+1}\).</li>

<li><b><b>Calculate Advantage (TD Error):</b></b>
Compute the TD error using the Critic's current value estimates:
\[ \delta_t = r_{t+1} + \gamma v(s_{t+1}, w_t) - v(s_t, w_t) \]</li>

<li><b><b>Actor Update (Policy):</b></b>
Update the policy parameters to encourage actions with high advantage:
\[ \theta_{t+1} = \theta_t + \alpha_{\theta} \delta_t \nabla_{\theta} \ln \pi(a_t | s_t, \theta_t) \]</li>

<li><b><b>Critic Update (Value):</b></b>
Update the value function parameters to minimize the TD error:
\[ w_{t+1} = w_t + \alpha_w \delta_t \nabla_w v(s_t, w_t) \]</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-org3ec2b2c" class="outline-3">
<h3 id="org3ec2b2c"><span class="section-number-3">7.3.</span> Off-Policy Actor-Critic</h3>
<div class="outline-text-3" id="text-7-3">
<p>
We use a behavior policy \(\beta\) to generate experience samples. To estimate the gradient of the target policy \(\pi\), we must use <b><b>Importance Sampling</b></b>.
</p>
</div>
<div id="outline-container-org14b7173" class="outline-4">
<h4 id="org14b7173"><span class="section-number-4">7.3.1.</span> The Objective Gradient</h4>
<div class="outline-text-4" id="text-7-3-1">
<p>
\[ \nabla_{\theta} J(\theta) = \mathbb{E}_{S \sim \rho, A \sim \beta} \left[ \frac{\pi(A|S, \theta)}{\beta(A|S)} \nabla_{\theta} \ln \pi(A|S, \theta) q_{\pi}(S, A) \right] \]
</p>
</div>
</div>
<div id="outline-container-orgf36b322" class="outline-4">
<h4 id="orgf36b322"><span class="section-number-4">7.3.2.</span> The Update Rule</h4>
<div class="outline-text-4" id="text-7-3-2">
<p>
The update is similar to A2C but scaled by the importance sampling ratio \(\rho_t\):
</p>

<p>
\[ \theta_{t+1} = \theta_t + \alpha \underbrace{ \frac{\pi(a_t|s_t, \theta_t)}{\beta(a_t|s_t)} }_{\text{Importance Weight } \rho_t} \delta_t(s_t, a_t) \nabla_{\theta} \ln \pi(a_t \mid s_t, \theta_t) \]
</p>

<p>
Because of the log-derivative trick:
\[ \theta_{t+1} = \theta_t + \alpha \frac{\delta_t(s_t, a_t)}{\beta(a_{t}|s_{t})} \nabla_{\theta} \pi(a_t \mid s_t, \theta_t) \]
</p>

<p>
Algorithm is similar only with difference of factor \(\beta(a_{t}|s_{t})\)
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org6b58c83" class="outline-2">
<h2 id="org6b58c83"><span class="section-number-2">8.</span> RL for LLM</h2>
<div class="outline-text-2" id="text-8">
</div>
<div id="outline-container-orga2616a3" class="outline-3">
<h3 id="orga2616a3"><span class="section-number-3">8.1.</span> Objective function to maximase the Reward</h3>
<div class="outline-text-3" id="text-8-1">
</div>
<div id="outline-container-orgaf9f60b" class="outline-4">
<h4 id="orgaf9f60b"><span class="section-number-4">8.1.1.</span> Start from MDP</h4>
<div class="outline-text-4" id="text-8-1-1">
<p>
We write the probability of a trajectory with MDP for \(\tau = (s_0, a_0, \dots, s_T, a_T)\) :
</p>
\begin{equation}
P(\tau | \pi_{\theta}) = P(s_1) \prod_{t=1}^{T} \pi_{\theta}(a_t | s_t) P(s_{t+1} | s_t, a_t)
\end{equation}
<ul class="org-ul">
<li>\(\tau\): Trajectory</li>
<li>\(\pi_{\theta}\): Policy parameterized by \(\theta\)</li>
<li>\(P(s_1)\): Probability of the initial state</li>
<li>\(\pi_{\theta}(a_t | s_t)\): Probability of taking action \(a_t\) in state \(s_t\) (The Policy)</li>
<li>\(P(s_{t+1} | s_t, a_t)\): Probability of transitioning to \(s_{t+1}\) given \(s_t\) and \(a_t\) (The Transition Function/Model)</li>
</ul>


<p>
Taking the gradient of the log-probability:
\[\nabla_\theta \log P(\tau|\pi_{\theta}) = \nabla_\theta \log \rho_0(s_0) + \sum_{t=0}^{T} \left( \nabla_\theta \log P(s_{t+1}|s_t, a_t) + \nabla_\theta \log \pi_\theta(a_t|s_t) \right)\]
</p>

<p>
Since the initial state distribution \(\rho_0\) and the environment dynamics \(P(s_{t+1}|s_t, a_t)\) 
do not depend on the policy parameters \(\theta\), their gradients are zero:
\[\nabla_\theta \log P(\tau|\theta) = \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t)\]
</p>
</div>
</div>
<div id="outline-container-org3ed8aaa" class="outline-4">
<h4 id="org3ed8aaa"><span class="section-number-4">8.1.2.</span> The Policy Gradient Theorem</h4>
<div class="outline-text-4" id="text-8-1-2">
<p>
We aim to maximize the objective \(J(\pi_\theta) = \mathbb{E}_{\tau \sim \pi_\theta} [R(\tau)]\).
</p>

\begin{align*}
\nabla_\theta J(\pi_\theta) &= \nabla_\theta \int_\tau P(\tau|\theta) R(\tau) d\tau & \text{Expand expectation} \\
&= \int_\tau \nabla_\theta P(\tau|\theta) R(\tau) d\tau & \text{Bring gradient under integral} \\
&= \int_\tau P(\tau|\theta) \frac{\nabla_\theta P(\tau|\theta)}{P(\tau|\theta)} R(\tau) d\tau & \text{Log-derivative trick: } \nabla \log x = \frac{\nabla x}{x} \\
&= \int_\tau P(\tau|\theta) \nabla_\theta \log P(\tau|\theta) R(\tau) d\tau & \text{Simplify} \\
&= \mathbb{E}_{\tau \sim \pi_\theta} [\nabla_\theta \log P(\tau|\theta) R(\tau)] & \text{Return to expectation form}
\end{align*}
</div>
</div>
<div id="outline-container-org362bc1d" class="outline-4">
<h4 id="org362bc1d"><span class="section-number-4">8.1.3.</span> Final Merged Expression for derivate of objective function</h4>
<div class="outline-text-4" id="text-8-1-3">
<p>
\[\nabla_\theta J(\pi_\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \left( \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t) \right) R(\tau) \right]\]
</p>
</div>
</div>
<div id="outline-container-orgd534fe6" class="outline-4">
<h4 id="orgd534fe6"><span class="section-number-4">8.1.4.</span> Advantage function \(A_{t}\)</h4>
<div class="outline-text-4" id="text-8-1-4">
<p>
From the above function, we introduce a baseline for \(R{\tau}\) for reducing the variance. A great option is the current value \(V(\tau)\). So we have the advantage function for objective function derivative: A = Q - V.
</p>

<ul class="org-ul">
<li>Monte Carlo:
using G to  estaminate Q: A = G - V</li>
<li>Temporal Difference:
using \(Q=r+ \gamma V(s_{t+1}â€‹)\), so \(A = \delta_t^{V} = Q-V = r_t + \gamma V(s_{t+1}) - V(s_t)\)</li>
<li>GAE:
 MC cover all the steps, and TD only see one step, so GAE use l steps for generall case
\(A_t^{GAE} = \sum_{l=0}^{\infty} (\gamma \lambda )^l \delta_{t+l}^V\)</li>
</ul>
</div>
</div>
<div id="outline-container-org9ace333" class="outline-4">
<h4 id="org9ace333"><span class="section-number-4">8.1.5.</span> Value model Loss function</h4>
<div class="outline-text-4" id="text-8-1-5">
<p>
use MSE, \((V-Q)^2\), where V is the output of value model and Q is the expected value of current step, we use Q = A + V to share the advantage calculation from above.
</p>

<p>
The value function (critic) is trained by minimizing the mean squared error between the predicted value and the estimated return:
</p>

<p>
\[
\mathcal{L}_{\text{value}} = \mathbb{E}_{t} \left[ \left( V_\psi(s_t) - ( A_t + V_\psi(s_t) ) \right)^2 \right]
\]
</p>
</div>
</div>
</div>
<div id="outline-container-orgd7b58a9" class="outline-3">
<h3 id="orgd7b58a9"><span class="section-number-3">8.2.</span> Vanilla Policy Gradient (VPG / REINFORCE)</h3>
<div class="outline-text-3" id="text-8-2">
<p>
\[g = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t) \hat{A}_t \right]\]
</p>

<ul class="org-ul">
<li><b><b>Replace</b></b>: Instead of using total Reward, we use Advantage function.</li>
<li><b><b>Advantage \(\hat{A}_t\)</b></b>: Often replaced by the return \(G_t\) or \(Q(s,a) - V(s)\) to reduce variance.</li>
<li><b><b>Problem</b></b>: High variance and extremely sensitive to step size. One "bad" update can collapse the policy's performance.</li>
</ul>
</div>
</div>
<div id="outline-container-orgc69d066" class="outline-3">
<h3 id="orgc69d066"><span class="section-number-3">8.3.</span> Trust Region Policy Optimization (TRPO)</h3>
<div class="outline-text-3" id="text-8-3">
<p>
TRPO solves the stability issue by ensuring the new policy doesn't move too far from the old policy, using KL Divergence as a constraint.
</p>



<p>
\[\max_\theta \mathbb{E}_{t} \left[ \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} \hat{A}_t \right]\]
\[\text{subject to } \mathbb{E}_{t} [KL(\pi_{\theta_{old}}(\cdot|s_t) || \pi_\theta(\cdot|s_t))] \leq \delta\]
</p>

<ul class="org-ul">
<li><b><b>Replace</b></b>: we use the ratio from important exampling of current to old policy</li>
<li><b><b>Key Idea</b></b>: It defines a "Trust Region" by KL divergence to keep the update in line</li>
</ul>
</div>
</div>
<div id="outline-container-orgd74daac" class="outline-3">
<h3 id="orgd74daac"><span class="section-number-3">8.4.</span> Proximal Policy Optimization (PPO)</h3>
<div class="outline-text-3" id="text-8-4">
<p>
PPO is the industry standard for LLM fine-tuning (RLHF). It mimics TRPO's stability but uses a much simpler "clipped" objective function.
</p>

<p>
\[L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t) \right]\]
</p>

<p>
Where:
</p>
<ul class="org-ul">
<li><b><b>Probability Ratio</b></b>: \(r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}\)</li>
<li><b><b>Epsilon \(\epsilon\)</b></b>: Usually set to 0.1 or 0.2.</li>
</ul>

<p>
The clipped PPO policy loss with Generalized Advantage Estimation (GAE) is defined as:
</p>

<p>
\[
\mathcal{L}_{\text{policy}} = - \mathbb{E}_{t} \Bigg[ \min \Bigg( r_t(\theta) \sum_{l=0}^{T-t} (\gamma \lambda)^l \delta_{t+l}, \; \operatorname{clip} \big( r_t(\theta), 1 - \epsilon, 1 + \epsilon \big) \sum_{l=0}^{T-t} (\gamma \lambda)^l \delta_{t+l} \Bigg) \Bigg]
\]
</p>
</div>
</div>
<div id="outline-container-org1cc446d" class="outline-3">
<h3 id="org1cc446d"><span class="section-number-3">8.5.</span> Training</h3>
<div class="outline-text-3" id="text-8-5">
</div>
<div id="outline-container-org98221de" class="outline-4">
<h4 id="org98221de"><span class="section-number-4">8.5.1.</span> Using sepreate network</h4>
<div class="outline-text-4" id="text-8-5-1">
<p>
Unlike traditional RL (e.g., Atari) where state representation is efficiently shared, LLM-based RL often requires decoupling the Actor and Critic due to <b><b>Negative Transfer</b></b> and <b><b>Optimization Divergence</b></b>.
</p>
</div>
<ol class="org-ol">
<li><a id="org9fa319c"></a>1. Negative Transfer (Objective Interference)<br />
<div class="outline-text-5" id="text-8-5-1-1">
<p>
The Actor and Critic optimize fundamentally conflicting objectives, leading to <b><b>Catastrophic Forgetting</b></b> in shared architectures.
</p>

<ul class="org-ul">
<li><b><b>Gradient Wash-Out:</b></b> The Critic's gradients can "wash out" the delicate weights required for language generation.</li>
<li><b><b>Manifold Collapse:</b></b> When the model maximizes reward aggressively, it loses coherence, causing catastrophic forgetting of the pre-trained language manifold.</li>
</ul>
</div>
</li>
<li><a id="orgc212b44"></a>2. Optimization Divergence (Timescale Mismatch)<br />
<div class="outline-text-5" id="text-8-5-1-2">
<p>
Shared weights prevent the necessary decoupling of learning dynamics between the two heads.
</p>

<ul class="org-ul">
<li><b><b>The Critic (Fast Learner):</b></b>
Requires rapidly tracking non-stationary value targets (\(V(s)\) changes constantly as \(\pi\) evolves).</li>
<li><b><b>The Actor (Slow Learner):</b></b>
Requires strict constraints (Trust Region/Clipping) to ensure monotonic improvement.</li>
<li><b><b>The Conflict:</b></b>
In a shared body, you cannot tune optimizers independently. A learning rate high enough for the Critic destabilizes the Actor; a rate low enough for the Actor starves the Critic.</li>
</ul>
</div>
</li>
<li><a id="orgfb72b23"></a>3. Phasic Policy Gradient<br />
<div class="outline-text-5" id="text-8-5-1-3">
<p>
Separating policy and value function training into distinct phases  <a href="https://arxiv.org/abs/2009.04416">PPG</a>. 
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-orgc9537bb" class="outline-4">
<h4 id="orgc9537bb"><span class="section-number-4">8.5.2.</span> Processing</h4>
<div class="outline-text-4" id="text-8-5-2">

<div id="org259fb23" class="figure">
<p><img src="./html/reinforcement_learning_ppo.png" alt="reinforcement_learning_ppo.png" />
</p>
</div>


<div class="org-src-container">
<pre class="src src-python"><span style="color: #51afef;">import</span> torch
<span style="color: #51afef;">import</span> torch.nn.functional <span style="color: #51afef;">as</span> F

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">constants
</span><span style="color: #dcaeea;">kl_beta</span> <span style="color: #51afef;">=</span> 0.1
<span style="color: #dcaeea;">critic_weight</span> <span style="color: #51afef;">=</span> 0.5
<span style="color: #dcaeea;">ppo_eps</span> <span style="color: #51afef;">=</span> 0.2

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">sample prompt completions and rewards
</span><span style="color: #51afef;">with</span> torch.no_grad():
    <span style="color: #dcaeea;">completions</span> <span style="color: #51afef;">=</span> LLM.generate(prompts)  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">(B*G, L)
</span>    <span style="color: #dcaeea;">rewards</span> <span style="color: #51afef;">=</span> RM(completions)  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">(B*G, 1)
</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">create a padding mask from lengths of completions in batch
</span><span style="color: #dcaeea;">completion_mask</span> <span style="color: #51afef;">=</span> <span style="color: #51afef;">&lt;</span>... mask out padding tokens ...<span style="color: #51afef;">&gt;</span>

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">compute value function / critic output
</span><span style="color: #dcaeea;">values</span> <span style="color: #51afef;">=</span> CRITIC(completions)  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">(B*G, L) - predicted reward per token!
</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">get policy logprobs for each action
</span><span style="color: #dcaeea;">llm_out</span> <span style="color: #51afef;">=</span> LLM(completions)
<span style="color: #dcaeea;">per_token_logps</span> <span style="color: #51afef;">=</span> F.log_softmax(llm_out, dim<span style="color: #51afef;">=-</span>1)  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">(B*G, L)
</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">get reference logprobs for each action
</span><span style="color: #dcaeea;">ref_out</span> <span style="color: #51afef;">=</span> REF(completions)
<span style="color: #dcaeea;">ref_per_token_logps</span> <span style="color: #51afef;">=</span> F.log_softmax(ref_out, dim<span style="color: #51afef;">=-</span>1)  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">(B*G, L)
</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">compute KL divergence between policy and reference policy
</span><span style="color: #dcaeea;">kl_div</span> <span style="color: #51afef;">=</span> per_token_logps <span style="color: #51afef;">-</span> ref_per_token_logps

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">directly subtract KL divergence from rewards
</span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">NOTE: KL div is per token, so reward becomes per token and reward
</span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">for all tokens (besides last token) is just kl divergence.
</span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Reward for last token is sum of outcome reward and KL div.
</span><span style="color: #dcaeea;">rewards</span> <span style="color: #51afef;">-=</span> kl_beta <span style="color: #51afef;">*</span> kl_div <span style="color: #5B6268;"># </span><span style="color: #5B6268;">(B*G, L)
</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">compute the advantage - simple approach
</span><span style="color: #dcaeea;">advantage</span> <span style="color: #51afef;">=</span> rewards <span style="color: #51afef;">-</span> values.detach()  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">(B*G, L)
</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">compute the policy ratio
</span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">NOTE: old_per_token_logps must be persisted during first policy
</span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">update for this batch of data and re-used in each subsequent update
</span><span style="color: #dcaeea;">policy_ratio</span> <span style="color: #51afef;">=</span> torch.exp(
    per_token_logps <span style="color: #51afef;">-</span> old_per_token_logps,
)  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">(B*G, L)
</span><span style="color: #dcaeea;">clip_policy_ratio</span> <span style="color: #51afef;">=</span> torch.clamp(
    policy_ratio,
    <span style="color: #c678dd;">min</span><span style="color: #51afef;">=</span>1.0 <span style="color: #51afef;">-</span> ppo_eps,
    <span style="color: #c678dd;">max</span><span style="color: #51afef;">=</span>1.0 <span style="color: #51afef;">+</span> ppo_eps,
)

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">compute the ppo loss
</span><span style="color: #dcaeea;">ppo_loss</span> <span style="color: #51afef;">=</span> torch.<span style="color: #c678dd;">min</span>(
    advantage <span style="color: #51afef;">*</span> policy_ratio,
    advantage <span style="color: #51afef;">*</span> clip_policy_ratio,
)  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">(B*G, L)
</span><span style="color: #dcaeea;">ppo_loss</span> <span style="color: #51afef;">=</span> <span style="color: #51afef;">-</span>ppo_loss

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">combine ppo loss and critic mse loss
</span><span style="color: #dcaeea;">critic_loss</span> <span style="color: #51afef;">=</span> ((rewards <span style="color: #51afef;">-</span> values) <span style="color: #51afef;">**</span> 2)  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">(B*G, L)
</span><span style="color: #dcaeea;">loss</span> <span style="color: #51afef;">=</span> ppo_loss <span style="color: #51afef;">+</span> critic_weight <span style="color: #51afef;">*</span> critic_loss

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">aggregate the loss across tokens (many options exist here)
</span><span style="color: #dcaeea;">loss</span> <span style="color: #51afef;">=</span> ((loss <span style="color: #51afef;">*</span> completion_mask).<span style="color: #c678dd;">sum</span>(axis<span style="color: #51afef;">=-</span>1) <span style="color: #51afef;">/</span>
    <span style="color: #358856e97137;"> </span>   completion_mask.<span style="color: #c678dd;">sum</span>(axis<span style="color: #51afef;">=-</span>1)).mean()
     
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">perform policy gradient update
</span>optimizer.zero_grad()
loss.backward()
optimizer.step()

</pre>
</div>

<p>
Initialize the policy model, value model, optimizer, and freeze a reference policy model
</p>

<p>
Sample a prompt from the dataset to start a rollout
</p>

<p>
Prompt + current policy model generate a completion (no gradient)
</p>

<p>
Completion + reward model produce a scalar reward per token or per sequence
</p>

<p>
Completion + current policy model produce current action log-probabilities
</p>

<p>
Completion + frozen reference policy model produce reference log-probabilities
</p>

<p>
KL divergence between current and reference log-probabilities is computed
</p>

<p>
The KL penalty is added to the reward to form the final shaped reward
</p>

<p>
Completion + value model produce value estimates for each timestep
</p>

<p>
Shaped rewards and value estimates are combined using GAE to compute advantages and returns
</p>

<p>
Stored rollout data are shuffled and split into minibatches for PPO training
</p>

<p>
Prompt and completion are passed again through the current policy to compute updated log-probabilities
</p>

<p>
The ratio between updated policy probabilities and stored old policy probabilities is computed
</p>

<p>
The clipped PPO objective uses this ratio and the advantages to compute the policy (actor) loss
</p>

<p>
The value model is trained using mean-squared error between predicted values and computed returns
</p>

<p>
Policy loss, value loss, and entropy bonus are summed to form the total PPO loss
</p>

<p>
Backpropagation updates both policy and value model parameters in one optimizer step
</p>

<p>
Steps 11â€“16 are repeated for multiple PPO epochs over the same rollout data
</p>

<p>
A new rollout is collected using the updated policy, and the process repeats
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org0ad63fb" class="outline-2">
<h2 id="org0ad63fb"><span class="section-number-2">9.</span> GRPO</h2>
<div class="outline-text-2" id="text-9">

<div id="org2fa208b" class="figure">
<p><img src="./reinforcement_learning_grpo.png" alt="reinforcement_learning_grpo.png" />
</p>
</div>


<p>
Group Relative Policy Optimization (GRPO) resolves these conflicts by <b>eliminating the Critic network entirely</b>.
</p>

<p>
By removing the Critic part, GRPO removes the <b>source</b> of the interference. There are no value-function gradients to clash with the language modeling objectives. Instead of a learned Value Function \(V(s)\) (which requires a separate network/optimizer), GRPO estimates the baseline using the <b><b>mean reward of a group of outputs</b></b>:
  \[ A_i = \frac{r_i - \text{mean}(r_{1..G})}{\text{std}(r_{1..G})} \]
</p>
</div>
<div id="outline-container-orge5628c7" class="outline-3">
<h3 id="orge5628c7"><span class="section-number-3">9.1.</span> MoE</h3>
<div class="outline-text-3" id="text-9-1">
<p>
With shared Experts for common sense training.
in order to training all Experts P, 
</p>
<ul class="org-ul">
<li>Switch transformer, minimal the loss to force the f and P to be uniformly distributed : \(loss = \alpha \cdot N \cdot \sum_{i=1}^{N} f_{i} \cdot P_{i}\)</li>
<li>Loss free: using self-adjusted bias before softmax to control the P.
<ul class="org-ul">
<li>if some experts has too much token, decrease the bias,</li>
<li>if some experts has too less token, incurease the bias.</li>
</ul></li>

<li>DeepSeek use bias parameter before active function for dynamical adjustment of token loading for each expert</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org38cf18f" class="outline-2">
<h2 id="org38cf18f"><span class="section-number-2">10.</span> Multi-token training</h2>
<div class="outline-text-2" id="text-10">
</div>
<div id="outline-container-orgc35d326" class="outline-3">
<h3 id="orgc35d326"><span class="section-number-3">10.1.</span> Multi-token prodiction</h3>
<div class="outline-text-3" id="text-10-1">
<p>
predict multiple token, some are from small model. If LLM accepte them, it does not need to generate them again.
</p>
</div>
<div id="outline-container-org2dbd2d3" class="outline-4">
<h4 id="org2dbd2d3"><span class="section-number-4">10.1.1.</span> The Core Concept: MTP as an Implicit Critic</h4>
<div class="outline-text-4" id="text-10-1-1">
<p>
We reject GRPO (which requires generating groups for a baseline). Instead, we use <b><b>Time</b></b> as our baseline.
</p>

<p>
We use a single Transformer. The <b><b>MTP Modules</b></b> (which normally predict future tokens) are slightly modified to also predict the <b><b>Value (Expected Future Reward)</b></b> of those tokens.
</p>

<ul class="org-ul">
<li><b><b>No Separate Critic Network:</b></b> The MTP heads <b>are</b> the Critic.</li>
<li><b><b>No Group Sampling:</b></b> We do not compare against a group average. We compare against our own prediction from the <b>next</b> step (Bootstrapping).</li>
<li><b><b>One Network:</b></b> Parameters \(\theta\) are shared.</li>
</ul>
</div>
</div>
<div id="outline-container-org3e7c777" class="outline-4">
<h4 id="org3e7c777"><span class="section-number-4">10.1.2.</span> Architecture: The "Value-Aware" MTP Head</h4>
<div class="outline-text-4" id="text-10-1-2">
<p>
Standard DeepSeek MTP predicts tokens \(t_{n+1}, t_{n+2} \dots\).
We modify the output projection of the MTP modules to output two things:
</p>
<ol class="org-ol">
<li><b><b>Token Logits:</b></b> \(P(t_{n+k})\) (What happens next?)</li>
<li><b><b>Scalar Value:</b></b> \(V_{n+k}\) (How good is it?)</li>
</ol>

<p>
<b>Equation:</b>
\[ [ \text{Logits}_{k}, v_{k} ] = \text{MTP Head}_k(h_n) \]
Where \(v_k\) represents the expected return starting from step \(n+k\).
</p>
</div>
</div>
<div id="outline-container-orgd62a136" class="outline-4">
<h4 id="orgd62a136"><span class="section-number-4">10.1.3.</span> The Algorithm: Recursive Value Propagation</h4>
<div class="outline-text-4" id="text-10-1-3">
</div>
<ol class="org-ol">
<li><a id="org53fc572"></a>A. Forward Pass (Generation &amp; Storage)<br />
<div class="outline-text-5" id="text-10-1-3-1">
<p>
At time step \(t\), the network produces:
</p>
<ol class="org-ol">
<li><b><b>Action:</b></b> Sample \(a_t\) from the main policy head.</li>
<li><b><b>Lookahead Values (Internal Value):</b></b> The MTP heads produce value estimates for future steps:
<ul class="org-ul">
<li>MTP Head 1 gives \(v^{(1)}_t\) (Estimate of \(V(s_{t+1})\))</li>
<li>MTP Head 2 gives \(v^{(2)}_t\) (Estimate of \(V(s_{t+2})\))</li>
</ul></li>
</ol>

<p>
We <b><b>save</b></b> these internal value estimates \(v^{(k)}_t\) into a buffer.
</p>
</div>
</li>
<li><a id="orgd53a2de"></a>B. Interaction<br />
<div class="outline-text-5" id="text-10-1-3-2">
<p>
Execute action \(a_t\).
Observe Reward \(r_{t+1}\) and next state \(s_{t+1}\).
</p>
</div>
</li>
<li><a id="org924d738"></a>C. The "Next Step" Operation (Bootstrapping)<br />
<div class="outline-text-5" id="text-10-1-3-3">
<p>
This is the key requirement you mentioned. We use the value calculated at \(t+1\) to update the network at \(t\).
</p>

<p>
We define the <b><b>TD Target</b></b> (Temporal Difference):
\[ Y_t = r_{t+1} + \gamma v^{(1)}_{t+1} \]
<b>Note:</b> \(v^{(1)}_{t+1}\) is the "Value of the next state" predicted by the MTP head at the <b>next</b> step.
</p>
</div>
</li>
<li><a id="orgddda82b"></a>D. The Update (Loss Function)<br />
<div class="outline-text-5" id="text-10-1-3-4">
<p>
We update the single network \(\theta\) with three components:
</p>

<ol class="org-ol">
<li><b><b>Policy Loss (Actor):</b></b>
Maximize likelihood of \(a_t\) if the Advantage is positive.
\[ \delta_t = Y_t - v^{(1)}_t \quad (\text{Advantage} = \text{Target} - \text{Prediction}) \]
\[ L_{policy} = - \delta_t \ln \pi(a_t|s_t) \]</li>

<li><b><b>MTP Value Consistency Loss (The "Saved Value" Update):</b></b>
Force the MTP head at step \(t\) to accurately predict the value at \(t+1\).
\[ L_{value} = (v^{(1)}_t - \text{stop\_grad}(Y_t))^2 \]</li>

<li><b><b>MTP Token Loss (Auxiliary):</b></b>
Keep the standard MTP token prediction to ensure the representations remain grounded in language/reasoning.
\[ L_{token} = \text{CrossEntropy}(\text{MTP\_Heads}) \]</li>
</ol>

<p>
\[ L_{total} = L_{policy} + \alpha L_{value} + \beta L_{token} \]
</p>

<p>
&#x2014;
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-org91d592f" class="outline-4">
<h4 id="org91d592f"><span class="section-number-4">10.1.4.</span> Why this meets the criteria</h4>
<div class="outline-text-4" id="text-10-1-4">
<ol class="org-ol">
<li><b><b>Single Network:</b></b> The Policy and Value are fused. The "Value" is just a tiny scalar output on the existing MTP heads.</li>
<li><b><b>No GRPO:</b></b> We don't need multiple samples to find a baseline. We use the <b><b>Bellman Consistency</b></b> (\(V_t \approx r + V_{t+1}\)) as the training signal.</li>
<li><b><b>MTP Integration:</b></b> The MTP heads are essential. They provide the "Lookahead" capability that stabilizes the single-network value estimation (reducing the noise of a single step).</li>
<li><b><b>Internal Value Saved:</b></b> The training relies on carrying the scalar \(v\) from step \(t+1\) backward to step \(t\).</li>
</ol>
</div>
</div>
<div id="outline-container-org8ee24ed" class="outline-4">
<h4 id="org8ee24ed"><span class="section-number-4">10.1.5.</span> Visualization of Data Flow</h4>
<div class="outline-text-4" id="text-10-1-5">
<div class="org-src-container">
<pre class="src src-text">Step T:
   Input -&gt; [Backbone] -&gt; h_t
              |-&gt; Main Head -&gt; Action a_t (Sampled)
              |-&gt; MTP Head  -&gt; Predicts V_t (Saved)

       --- Environment Step (r_t) ---&gt;

Step T+1:
   Input -&gt; [Backbone] -&gt; h_{t+1}
              |-&gt; MTP Head  -&gt; Predicts V_{t+1} (Used as Target)

Update T:
   Target = r_t + gamma * V_{t+1}
   Error  = Target - V_t
   Backprop Error through h_t
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-orgc6d1934" class="outline-3">
<h3 id="orgc6d1934"><span class="section-number-3">10.2.</span> New way with stable assumpation</h3>
<div class="outline-text-3" id="text-10-2">
</div>
<div id="outline-container-orgaa35fd1" class="outline-4">
<h4 id="orgaa35fd1"><span class="section-number-4">10.2.1.</span> 1. The Core Innovation</h4>
<div class="outline-text-4" id="text-10-2-1">
<p>
A unified architecture where <b><b>MTP Heads</b></b> serve a dual purpose:
</p>
<ol class="org-ol">
<li><b><b>Syntax/Logic:</b></b> Predicting future tokens (Language Modeling).</li>
<li><b><b>Planning:</b></b> Predicting future <b>value</b> (Implicit Critic).</li>
</ol>
</div>
</div>
<div id="outline-container-org76afdc5" class="outline-4">
<h4 id="org76afdc5"><span class="section-number-4">10.2.2.</span> 2. Strengths (Why do this?)</h4>
<div class="outline-text-4" id="text-10-2-2">
<ul class="org-ul">
<li><b><b>Extreme Efficiency:</b></b> Eliminates the memory cost of a separate Critic (PPO) and the compute cost of Group Sampling (GRPO).</li>
<li><b><b>Temporal Credit Assignment:</b></b> Unlike GRPO (which gives the same reward to the whole sentence), this method assigns specific values to specific tokens via bootstrapping.</li>
<li><b><b>Dopamine Signals:</b></b> The MTP value prediction (\(v_t\)) acts like a localized dopamine signal, telling the model <b>exactly</b> when it made a good move, not just at the end.</li>
</ul>
</div>
</div>
<div id="outline-container-org9e91d2d" class="outline-4">
<h4 id="org9e91d2d"><span class="section-number-4">10.2.3.</span> 3. The "Stability" Bottleneck (Critical Risk)</h4>
</div>
<div id="outline-container-orgcf5080f" class="outline-4">
<h4 id="orgcf5080f"><span class="section-number-4">10.2.4.</span> The Problem: Chasing your own Tail</h4>
<div class="outline-text-4" id="text-10-2-4">
<p>
Since we use the <b>same network</b> to generate the target \(V_{t+1}\) and the prediction \(V_t\), the training can oscillate or diverge.
</p>
</div>
</div>
<div id="outline-container-orgb18a65e" class="outline-4">
<h4 id="orgb18a65e"><span class="section-number-4">10.2.5.</span> The Solution: Periodic Target Updates (Polyak Averaging)</h4>
<div class="outline-text-4" id="text-10-2-5">
<p>
We cannot easily afford a second full network. However, we can keep a <b><b>lightweight copy</b></b> of <b>just</b> the MTP heads (a few MBs).
</p>

<ul class="org-ul">
<li><b><b>Active Heads (\(\theta\)):</b></b> Learn rapidly.</li>
<li><b><b>Target Heads (\(\theta'\)):</b></b> Update slowly (\(\theta' \leftarrow \tau\theta + (1-\tau)\theta'\)).</li>
<li><b><b>Stabilized Rule:</b></b>
\[ Y_t = r_{t+1} + \gamma \text{MTP}_{\text{target}}(s_{t+1}) \]</li>
</ul>
</div>
</div>
<div id="outline-container-org5db302f" class="outline-4">
<h4 id="org5db302f"><span class="section-number-4">10.2.6.</span> 4. Final Recommendation</h4>
<div class="outline-text-4" id="text-10-2-6">
<p>
This algorithm is feasible but requires careful tuning of the <b><b>Auxiliary Loss Balance</b></b>.
</p>
</div>
</div>
<div id="outline-container-org634dc78" class="outline-4">
<h4 id="org634dc78"><span class="section-number-4">10.2.7.</span> The "Golden Ratio" Loss:</h4>
<div class="outline-text-4" id="text-10-2-7">
<p>
\[ L = L_{\text{token}} + \lambda_1 L_{\text{MTP\_tokens}} + \lambda_2 L_{\text{TD\_Value}} \]
</p>

<ul class="org-ul">
<li>If \(\lambda_2\) is too high, the "Value" objective will overwrite the "Language" objective (Catastrophic Forgetting).</li>
<li>If \(\lambda_2\) is too low, the agent won't plan.</li>
<li><b><b>Recommendation:</b></b> Start with \(\lambda_2 = 0.1\) and clamp the value gradients.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: silin</p>
<p class="date">Created: 2026-01-25 So 20:30</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
