<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2025-12-31 Mi 16:11 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>RL</title>
<meta name="author" content="si" />
<meta name="generator" content="Org Mode" />
<style type="text/css">
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="stylesheet" type="text/css" href="https://gongzhitaao.org/orgcss/org.css"/>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">RL</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org5903eaf">1. Markov Decision Process (MDP)</a></li>
<li><a href="#org9443e92">2. Bellman Equation</a></li>
<li><a href="#orgfba3a2f">3. Dynamic Programming Iterations in RL</a>
<ul>
<li><a href="#org1fa5a08">3.1. Value Iteration</a></li>
<li><a href="#org2591da6">3.2. Policy Iteration</a>
<ul>
<li><a href="#org1a0ad94">3.2.1. Phase 1: Policy Evaluation (The Inner Loop)</a></li>
<li><a href="#org15430b4">3.2.2. Phase 2: Policy Improvement (The Outer Loop)</a></li>
</ul>
</li>
<li><a href="#orgefb4c0a">3.3. Truncated Policy Iteration</a></li>
</ul>
</li>
<li><a href="#org9c460b7">4. Monte Carlo Methods in RL</a>
<ul>
<li><a href="#orgee5a0cf">4.1. 1. MC Basic (The Theoretical Baseline)</a></li>
<li><a href="#orgc19be10">4.2. 2. MC Exploring Starts (The Efficient Simulator)</a></li>
<li><a href="#org7bfb84d">4.3. 3. MC Epsilon-Greedy (The Practical Solution)</a>
<ul>
<li><a href="#org4107027">4.3.1. Core Concept: Soft Policies</a></li>
<li><a href="#orga9cefbd">4.3.2. Exploration vs. Exploitation</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org22e10a7">5. Temporal Difference</a>
<ul>
<li><a href="#org4097dc6">5.1. The Mathematical Driver</a></li>
<li><a href="#orgf07a17b">5.2. The Derivation</a></li>
<li><a href="#org0dbada1">5.3. Unified View of TD-series</a>
<ul>
<li><a href="#org285eeff">5.3.1. 1. The General Update Rule</a></li>
<li><a href="#org2e2f0b9">5.3.2. 2. Algorithm Specifics</a></li>
</ul>
</li>
<li><a href="#org93add22">5.4. Deep Q-learning</a></li>
</ul>
</li>
<li><a href="#orgfb1b512">6. Policy Gradient</a>
<ul>
<li><a href="#org895b7c9">6.1. Metrics</a></li>
<li><a href="#org5e9a0b8">6.2. General objective function for metrics</a></li>
<li><a href="#org9d991d1">6.3. Policy parameters update</a></li>
<li><a href="#org1f960e3">6.4. Theory</a></li>
<li><a href="#org3b75232">6.5. REINFORCE Algorithm</a></li>
</ul>
</li>
<li><a href="#org6b507ba">7. Actor-Critic</a>
<ul>
<li><a href="#org434da58">7.1. Q-Actor-Critic (QAC)</a>
<ul>
<li><a href="#org51422be">7.1.1. Background</a></li>
<li><a href="#orge60766e">7.1.2. Initialization</a></li>
<li><a href="#org7e9deb7">7.1.3. Loop (for each time step \(t\) in episode):</a></li>
</ul>
</li>
<li><a href="#org2e5e781">7.2. Advantage Actor-Critic</a>
<ul>
<li><a href="#org68844ec">7.2.1. Add baseline</a></li>
<li><a href="#orge9134d1">7.2.2. The Advantage Function (\(\delta_t\))</a></li>
<li><a href="#org484ecbf">7.2.3. Algorithm</a></li>
</ul>
</li>
<li><a href="#org2cb9a40">7.3. Off-Policy Actor-Critic</a>
<ul>
<li><a href="#org8eda363">7.3.1. The Objective Gradient</a></li>
<li><a href="#org3a6556e">7.3.2. The Update Rule</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org4541080">8. PPO</a></li>
<li><a href="#orgdd48c41">9. LLM Decouple Actor and Critic</a>
<ul>
<li><a href="#org840084a">9.1. 1. Negative Transfer (Objective Interference)</a></li>
<li><a href="#orga8e6cce">9.2. 2. Optimization Divergence (Timescale Mismatch)</a></li>
<li><a href="#org438e6c8">9.3. 3. Phasic Policy Gradient</a></li>
</ul>
</li>
<li><a href="#org35d9dee">10. GRPO</a></li>
<li><a href="#org6ed9c72">11. MoE</a></li>
<li><a href="#orgb52cfce">12. Multi-token prodiction</a></li>
<li><a href="#org3aabe59">13. Multi-token training</a>
<ul>
<li><a href="#org4fd7c5a">13.1. The Core Concept: MTP as an Implicit Critic</a></li>
<li><a href="#org7451698">13.2. Architecture: The "Value-Aware" MTP Head</a></li>
<li><a href="#org8f0e852">13.3. The Algorithm: Recursive Value Propagation</a>
<ul>
<li><a href="#org8aea54f">13.3.1. A. Forward Pass (Generation &amp; Storage)</a></li>
<li><a href="#org9f11a3a">13.3.2. B. Interaction</a></li>
<li><a href="#orgfecd9c8">13.3.3. C. The "Next Step" Operation (Bootstrapping)</a></li>
<li><a href="#orge8a3d3a">13.3.4. D. The Update (Loss Function)</a></li>
</ul>
</li>
<li><a href="#orgbc03308">13.4. Why this meets the criteria</a></li>
<li><a href="#org2dc1d86">13.5. Visualization of Data Flow</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-org5903eaf" class="outline-2">
<h2 id="org5903eaf"><span class="section-number-2">1.</span> Markov Decision Process (MDP)</h2>
<div class="outline-text-2" id="text-1">
<ul class="org-ul">
<li><b>State</b>: the set of states \( S \)
<ul class="org-ul">
<li>at a time point, for all parts, how the world looks like.</li>
<li>State space: at a time point, for all parts, how the world could look like</li>
</ul></li>
<li><b>Action</b>: the set of actions \( \mathcal{A}(s) \) is associated with state \( s \in S \)
<ul class="org-ul">
<li>at a time point, for each parts, what I do.</li>
<li>Action space : at a time point, for each parts, what I could do</li>
</ul></li>
<li><b>Reward</b>: the set of rewards \( \mathcal{R}(s, a) \)</li>

<li><b>Trajectory</b>
<ul class="org-ul">
<li>episode, return: sum of discount return \(\gamma\) in a episode(trajectory)</li>
</ul></li>

<li><b>State transition probability</b>:
<ul class="org-ul">
<li>state transition: at a time point, if I did one operation, what the world is changed to.</li>
<li>at a time point, if I did one operation, How the world can be changed.</li>
<li>At state \( s \), taking action \( a \), the probability to transit to state \( s' \) is    \( p(s' \mid s, a) \)</li>
</ul></li>
<li><b>Reward probability</b>:  
At state \( s \), taking action \( a \), the probability to get reward \( r \) is    \( p(r \mid s, a) \)</li>
<li><b>Policy</b>
<ul class="org-ul">
<li>deterministic &amp; stochastic</li>
<li>At state \( s \), the probability to choose action \( a \) is   \( \pi(a \mid s)  \)</li>
</ul></li>
</ul>


<ul class="org-ul">
<li><p>
<b>Markov property</b>
Memoryless property:
\[
  p(s_{t+1} \mid a_{t+1}, s_t, \ldots, a_1, s_0)
  =
  p(s_{t+1} \mid a_{t+1}, s_t)
  \]
</p>

<p>
\[
  p(r_{t+1} \mid a_{t+1}, s_t, \ldots, a_1, s_0)
  =
  p(r_{t+1} \mid a_{t+1}, s_t)
  \]
</p></li>
</ul>
</div>
</div>
<div id="outline-container-org9443e92" class="outline-2">
<h2 id="org9443e92"><span class="section-number-2">2.</span> Bellman Equation</h2>
<div class="outline-text-2" id="text-2">
<ul class="org-ul">
<li><b>Return</b> : summary of all discount return in one complete trajectory</li>

<li><b>State Value</b>: expectation of  Return for all possiable trajectories
 \[ V_{\pi}(s) = \sum_{a} \pi(a|s) \sum_{s', r} p(s', r | s, a) \left[ r + \gamma V_{\pi}(s') \right] \]
Relationship to action value:    \[ V_{\pi}(s) = \sum_{a \in \mathcal{A}} \pi(a|s) Q_{\pi}(s, a) \]</li>

<li><b>Action Value</b>: expectation of  Return for all possiable trajectories  after taking a specified action
 \[ Q_{\pi}(s, a) = \sum_{s', r} p(s', r | s, a) \left[ r + \gamma \sum_{a'} \pi(a'|s') Q_{\pi}(s', a') \right] \]
Relationship to state value   \[ Q_{\pi}(s, a) = \sum_{s', r} p(s', r | s, a) \left[ r + \gamma V_{\pi}(s') \right] \]</li>
</ul>


<ul class="org-ul">
<li><b>Bellman Optimality Equation</b> is bellman equation with the best policy</li>
</ul>
</div>
</div>
<div id="outline-container-orgfba3a2f" class="outline-2">
<h2 id="orgfba3a2f"><span class="section-number-2">3.</span> Dynamic Programming Iterations in RL</h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="outline-container-org1fa5a08" class="outline-3">
<h3 id="org1fa5a08"><span class="section-number-3">3.1.</span> Value Iteration</h3>
<div class="outline-text-3" id="text-3-1">
<p>
Value Iteration combines policy improvement and a single step of policy evaluation into one operation.
</p>
<ul class="org-ul">
<li><b><b>Initialization:</b></b> Start with an arbitrary initial value for all states (e.g., \(v_0 = 0\)).</li>
<li><b><b>Iteration:</b></b>
<ol class="org-ol">
<li><b>Implicit Policy Update:</b> For all states, apply all possible action (Q-table), look ahead to find the best action.
\[ \pi_{k+1} = \arg\max_{\pi} \left( r_{\pi} + \gamma P_{\pi} v_k \right) \]</li>
<li><b>Value Update:</b> Use that best action to update the state value immediately.
\[ v_{k+1} = r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}} v_k \]</li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-org2591da6" class="outline-3">
<h3 id="org2591da6"><span class="section-number-3">3.2.</span> Policy Iteration</h3>
<div class="outline-text-3" id="text-3-2">
<p>
Policy Iteration separates the process into two distinct phases. Crucially, the <b><b>Policy Evaluation</b></b> phase is itself an iterative process that looks very similar to Value Iteration, but with a <b>fixed</b> policy.
</p>
</div>
<div id="outline-container-org1a0ad94" class="outline-4">
<h4 id="org1a0ad94"><span class="section-number-4">3.2.1.</span> Phase 1: Policy Evaluation (The Inner Loop)</h4>
<div class="outline-text-4" id="text-3-2-1">
<p>
This phase starts with a policy \(\pi\) and calculates its value \(v_{\pi}\).
</p>
<ul class="org-ul">
<li><b><b>Input:</b></b> The current fixed policy \(\pi_k\).</li>
<li><b><b>Process:</b></b> We iterate to find the value by performing "Value Updates" repeatedly.
<ol class="org-ol">
<li>Initialize a value estimate \(v\) (e.g., \(v=0\)).</li>
<li>Apply the Bellman Expectation Operator repeatedly (infinite steps):
\[ v_{i+1} = r_{\pi_k} + \gamma P_{\pi_k} v_i \]</li>
<li><b><b>Stop</b></b> when \(v\) converges (i.e., \(i \to \infty\)).</li>
</ol></li>
<li><b><b>Output:</b></b> The converged state-value function \(v_{\pi_k}\).</li>
</ul>
</div>
</div>
<div id="outline-container-org15430b4" class="outline-4">
<h4 id="org15430b4"><span class="section-number-4">3.2.2.</span> Phase 2: Policy Improvement (The Outer Loop)</h4>
<div class="outline-text-4" id="text-3-2-2">
<p>
Once we know exactly how good the current policy \(\pi_k\) is (from Phase 1), we make it greedy.
</p>
<ul class="org-ul">
<li><b><b>Process:</b></b>
\[ \pi_{k+1} = \arg\max_{\pi} \left( r_{\pi} + \gamma P_{\pi} v_{\pi_k} \right) \]</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgefb4c0a" class="outline-3">
<h3 id="orgefb4c0a"><span class="section-number-3">3.3.</span> Truncated Policy Iteration</h3>
<div class="outline-text-3" id="text-3-3">
<p>
The mathematical connection is defined by the depth of the evaluation step (the number of iterations \(j\) in Phase 1):
</p>
<ul class="org-ul">
<li><b><b>Value Iteration</b></b> is a special case of Truncated Policy Iteration where the evaluation depth is \(j=1\).</li>
<li><b><b>Policy Iteration</b></b> is the limit of Truncated Policy Iteration where the evaluation depth \(j \to \infty\).</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org9c460b7" class="outline-2">
<h2 id="org9c460b7"><span class="section-number-2">4.</span> Monte Carlo Methods in RL</h2>
<div class="outline-text-2" id="text-4">
<p>
Model-free, without to know the model, we use the expecation of example from MC to fine-tune Action value:  \(q_{\pi_{k}(s,a)} = E(G_{t}| S_{t}=s, A_{t}=a )\). Basic the process works the same as Policy iteration(Policy evaluation and policy improvement), only the evaluation is with the exampling from MC.
</p>
</div>
<div id="outline-container-orgee5a0cf" class="outline-3">
<h3 id="orgee5a0cf"><span class="section-number-3">4.1.</span> 1. MC Basic (The Theoretical Baseline)</h3>
<div class="outline-text-3" id="text-4-1">
<p>
This is the simplest form, adapted directly from Policy Iteration logic but using sample returns instead of models.
</p>
<ul class="org-ul">
<li><b>Theory:</b>
<ul class="org-ul">
<li>Operates in strict distinct steps: <b><b>Policy Evaluation</b></b> (wait for many episodes) \(\rightarrow\) <b><b>Policy Improvement</b></b> (update policy).</li>
<li>Uses the <b><b>Initial-Visit Strategy</b></b>: Only the start of an episode is used to update the value of the starting state-action pair \((s_0, a_0)\). Intermediate steps in the episode are ignored for updates.</li>
</ul></li>
<li><b>Limitation:</b>
<ul class="org-ul">
<li><b><b>Low Sample Efficiency</b></b>: Wastes data by ignoring subsequent state visits in an episode.</li>
<li><b><b>Impractical</b></b>: Requires collecting "sufficiently many episodes" for <b>every</b> state-action pair before making a single policy update.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgc19be10" class="outline-3">
<h3 id="orgc19be10"><span class="section-number-3">4.2.</span> 2. MC Exploring Starts (The Efficient Simulator)</h3>
<div class="outline-text-3" id="text-4-2">
<p>
An extension designed to fix sample efficiency but introduces a strong dependency on environment control.
</p>
<ul class="org-ul">
<li><b>Theory:</b>
<ul class="org-ul">
<li><b><b>Data Efficiency</b></b>: Uses the <b><b>Every-Visit Strategy</b></b> (or sub-episode decomposition). One long episode is broken down into multiple "sub-episodes" to update values for every state visited, not just the start.</li>
<li><b><b>Episode-by-Episode Update</b></b>: Updates the policy immediately after a single episode (Generalized Policy Iteration), rather than waiting for a batch.</li>
</ul></li>
<li><b>Limitation:</b>
<ul class="org-ul">
<li><b><b>The "Exploring Starts" Assumption</b></b>: It requires the environment to be able to start an episode at <b>any</b> random state-action pair \((s, a)\).</li>
<li><b><b>Real-world Friction</b></b>: This is often impossible in physical reality (e.g., you cannot initialize a robot in a specific "falling over" state instantly).</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org7bfb84d" class="outline-3">
<h3 id="org7bfb84d"><span class="section-number-3">4.3.</span> 3. MC Epsilon-Greedy (The Practical Solution)</h3>
<div class="outline-text-3" id="text-4-3">
<p>
This method removes the unrealistic "Exploring Starts" assumption, making MC methods viable for real-world learning where you cannot control the starting state.
</p>
</div>
<div id="outline-container-org4107027" class="outline-4">
<h4 id="org4107027"><span class="section-number-4">4.3.1.</span> Core Concept: Soft Policies</h4>
<div class="outline-text-4" id="text-4-3-1">
<p>
Instead of forcing the environment to <b>start</b> randomly, we force the agent to <b>behave</b> randomly occasionally.
</p>
<ul class="org-ul">
<li><b><b>Goal</b></b>: Ensure all state-action pairs are visited "sufficiently many times" without external resets.</li>
<li><b><b>Mechanism</b></b>: Uses a <b><b>Soft Policy</b></b> ( \(\epsilon\) -greedy), meaning there is always a non-zero probability of taking any action in any state.</li>
</ul>
</div>
</div>
<div id="outline-container-orga9cefbd" class="outline-4">
<h4 id="orga9cefbd"><span class="section-number-4">4.3.2.</span> Exploration vs. Exploitation</h4>
<div class="outline-text-4" id="text-4-3-2">
<p>
The algorithm balances these two conflicting goals via the parameter \(\epsilon\) (epsilon).
</p>

<ul class="org-ul">
<li>Exploration ( The \(\epsilon\) component)
<ul class="org-ul">
<li><b><b>Purpose</b></b>: To discover new strategies and ensure the agent does not get stuck in a suboptimal loop. By taking random actions, the agent eventually wanders into every possible state, fulfilling the coverage requirement that "Exploring Starts" used to handle.</li>
<li><b><b>Mechanism</b></b>: With probability \(\epsilon\), the agent ignores its knowledge and chooses randomly.</li>
</ul></li>

<li>Exploitation (The \(1-\epsilon\) component)
<ul class="org-ul">
<li><b><b>Purpose</b></b>: To maximize rewards based on current knowledge. The agent selects the "Greedy" action (the one with the highest estimated value \(q(s,a)\)).</li>
<li><b><b>Mechanism</b></b>: With probability \(1 - \epsilon\), the agent chooses the best known action.</li>
</ul></li>
</ul>

<ul class="org-ul">
<li><b><b>Continuous Learning</b></b>: Even if the agent always starts at the same spot (Start Line), the stochastic nature of the policy (\(\epsilon\)) ensures that over many episodes, it will eventually drift into unvisited states.</li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-org22e10a7" class="outline-2">
<h2 id="org22e10a7"><span class="section-number-2">5.</span> Temporal Difference</h2>
<div class="outline-text-2" id="text-5">
<p>
<b>Core Concept</b>
TD Learning is the solution(policy) to the Bellman Expectation Equation, formulated as a root-finding problem and solved using the Robbins-Monro stochastic approximation algorithm.
</p>
</div>
<div id="outline-container-org4097dc6" class="outline-3">
<h3 id="org4097dc6"><span class="section-number-3">5.1.</span> The Mathematical Driver</h3>
<div class="outline-text-3" id="text-5-1">
<ul class="org-ul">
<li>The Goal: Bellman Expectation Equation
We seek \(V_{\pi}(s)\) such that:
\[V_{\pi}(s) = \mathbb{E}_{\pi} [ R_{t+1} + \gamma V_{\pi}(S_{t+1}) \mid S_t = s ]\]</li>

<li>The Problem: Root Finding
Define the objective function (Bellman Error) \(J(V)\), where we want the root \(J(V)=0\):
\[J(V)(s) = \mathbb{E} [ R_{t+1} + \gamma V(S_{t+1}) ] - V(s) = 0\]</li>

<li>The Solver: Robbins-Monro Algorithm
Iteratively find root \(\theta^*\) of \(f(\theta)=0\) using noisy observations \(\tilde{f}(\theta)\):   \[\theta_{t+1} = \theta_t + \alpha_t \cdot \tilde{f}(\theta_t)\]
<ul class="org-ul">
<li>\(\theta\) &gt; \(\theta^*\)   minus \(\tilde{f}(\theta_t)\)</li>
<li>\(\theta\) &lt; \(\theta^*\)   plus \(\tilde{f}(\theta_t)\)</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgf07a17b" class="outline-3">
<h3 id="orgf07a17b"><span class="section-number-3">5.2.</span> The Derivation</h3>
<div class="outline-text-3" id="text-5-2">
<ul class="org-ul">
<li>The Noisy Observation (TD Error)
Since we cannot compute the expectation \(\mathbb{E}\), we take a single sample:
<ul class="org-ul">
<li><b>Observation:</b> \(\tilde{J}(V_t) = (r_{t+1} + \gamma V_t(s_{t+1})) - V_t(s_t)\)</li>
<li>This is the <b><b>TD Error</b></b> (\(\delta_t\)).</li>
</ul></li>

<li>The Sample Target
The term \(r_{t+1} + \gamma V_t(s_{t+1})\) acts as the "Label":
<ul class="org-ul">
<li><b>Reality:</b> \(r_{t+1}\) (Ground truth, low variance).</li>
<li><b>Guess:</b> \(\gamma V_t(s_{t+1})\) (Bootstrap estimate).</li>
<li><b>Function:</b> It acts as the target \(y\) for \(V(s_t)\), providing a better estimate than \(V(s_t)\) alone because it includes real data.</li>
</ul></li>

<li>The Solution (TD Update Rule)
Substituting \(\tilde{f}(\theta)\) into the Robbins-Monro update:
\[V(s_t) \leftarrow V(s_t) + \alpha \delta_t\]
\[V(s_t) \leftarrow V(s_t) + \alpha \underbrace{[ (r_{t+1} + \gamma V(s_{t+1})) - V(s_t) ]}_{\text{TD Error}}\]
\[V(s_t) \leftarrow V(s_t) - \alpha [ V(s_t) - \underbrace{(r_{t+1} + \gamma V(s_{t+1}))}_{\text{Target}} ]\]</li>
</ul>
</div>
</div>
<div id="outline-container-org0dbada1" class="outline-3">
<h3 id="org0dbada1"><span class="section-number-3">5.3.</span> Unified View of TD-series</h3>
<div class="outline-text-3" id="text-5-3">
</div>
<div id="outline-container-org285eeff" class="outline-4">
<h4 id="org285eeff"><span class="section-number-4">5.3.1.</span> 1. The General Update Rule</h4>
<div class="outline-text-4" id="text-5-3-1">
<p>
When we use Action value instead of policy for TD, all discussed algorithms can be expressed as a stochastic approximation update solving a Bellman equation.
The unified update rule (Gradient Descent form) is:
</p>

<p>
\[q_{t+1}(s_t, a_t) = q_t(s_t, a_t) - \alpha_t(s_t, a_t) [ q_t(s_t, a_t) - \bar{q}_t ]\]
</p>

<ul class="org-ul">
<li>\(q_t(s_t, a_t)\): Current Estimate</li>
<li>\(\alpha_t\): Learning rate (Step size)</li>
<li>\(\bar{q}_t\): The <b><b>Target</b></b> (The noisy sample derived from environment interaction)</li>
<li>The term \([q_t - \bar{q}_t]\) represents the error to minimize.</li>
</ul>
</div>
</div>
<div id="outline-container-org2e2f0b9" class="outline-4">
<h4 id="org2e2f0b9"><span class="section-number-4">5.3.2.</span> 2. Algorithm Specifics</h4>
<div class="outline-text-4" id="text-5-3-2">
<ul class="org-ul">
<li>Sarsa:
<ul class="org-ul">
<li><b><b>Target Expression (\(\bar{q}_t\)):</b></b>    \[\bar{q}_t = r_{t+1} + \gamma q_t(s_{t+1}, a_{t+1})\]</li>
<li><b><b>Equation Aimed to Solve:</b></b>   <b><b>Bellman Expectation Equation (BE) for \(q_{\pi}\)</b></b>:
\[q_{\pi}(s, a) = \mathbb{E} [R_{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1}) \mid S_t = s, A_t = a]\]</li>
<li><b><b>Note:</b></b> This is an on-policy method using the action \(a_{t+1}\) actually taken by the current policy.</li>
</ul></li>

<li>Q-learning
<ul class="org-ul">
<li><b><b>Target Expression (\(\bar{q}_t\)):</b></b>   \[\bar{q}_t = r_{t+1} + \gamma \max_{a} q_t(s_{t+1}, a)\]</li>
<li><b><b>Equation Aimed to Solve:</b></b>  <b>Bellman Optimality Equation  (BOE) for \(q_*\)</b>:
\[q_{*}(s, a) = \mathbb{E} [R_{t+1} + \max_{a} q_{*}(S_{t+1}, a) \mid S_t = s, A_t = a]\]</li>
<li><b><b>Note:</b></b> This is an off-policy method because it updates towards the best possible action (\(\max\)), regardless of the policy actually followed.</li>
</ul></li>
</ul>


<ul class="org-ul">
<li>Expected Sarsa
<ul class="org-ul">
<li><b><b>Target Expression (\(\bar{q}_t\)):</b></b>   \[\bar{q}_t = r_{t+1} + \gamma \sum_{a} \pi_t(a|s_{t+1})q_t(s_{t+1}, a)\]</li>
<li><b><b>Equation Aimed to Solve:</b></b>   <b><b>Bellman Expectation Equation (BE) for \(q_{\pi}\)</b></b>:
\[q_{\pi}(s, a) = \mathbb{E} [R_{t+1} + \gamma \mathbb{E}_{A_{t+1}}[q_{\pi}(S_{t+1}, A_{t+1})] \mid S_t = s, A_t = a]\]</li>
<li><b><b>Note:</b></b> It reduces variance by taking the expectation over all possible next actions rather than just sampling one.</li>
</ul></li>

<li>N-step Sarsa
<ul class="org-ul">
<li><b><b>Target Expression (\(\bar{q}_t\)):</b></b>   \[\bar{q}_t = r_{t+1} + \gamma r_{t+2} + \dots + \gamma^n q_t(s_{t+n}, a_{t+n})\]</li>
<li><b><b>Equation Aimed to Solve:</b></b>   <b><b>Bellman Expectation Equation (BE) for \(q_{\pi}\)</b></b>:
\[q_{\pi}(s, a) = \mathbb{E} [R_{t+1} + \gamma R_{t+2} + \dots + \gamma^n q_{\pi}(S_{t+n}, A_{t+n}) \mid S_t = s, A_t = a]\]</li>
<li><b><b>Note:</b></b> It balances bias and variance by looking \(n\) steps ahead before bootstrapping.</li>
</ul></li>

<li>Monte Carlo (MC)
<ul class="org-ul">
<li><b><b>Target Expression (\(\bar{q}_t\)):</b></b>  \[\bar{q}_t = r_{t+1} + \gamma r_{t+2} + \dots \text{ (Full Return } G_t)\]</li>
<li><b><b>Equation Aimed to Solve:</b></b>   <b><b>Bellman Expectation Equation (BE) for \(q_{\pi}\)</b></b>:
\[q_{\pi}(s, a) = \mathbb{E} [R_{t+1} + \gamma R_{t+2} + \dots \mid S_t = s, A_t = a]\]</li>
<li><b><b>Note:</b></b> Can be viewed as the unified expression where \(\alpha_t(s_t, a_t) = 1\), making \(q_{t+1} = \bar{q}_t\) (direct assignment of the return).</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org93add22" class="outline-3">
<h3 id="org93add22"><span class="section-number-3">5.4.</span> Deep Q-learning</h3>
<div class="outline-text-3" id="text-5-4">
<ul class="org-ul">
<li>Deep Q-learning replaces the tabular \(q(s,a)\) with a parameterized neural network \(\hat{q}(s, a, w)\).</li>
<li>We want the neural network to satisfy the <b><b>Bellman Optimality Equation</b></b>:
\[q_*(s, a) = \mathbb{E} [R_{t+1} + \gamma \max_{a'} q_*(S_{t+1}, a') \mid S_t=s, A_t=a]\]</li>
<li>It aims to minimize the loss function \(J(w)\):
\[J(w) = \mathbb{E} \left[ \left( \underbrace{R + \gamma \max_{a' \in \mathcal{A}(S')} \hat{q}(S', a', w)}_{\text{Target (Bellman Optimality)}} - \underbrace{\hat{q}(S, A, w)}_{\text{Prediction}} \right)^2 \right]\]</li>
</ul>


<p>
where The term inside the squared brackets is the <b><b>TD Error</b></b> (specifically for Q-Learning).
 \[\delta = (R + \gamma \max \hat{q}(S', a', w)) - \hat{q}(S, A, w)\]. In order to minimize the distance (error) between the <b>Prediction</b> and the <b>Target</b>, the network \(\hat{q}\) converges towards the optimal value function \(q_*\).
</p>

<ul class="org-ul">
<li>Algorithm
<ul class="org-ul">
<li><b><b>Sample:</b></b>
Uniformly draw a mini-batch of samples from \(\mathcal{B}\).</li>

<li><b><b>Calculate Targets:</b></b>
For each sample \((s, a, r, s')\) in the mini-batch, calculate the target value \(y_T\):
\[y_T = r + \gamma \max_{a \in \mathcal{A}(s')} \hat{q}(s', a, w_T)\]
<ul class="org-ul">
<li>Where \(w_T\) is the parameter of the <b><b>target network</b></b>.</li>
</ul></li>

<li><b><b>Update Main Network:</b></b>
Update the main network parameter \(w\) to minimize the loss:
\[Loss = (y_T - \hat{q}(s, a, w))^2\]
<ul class="org-ul">
<li>This update uses the mini-batch data \(\{(s, a, y_T)\}\).</li>
</ul></li>

<li><b><b>Update Target Network:</b></b>
Set \(w_T = w\) every \(C\) iterations.</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgfb1b512" class="outline-2">
<h2 id="orgfb1b512"><span class="section-number-2">6.</span> Policy Gradient</h2>
<div class="outline-text-2" id="text-6">
</div>
<div id="outline-container-org895b7c9" class="outline-3">
<h3 id="org895b7c9"><span class="section-number-3">6.1.</span> Metrics</h3>
<div class="outline-text-3" id="text-6-1">
<ul class="org-ul">
<li>\(\bar{v}_\pi\)  (Discounted Average Value)
\[\sum_{s \in S} d(s)\, v_\pi(s)\]
\[\mathbb{E}_{S \sim d}[v_\pi(S)]\]
\[\mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^{t} R_{t+1}\right]\]</li>

<li>\(\bar{r}_\pi\) (Average Reward Objective)
\[\sum_{s \in S} d_\pi(s)\, r_\pi(s)\]
\[\mathbb{E}_{S \sim d_\pi}[r_\pi(S)]\]
\[\lim_{n \to \infty} \frac{1}{n} \mathbb{E}\left[\sum_{t=0}^{n-1} R_{t+1}\right]\]</li>
</ul>
</div>
</div>
<div id="outline-container-org5e9a0b8" class="outline-3">
<h3 id="org5e9a0b8"><span class="section-number-3">6.2.</span> General objective function for metrics</h3>
<div class="outline-text-3" id="text-6-2">
<p>
The gradient of the objective function \(J(\theta)\) is given by the Policy Gradient Theorem:
\[\nabla_\theta J(\theta)
= \sum_{s \in S} \eta(s)
  \sum_{a \in A}
  \nabla_\theta \pi(a \mid s, \theta)\, q_\pi(s, a)\]
</p>

<p>
where:
</p>
<ul class="org-ul">
<li>\(\eta(s)\) is the state distribution (discounted or stationary depending on metric)</li>
<li>\(\nabla_\theta \pi(a \mid s, \theta)\) denotes the gradient of the policy \(\pi\) with respect to the parameters \(\theta\),</li>
<li>\(q_\pi(s, a)\) is the action-value function.</li>
<li>Note: The theorem proves equality, but in practice, ignoring the gradient of the state distribution leads to an approximation often called the "proportional" gradient.</li>
</ul>

<p>
Moreover, as an expectation:\[  \nabla_\theta J(\theta)
  = \mathbb{E}_{S \sim \eta,\; A \sim \pi(S,\theta)}
  \left[
  \nabla_\theta \ln \pi(A \mid S, \theta)\, q_\pi(S, A)
  \right].\]
</p>
</div>
</div>
<div id="outline-container-org9d991d1" class="outline-3">
<h3 id="org9d991d1"><span class="section-number-3">6.3.</span> Policy parameters update</h3>
<div class="outline-text-3" id="text-6-3">
<p>
\[  \theta_{t+1} = \theta_t + \alpha \nabla_\theta J(\theta_t)\]
</p>

<p>
Using the expectation form of the policy gradient, this becomes:
</p>


<p>
\[\theta_{t+1} = \theta_t + \alpha \mathbb{E} [ \nabla_\theta \ln \pi(A \mid S, \theta_{t})\, q_{\pi}(S, A) ]\]
</p>

<p>
We do not have \(q_{\pi}(S, A)\), so use \(\hat{q}(S_t, A_t)\) from sampling.
\[\theta_{t+1} = \theta_t + \alpha \nabla_\theta \ln \pi(A_t \mid S_t, \theta_t)\, \hat{q}(S_t, A_t)\]
</p>
<ul class="org-ul">
<li>Sampling from MC: REINFORCE</li>
<li>Sampling from TD: Actor-Critic</li>
</ul>
</div>
</div>
<div id="outline-container-org1f960e3" class="outline-3">
<h3 id="org1f960e3"><span class="section-number-3">6.4.</span> Theory</h3>
<div class="outline-text-3" id="text-6-4">
<p>
\[\theta_{t+1} = \theta_t + \alpha \nabla_{\theta} \ln \pi(a_t \mid s_t, \theta_t) q_t(s_t, a_t)\]
Because of the log-derivative trick:
\[\nabla_{\theta} \ln \pi(a_t \mid s_t, \theta_t) = \frac{\nabla_{\theta} \pi(a_t \mid s_t, \theta_t)}{\pi(a_t \mid s_t, \theta_t)}\]
We have:
\[\theta_{t+1} = \theta_t + \alpha \frac{\nabla_{\theta} \pi(a_t \mid s_t, \theta_t)}{\pi(a_t \mid s_t, \theta_t)} q_t(s_t, a_t)\]
So, defining \(\beta_t = \frac{q_t(s_t, a_t)}{\pi(a_t \mid s_t, \theta_t)}\):
\[\theta_{t+1} = \theta_t + \alpha \beta_t \nabla_{\theta} \pi(a_t \mid s_t, \theta_t)\]
</p>
<ul class="org-ul">
<li>If \(\beta_t \ge 0 \implies\)  Move in direction of gradient:
<ul class="org-ul">
<li>-\(\implies \pi(a_t | s_t, \theta_{t+1}) \ge \pi(a_t | s_t, \theta_t)\)</li>
<li>Enhancement</li>
<li>Reinforce good actions</li>
</ul></li>

<li>If \(\beta_t < 0 \implies\) Move opposite to gradient :
<ul class="org-ul">
<li>\(\implies \pi(a_t | s_t, \theta_{t+1}) < \pi(a_t | s_t, \theta_t)\)</li>
<li>Decrease</li>
<li>Suppress bad actions</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org3b75232" class="outline-3">
<h3 id="org3b75232"><span class="section-number-3">6.5.</span> REINFORCE Algorithm</h3>
<div class="outline-text-3" id="text-6-5">
<p>
Monte Carlo Policy Gradient
</p>
<ul class="org-ul">
<li>Initialize: \(\theta\), \(\gamma \in (0,1)\), \(\alpha > 0\)</li>
<li><p>
Loop for each episode:
</p>
<ul class="org-ul">
<li>Generate episode \(\{s_0, a_0, r_1, \dots, s_{T-1}, a_{T-1}, r_T\}\) following policy \(\pi(\cdot|\cdot, \theta_k)\)</li>
<li>For \(t = 0\) to \(T-1\):
<ul class="org-ul">
<li>\(G_t \leftarrow \sum_{k=t+1}^{T} \gamma^{k-t-1} r_k\)  (Calculate return)</li>
<li>\(\theta_{t} \leftarrow \theta_{t} + \alpha \gamma^t G_t \nabla_{\theta} \ln \pi(a_t | s_t, \theta_{k})\)</li>
</ul></li>
</ul>
<ul class="org-ul">
<li>\(\theta_{k} \leftarrow \theta_{t}\)</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org6b507ba" class="outline-2">
<h2 id="org6b507ba"><span class="section-number-2">7.</span> Actor-Critic</h2>
<div class="outline-text-2" id="text-7">
</div>
<div id="outline-container-org434da58" class="outline-3">
<h3 id="org434da58"><span class="section-number-3">7.1.</span> Q-Actor-Critic (QAC)</h3>
<div class="outline-text-3" id="text-7-1">
</div>
<div id="outline-container-org51422be" class="outline-4">
<h4 id="org51422be"><span class="section-number-4">7.1.1.</span> Background</h4>
<div class="outline-text-4" id="text-7-1-1">
<p>
Derived from the Policy Gradient theorem, QAC replaces the Monte Carlo (MC) return with a <b><b>Temporal Difference (TD)</b></b> value function approximation.
The policy parameters are updated using the gradient of the log-probability scaled by the action-value function:
\[\theta_{t+1} = \theta_t + \alpha \nabla_{\theta} \ln \pi(a_t \mid s_t, \theta_t) q_t(s_t, a_t)\]
</p>
</div>
</div>
<div id="outline-container-orge60766e" class="outline-4">
<h4 id="orge60766e"><span class="section-number-4">7.1.2.</span> Initialization</h4>
<div class="outline-text-4" id="text-7-1-2">
<ul class="org-ul">
<li><b>Policy Function:</b> \(\pi(a|s, \theta_0)\) with initial parameters \(\theta_0\).</li>
<li><b>Value Function:</b> \(q(s, a, w_0)\) with initial parameters \(w_0\).</li>
<li><b>Learning Rates:</b> \(\alpha_w, \alpha_{\theta} > 0\).</li>
<li><b>Goal:</b> Maximize the expected return \(J(\theta)\).</li>
</ul>
</div>
</div>
<div id="outline-container-org7e9deb7" class="outline-4">
<h4 id="org7e9deb7"><span class="section-number-4">7.1.3.</span> Loop (for each time step \(t\) in episode):</h4>
<div class="outline-text-4" id="text-7-1-3">
<ol class="org-ol">
<li><b><b>Generate Action:</b></b> Sample \(a_t \sim \pi(a|s_t, \theta_t)\).</li>
<li><b><b>Observe Reward:</b></b> Get \(r_{t+1}\) and next state \(s_{t+1}\).</li>
<li><b><b>Select Next Action:</b></b> Sample \(a_{t+1} \sim \pi(a|s_{t+1}, \theta_t)\).</li>

<li><b><b>Actor (Policy Update):</b></b>
Update the policy parameters in the direction of higher rewards:
\[\theta_{t+1} = \theta_t + \alpha_{\theta} \nabla_{\theta} \ln \pi(a_t | s_t, \theta_t) q(s_t, a_t, w_t)\]</li>

<li><b><b>Critic (Value Update):</b></b>
Update the action-value parameters using the semi-gradient TD error:
\[w_{t+1} = w_t + \alpha_w \left[ r_{t+1} + \gamma q(s_{t+1}, a_{t+1}, w_t) - q(s_t, a_t, w_t) \right] \nabla_w q(s_t, a_t, w_t)\]</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-org2e5e781" class="outline-3">
<h3 id="org2e5e781"><span class="section-number-3">7.2.</span> Advantage Actor-Critic</h3>
<div class="outline-text-3" id="text-7-2">
</div>
<div id="outline-container-org68844ec" class="outline-4">
<h4 id="org68844ec"><span class="section-number-4">7.2.1.</span> Add baseline</h4>
<div class="outline-text-4" id="text-7-2-1">
<p>
To improve the stability of the Policy Gradient, we introduce a <b><b>baseline</b></b> to the update function. Subtracting a state-value function \(v(s)\) from the return reduces variance without introducing bias. The standard gradient update is modified by subtracting the baseline \(v_t(s_t)\):
\[ \theta_{t+1} = \theta_t + \alpha \nabla_{\theta} \ln \pi(a_t \mid s_t, \theta_t) [q_t(s_t, a_t) - v_t(s_t)] \]
</p>
</div>
</div>
<div id="outline-container-orge9134d1" class="outline-4">
<h4 id="orge9134d1"><span class="section-number-4">7.2.2.</span> The Advantage Function (\(\delta_t\))</h4>
<div class="outline-text-4" id="text-7-2-2">
<p>
We define the <b><b>Advantage Function</b></b> as the difference between the action-value and the state-value. However, since we often do not know \(q_t\) explicitly, we approximate it using the <b><b>TD Error</b></b> \(\delta_t\):
</p>

<p>
\[ \delta_t(s_t, a_t) = q_t(s_t, a_t) - v_t(s_t) \approx r_{t+1} + \gamma v_t(s_{t+1}) - v_t(s_t) \]
</p>

<p>
Substituting this back into our update rule gives us a more robust update:
\[ \theta_{t+1} = \theta_t + \alpha \nabla_{\theta} \ln \pi(a_t \mid s_t, \theta_t) \delta_t(s_t, a_t) \]
</p>

<p>
<b>Why use the Advantage Function?</b>
The advantage \(\delta_t\) effectively measures how much better an action \(a_t\) was compared to the "average" value of the state. This helps balance <b><b>exploration and exploitation</b></b> more effectively than raw returns, as the agent explicitly learns which actions outperform the expected baseline.
</p>
</div>
</div>
<div id="outline-container-org484ecbf" class="outline-4">
<h4 id="org484ecbf"><span class="section-number-4">7.2.3.</span> Algorithm</h4>
<div class="outline-text-4" id="text-7-2-3">
<ul class="org-ul">
<li><b>Policy Function (Actor):</b> \(\pi(a|s, \theta_0)\) with initial parameters \(\theta_0\).</li>
<li><b>Value Function (Critic):</b> \(v(s, w_0)\) with initial parameters \(w_0\).</li>
<li><b>Learning Rates:</b> \(\alpha_w, \alpha_{\theta} > 0\).</li>
<li><b>Goal:</b> Learn an optimal policy to maximize expected return \(J(\theta)\).</li>
</ul>

<p>
At time step \(t\) in each episode:
</p>

<ol class="org-ol">
<li><b><b>Generate Action &amp; Observe:</b></b>
Generate \(a_t\) following \(\pi(a|s_t, \theta_t)\), then observe reward \(r_{t+1}\) and next state \(s_{t+1}\).</li>

<li><b><b>Calculate Advantage (TD Error):</b></b>
Compute the TD error using the Critic's current value estimates:
\[ \delta_t = r_{t+1} + \gamma v(s_{t+1}, w_t) - v(s_t, w_t) \]</li>

<li><b><b>Actor Update (Policy):</b></b>
Update the policy parameters to encourage actions with high advantage:
\[ \theta_{t+1} = \theta_t + \alpha_{\theta} \delta_t \nabla_{\theta} \ln \pi(a_t | s_t, \theta_t) \]</li>

<li><b><b>Critic Update (Value):</b></b>
Update the value function parameters to minimize the TD error:
\[ w_{t+1} = w_t + \alpha_w \delta_t \nabla_w v(s_t, w_t) \]</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-org2cb9a40" class="outline-3">
<h3 id="org2cb9a40"><span class="section-number-3">7.3.</span> Off-Policy Actor-Critic</h3>
<div class="outline-text-3" id="text-7-3">
<p>
We use a behavior policy \(\beta\) to generate experience samples. To estimate the gradient of the target policy \(\pi\), we must use <b><b>Importance Sampling</b></b>.
</p>
</div>
<div id="outline-container-org8eda363" class="outline-4">
<h4 id="org8eda363"><span class="section-number-4">7.3.1.</span> The Objective Gradient</h4>
<div class="outline-text-4" id="text-7-3-1">
<p>
\[ \nabla_{\theta} J(\theta) = \mathbb{E}_{S \sim \rho, A \sim \beta} \left[ \frac{\pi(A|S, \theta)}{\beta(A|S)} \nabla_{\theta} \ln \pi(A|S, \theta) q_{\pi}(S, A) \right] \]
</p>
</div>
</div>
<div id="outline-container-org3a6556e" class="outline-4">
<h4 id="org3a6556e"><span class="section-number-4">7.3.2.</span> The Update Rule</h4>
<div class="outline-text-4" id="text-7-3-2">
<p>
The update is similar to A2C but scaled by the importance sampling ratio \(\rho_t\):
</p>

<p>
\[ \theta_{t+1} = \theta_t + \alpha \underbrace{ \frac{\pi(a_t|s_t, \theta_t)}{\beta(a_t|s_t)} }_{\text{Importance Weight } \rho_t} \delta_t(s_t, a_t) \nabla_{\theta} \ln \pi(a_t \mid s_t, \theta_t) \]
</p>

<p>
Because of the log-derivative trick:
\[ \theta_{t+1} = \theta_t + \alpha \frac{\delta_t(s_t, a_t)}{\beta(a_{t}|s_{t})} \nabla_{\theta} \pi(a_t \mid s_t, \theta_t) \]
</p>

<p>
Algorithm is similar only with difference of factor \(\beta(a_{t}|s_{t})\)
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org4541080" class="outline-2">
<h2 id="org4541080"><span class="section-number-2">8.</span> PPO</h2>
<div class="outline-text-2" id="text-8">
<p>
use  Importance Sampling as \(r_t(\theta)\) , old policy as \(\beta\) behavior policy 
\[r_t(\theta) = \frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}\]
</p>


<p>
if \(\beta(a|s)\) is very small (the behavior policy rarely took the action) but \(\pi(a|s)\) is large (the new policy really wants to take it), the ratio  \(r_t(\theta)\) becomes a huge number.
</p>
<ul class="org-ul">
<li>This causes massive gradient updates.</li>
<li>This destroys the training stability (the "step" is too big).</li>
</ul>


<p>
limit the potential explode by selecting the minimum between:
  \[L^{CLIP} = \mathbb{E} \left[ \min( r_t \delta_t, \text{clip}(r_t, 1-\epsilon, 1+\epsilon) \delta_t ) \right]\]
</p>
</div>
</div>
<div id="outline-container-orgdd48c41" class="outline-2">
<h2 id="orgdd48c41"><span class="section-number-2">9.</span> LLM Decouple Actor and Critic</h2>
<div class="outline-text-2" id="text-9">
<p>
Unlike traditional RL (e.g., Atari) where state representation is efficiently shared, LLM-based RL often requires decoupling the Actor and Critic due to <b><b>Negative Transfer</b></b> and <b><b>Optimization Divergence</b></b>.
</p>
</div>
<div id="outline-container-org840084a" class="outline-3">
<h3 id="org840084a"><span class="section-number-3">9.1.</span> 1. Negative Transfer (Objective Interference)</h3>
<div class="outline-text-3" id="text-9-1">
<p>
The Actor and Critic optimize fundamentally conflicting objectives, leading to <b><b>Catastrophic Forgetting</b></b> in shared architectures.
</p>

<ul class="org-ul">
<li><b><b>Gradient Wash-Out:</b></b> The Critic's gradients can "wash out" the delicate weights required for language generation.</li>
<li><b><b>Manifold Collapse:</b></b> When the model maximizes reward aggressively, it loses coherence, causing catastrophic forgetting of the pre-trained language manifold.</li>
</ul>
</div>
</div>
<div id="outline-container-orga8e6cce" class="outline-3">
<h3 id="orga8e6cce"><span class="section-number-3">9.2.</span> 2. Optimization Divergence (Timescale Mismatch)</h3>
<div class="outline-text-3" id="text-9-2">
<p>
Shared weights prevent the necessary decoupling of learning dynamics between the two heads.
</p>

<ul class="org-ul">
<li><b><b>The Critic (Fast Learner):</b></b>
Requires rapidly tracking non-stationary value targets (\(V(s)\) changes constantly as \(\pi\) evolves).</li>
<li><b><b>The Actor (Slow Learner):</b></b>
Requires strict constraints (Trust Region/Clipping) to ensure monotonic improvement.</li>
<li><b><b>The Conflict:</b></b>
In a shared body, you cannot tune optimizers independently. A learning rate high enough for the Critic destabilizes the Actor; a rate low enough for the Actor starves the Critic.</li>
</ul>
</div>
</div>
<div id="outline-container-org438e6c8" class="outline-3">
<h3 id="org438e6c8"><span class="section-number-3">9.3.</span> 3. Phasic Policy Gradient</h3>
<div class="outline-text-3" id="text-9-3">
<p>
Separating policy and value function training into distinct phases  <a href="https://arxiv.org/abs/2009.04416">PPG</a>. 
</p>
</div>
</div>
</div>
<div id="outline-container-org35d9dee" class="outline-2">
<h2 id="org35d9dee"><span class="section-number-2">10.</span> GRPO</h2>
<div class="outline-text-2" id="text-10">
<p>
Group Relative Policy Optimization (GRPO) resolves these conflicts by <b>eliminating the Critic network entirely</b>.
</p>

<p>
By removing the Critic part, GRPO removes the <b>source</b> of the interference. There are no value-function gradients to clash with the language modeling objectives. Instead of a learned Value Function \(V(s)\) (which requires a separate network/optimizer), GRPO estimates the baseline using the <b><b>mean reward of a group of outputs</b></b>:
  \[ A_i = \frac{r_i - \text{mean}(r_{1..G})}{\text{std}(r_{1..G})} \]
</p>
</div>
</div>
<div id="outline-container-org6ed9c72" class="outline-2">
<h2 id="org6ed9c72"><span class="section-number-2">11.</span> MoE</h2>
<div class="outline-text-2" id="text-11">
<p>
With shared Experts for common sense training.
in order to training all Experts P, 
</p>
<ul class="org-ul">
<li>Switch transformer, minimal the loss to force the f and P to be uniformly distributed : \(loss = \alpha \cdot N \cdot \sum_{i=1}^{N} f_{i} \cdot P_{i}\)</li>
<li>Loss free: using self-adjusted bias before softmax to control the P.
<ul class="org-ul">
<li>if some experts has too much token, decrease the bias,</li>
<li>if some experts has too less token, incurease the bias.</li>
</ul></li>

<li>DeepSeek use bias parameter before active function for dynamical adjustment of token loading for each expert</li>
</ul>
</div>
</div>
<div id="outline-container-orgb52cfce" class="outline-2">
<h2 id="orgb52cfce"><span class="section-number-2">12.</span> Multi-token prodiction</h2>
<div class="outline-text-2" id="text-12">
<p>
predict multiple token, some are from small model. If LLM accepte them, it does not need to generate them again.
</p>
</div>
</div>
<div id="outline-container-org3aabe59" class="outline-2">
<h2 id="org3aabe59"><span class="section-number-2">13.</span> Multi-token training</h2>
<div class="outline-text-2" id="text-13">
</div>
<div id="outline-container-org4fd7c5a" class="outline-3">
<h3 id="org4fd7c5a"><span class="section-number-3">13.1.</span> The Core Concept: MTP as an Implicit Critic</h3>
<div class="outline-text-3" id="text-13-1">
<p>
We reject GRPO (which requires generating groups for a baseline). Instead, we use <b><b>Time</b></b> as our baseline.
</p>

<p>
We use a single Transformer. The <b><b>MTP Modules</b></b> (which normally predict future tokens) are slightly modified to also predict the <b><b>Value (Expected Future Reward)</b></b> of those tokens.
</p>

<ul class="org-ul">
<li><b><b>No Separate Critic Network:</b></b> The MTP heads <b>are</b> the Critic.</li>
<li><b><b>No Group Sampling:</b></b> We do not compare against a group average. We compare against our own prediction from the <b>next</b> step (Bootstrapping).</li>
<li><b><b>One Network:</b></b> Parameters \(\theta\) are shared.</li>
</ul>
</div>
</div>
<div id="outline-container-org7451698" class="outline-3">
<h3 id="org7451698"><span class="section-number-3">13.2.</span> Architecture: The "Value-Aware" MTP Head</h3>
<div class="outline-text-3" id="text-13-2">
<p>
Standard DeepSeek MTP predicts tokens \(t_{n+1}, t_{n+2} \dots\).
We modify the output projection of the MTP modules to output two things:
</p>
<ol class="org-ol">
<li><b><b>Token Logits:</b></b> \(P(t_{n+k})\) (What happens next?)</li>
<li><b><b>Scalar Value:</b></b> \(V_{n+k}\) (How good is it?)</li>
</ol>

<p>
<b>Equation:</b>
\[ [ \text{Logits}_{k}, v_{k} ] = \text{MTP Head}_k(h_n) \]
Where \(v_k\) represents the expected return starting from step \(n+k\).
</p>
</div>
</div>
<div id="outline-container-org8f0e852" class="outline-3">
<h3 id="org8f0e852"><span class="section-number-3">13.3.</span> The Algorithm: Recursive Value Propagation</h3>
<div class="outline-text-3" id="text-13-3">
</div>
<div id="outline-container-org8aea54f" class="outline-4">
<h4 id="org8aea54f"><span class="section-number-4">13.3.1.</span> A. Forward Pass (Generation &amp; Storage)</h4>
<div class="outline-text-4" id="text-13-3-1">
<p>
At time step \(t\), the network produces:
</p>
<ol class="org-ol">
<li><b><b>Action:</b></b> Sample \(a_t\) from the main policy head.</li>
<li><b><b>Lookahead Values (Internal Value):</b></b> The MTP heads produce value estimates for future steps:
<ul class="org-ul">
<li>MTP Head 1 gives \(v^{(1)}_t\) (Estimate of \(V(s_{t+1})\))</li>
<li>MTP Head 2 gives \(v^{(2)}_t\) (Estimate of \(V(s_{t+2})\))</li>
</ul></li>
</ol>

<p>
We <b><b>save</b></b> these internal value estimates \(v^{(k)}_t\) into a buffer.
</p>
</div>
</div>
<div id="outline-container-org9f11a3a" class="outline-4">
<h4 id="org9f11a3a"><span class="section-number-4">13.3.2.</span> B. Interaction</h4>
<div class="outline-text-4" id="text-13-3-2">
<p>
Execute action \(a_t\).
Observe Reward \(r_{t+1}\) and next state \(s_{t+1}\).
</p>
</div>
</div>
<div id="outline-container-orgfecd9c8" class="outline-4">
<h4 id="orgfecd9c8"><span class="section-number-4">13.3.3.</span> C. The "Next Step" Operation (Bootstrapping)</h4>
<div class="outline-text-4" id="text-13-3-3">
<p>
This is the key requirement you mentioned. We use the value calculated at \(t+1\) to update the network at \(t\).
</p>

<p>
We define the <b><b>TD Target</b></b> (Temporal Difference):
\[ Y_t = r_{t+1} + \gamma v^{(1)}_{t+1} \]
<b>Note:</b> \(v^{(1)}_{t+1}\) is the "Value of the next state" predicted by the MTP head at the <b>next</b> step.
</p>
</div>
</div>
<div id="outline-container-orge8a3d3a" class="outline-4">
<h4 id="orge8a3d3a"><span class="section-number-4">13.3.4.</span> D. The Update (Loss Function)</h4>
<div class="outline-text-4" id="text-13-3-4">
<p>
We update the single network \(\theta\) with three components:
</p>

<ol class="org-ol">
<li><b><b>Policy Loss (Actor):</b></b>
Maximize likelihood of \(a_t\) if the Advantage is positive.
\[ \delta_t = Y_t - v^{(1)}_t \quad (\text{Advantage} = \text{Target} - \text{Prediction}) \]
\[ L_{policy} = - \delta_t \ln \pi(a_t|s_t) \]</li>

<li><b><b>MTP Value Consistency Loss (The "Saved Value" Update):</b></b>
Force the MTP head at step \(t\) to accurately predict the value at \(t+1\).
\[ L_{value} = (v^{(1)}_t - \text{stop\_grad}(Y_t))^2 \]</li>

<li><b><b>MTP Token Loss (Auxiliary):</b></b>
Keep the standard MTP token prediction to ensure the representations remain grounded in language/reasoning.
\[ L_{token} = \text{CrossEntropy}(\text{MTP\_Heads}) \]</li>
</ol>

<p>
\[ L_{total} = L_{policy} + \alpha L_{value} + \beta L_{token} \]
</p>

<p>
&#x2014;
</p>
</div>
</div>
</div>
<div id="outline-container-orgbc03308" class="outline-3">
<h3 id="orgbc03308"><span class="section-number-3">13.4.</span> Why this meets the criteria</h3>
<div class="outline-text-3" id="text-13-4">
<ol class="org-ol">
<li><b><b>Single Network:</b></b> The Policy and Value are fused. The "Value" is just a tiny scalar output on the existing MTP heads.</li>
<li><b><b>No GRPO:</b></b> We don't need multiple samples to find a baseline. We use the <b><b>Bellman Consistency</b></b> (\(V_t \approx r + V_{t+1}\)) as the training signal.</li>
<li><b><b>MTP Integration:</b></b> The MTP heads are essential. They provide the "Lookahead" capability that stabilizes the single-network value estimation (reducing the noise of a single step).</li>
<li><b><b>Internal Value Saved:</b></b> The training relies on carrying the scalar \(v\) from step \(t+1\) backward to step \(t\).</li>
</ol>
</div>
</div>
<div id="outline-container-org2dc1d86" class="outline-3">
<h3 id="org2dc1d86"><span class="section-number-3">13.5.</span> Visualization of Data Flow</h3>
<div class="outline-text-3" id="text-13-5">
<div class="org-src-container">
<pre class="src src-text">Step T:
   Input -&gt; [Backbone] -&gt; h_t
              |-&gt; Main Head -&gt; Action a_t (Sampled)
              |-&gt; MTP Head  -&gt; Predicts V_t (Saved)

       --- Environment Step (r_t) ---&gt;

Step T+1:
   Input -&gt; [Backbone] -&gt; h_{t+1}
              |-&gt; MTP Head  -&gt; Predicts V_{t+1} (Used as Target)

Update T:
   Target = r_t + gamma * V_{t+1}
   Error  = Target - V_t
   Backprop Error through h_t
</pre>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: si</p>
<p class="date">Created: 2025-12-31 Mi 16:11</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
